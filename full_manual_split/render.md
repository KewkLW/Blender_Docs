# Index for render

- [Cameras](#Cameras)
- [Color Management](#Color-Management)
- [Index](#Index)
- [Introduction](#Introduction)
- [Baking](#Baking)
- [Features](#Features)
- [Gpu Rendering](#Gpu-Rendering)
- [Index](#Index)
- [Introduction](#Introduction)
- [Light Settings](#Light-Settings)
- [Material Settings](#Material-Settings)
- [World Settings](#World-Settings)
- [Adaptive Subdiv](#Adaptive-Subdiv)
- [Cameras](#Cameras)
- [Index](#Index)
- [Light Linking](#Light-Linking)
- [Object Data](#Object-Data)
- [Index](#Index)
- [Nodes](#Nodes)
- [Reducing Noise](#Reducing-Noise)
- [Film](#Film)
- [Grease Pencil](#Grease-Pencil)
- [Hair](#Hair)
- [Index](#Index)
- [Light Paths](#Light-Paths)
- [Motion Blur](#Motion-Blur)
- [Performance](#Performance)
- [Sampling](#Sampling)
- [Simplify](#Simplify)
- [Subdivision](#Subdivision)
- [Volumes](#Volumes)
- [Index](#Index)
- [Introduction](#Introduction)
- [Light Settings](#Light-Settings)
- [Material Settings](#Material-Settings)
- [Scene Settings](#Scene-Settings)
- [World Settings](#World-Settings)
- [Index](#Index)
- [Plane](#Plane)
- [Sphere](#Sphere)
- [Volume](#Volume)
- [Index](#Index)
- [Limitations](#Limitations)
- [Nodes Support](#Nodes-Support)
- [Index](#Index)
- [Object Data](#Object-Data)
- [Clamping](#Clamping)
- [Curves](#Curves)
- [Depth Of Field](#Depth-Of-Field)
- [Film](#Film)
- [Grease Pencil](#Grease-Pencil)
- [Index](#Index)
- [Motion Blur](#Motion-Blur)
- [Performance](#Performance)
- [Raytracing](#Raytracing)
- [Sampling](#Sampling)
- [Volumes](#Volumes)
- [Index](#Index)
- [Introduction](#Introduction)
- [Material](#Material)
- [Python](#Python)
- [Render](#Render)
- [Freestyle](#Freestyle)
- [Index](#Index)
- [Line Set](#Line-Set)
- [Alpha](#Alpha)
- [Color](#Color)
- [Geometry](#Geometry)
- [Index](#Index)
- [Introduction](#Introduction)
- [Strokes](#Strokes)
- [Texture](#Texture)
- [Thickness](#Thickness)
- [Index](#Index)
- [Along Stroke](#Along-Stroke)
- [Crease Angle](#Crease-Angle)
- [Curvature 3D](#Curvature-3D)
- [Distance From Camera](#Distance-From-Camera)
- [Distance From Object](#Distance-From-Object)
- [Index](#Index)
- [Material](#Material)
- [Noise](#Noise)
- [Tangent](#Tangent)
- [Along Stroke](#Along-Stroke)
- [Crease Angle](#Crease-Angle)
- [Curvature 3D](#Curvature-3D)
- [Distance From Camera](#Distance-From-Camera)
- [Distance From Object](#Distance-From-Object)
- [Index](#Index)
- [Material](#Material)
- [Noise](#Noise)
- [Tangent](#Tangent)
- [2D Offset](#2D-Offset)
- [2D Transform](#2D-Transform)
- [Backbone Stretcher](#Backbone-Stretcher)
- [Bezier Curve](#Bezier-Curve)
- [Blueprint](#Blueprint)
- [Guiding Lines](#Guiding-Lines)
- [Index](#Index)
- [Perlin Noise 1D](#Perlin-Noise-1D)
- [Perlin Noise 2D](#Perlin-Noise-2D)
- [Polygonization](#Polygonization)
- [Sampling](#Sampling)
- [Simplification](#Simplification)
- [Sinus Displacement](#Sinus-Displacement)
- [Spatial Noise](#Spatial-Noise)
- [Tip Remover](#Tip-Remover)
- [Along Stroke](#Along-Stroke)
- [Calligraphy](#Calligraphy)
- [Crease Angle](#Crease-Angle)
- [Curvature 3D](#Curvature-3D)
- [Distance From Camera](#Distance-From-Camera)
- [Distance From Object](#Distance-From-Object)
- [Index](#Index)
- [Material](#Material)
- [Noise](#Noise)
- [Tangent](#Tangent)
- [Index](#Index)
- [Introduction](#Introduction)
- [Passes](#Passes)
- [View Layer](#View-Layer)
- [Index](#Index)
- [Light Object](#Light-Object)
- [World](#World)
- [Assignment](#Assignment)
- [Index](#Index)
- [Introduction](#Introduction)
- [Line Art](#Line-Art)
- [Preview](#Preview)
- [Settings](#Settings)
- [Displacement](#Displacement)
- [Index](#Index)
- [Surface](#Surface)
- [Volume](#Volume)
- [Colors](#Colors)
- [Index](#Index)
- [Introduction](#Introduction)
- [Blend](#Blend)
- [Clouds](#Clouds)
- [Distorted Noise](#Distorted-Noise)
- [Image Movie](#Image-Movie)
- [Magic](#Magic)
- [Marble](#Marble)
- [Musgrave](#Musgrave)
- [Noise](#Noise)
- [Stucci](#Stucci)
- [Voronoi](#Voronoi)
- [Wood](#Wood)
- [Animation](#Animation)
- [Animation Player](#Animation-Player)
- [Index](#Index)
- [Introduction](#Introduction)
- [Index](#Index)
- [Introduction](#Introduction)
- [Speaker](#Speaker)
- [Format](#Format)
- [Frame Range](#Frame-Range)
- [Index](#Index)
- [Metadata](#Metadata)
- [Output](#Output)
- [Post Processing](#Post-Processing)
- [Index](#Index)
- [Introduction](#Introduction)
- [Usage](#Usage)
- [Groups](#Groups)
- [Index](#Index)
- [Introduction](#Introduction)
- [Osl](#Osl)
- [Bright Contrast](#Bright-Contrast)
- [Gamma](#Gamma)
- [Hue Saturation](#Hue-Saturation)
- [Index](#Index)
- [Invert Color](#Invert-Color)
- [Light Falloff](#Light-Falloff)
- [Mix](#Mix)
- [Rgb Curves](#Rgb-Curves)
- [Blackbody](#Blackbody)
- [Clamp](#Clamp)
- [Color Ramp](#Color-Ramp)
- [Combine Color](#Combine-Color)
- [Combine Xyz](#Combine-Xyz)
- [Float Curve](#Float-Curve)
- [Index](#Index)
- [Map Range](#Map-Range)
- [Math](#Math)
- [Mix](#Mix)
- [Rgb To Bw](#Rgb-To-Bw)
- [Separate Color](#Separate-Color)
- [Separate Xyz](#Separate-Xyz)
- [Shader To Rgb](#Shader-To-Rgb)
- [Vector Math](#Vector-Math)
- [Wavelength](#Wavelength)
- [Ao](#Ao)
- [Attribute](#Attribute)
- [Bevel](#Bevel)
- [Camera Data](#Camera-Data)
- [Fresnel](#Fresnel)
- [Geometry](#Geometry)
- [Hair Info](#Hair-Info)
- [Index](#Index)
- [Layer Weight](#Layer-Weight)
- [Light Path](#Light-Path)
- [Object Info](#Object-Info)
- [Particle Info](#Particle-Info)
- [Point Info](#Point-Info)
- [Rgb](#Rgb)
- [Tangent](#Tangent)
- [Texture Coordinate](#Texture-Coordinate)
- [Uv Map](#Uv-Map)
- [Value](#Value)
- [Vertex Color](#Vertex-Color)
- [Volume Info](#Volume-Info)
- [Wireframe](#Wireframe)
- [Aov](#Aov)
- [Index](#Index)
- [Light](#Light)
- [Material](#Material)
- [World](#World)
- [Add](#Add)
- [Background](#Background)
- [Diffuse](#Diffuse)
- [Emission](#Emission)
- [Glass](#Glass)
- [Glossy](#Glossy)
- [Hair](#Hair)
- [Hair Principled](#Hair-Principled)
- [Holdout](#Holdout)
- [Index](#Index)
- [Mix](#Mix)
- [Principled](#Principled)
- [Ray Portal](#Ray-Portal)
- [Refraction](#Refraction)
- [Sheen](#Sheen)
- [Specular Bsdf](#Specular-Bsdf)
- [Sss](#Sss)
- [Toon](#Toon)
- [Translucent](#Translucent)
- [Transparent](#Transparent)
- [Volume Absorption](#Volume-Absorption)
- [Volume Principled](#Volume-Principled)
- [Volume Scatter](#Volume-Scatter)
- [Brick](#Brick)
- [Checker](#Checker)
- [Environment](#Environment)
- [Gradient](#Gradient)
- [Ies](#Ies)
- [Image](#Image)
- [Index](#Index)
- [Magic](#Magic)
- [Musgrave](#Musgrave)
- [Noise](#Noise)
- [Point Density](#Point-Density)
- [Sky](#Sky)
- [Voronoi](#Voronoi)
- [Wave](#Wave)
- [White Noise](#White-Noise)
- [Bump](#Bump)
- [Curves](#Curves)
- [Displacement](#Displacement)
- [Index](#Index)
- [Mapping](#Mapping)
- [Normal](#Normal)
- [Normal Map](#Normal-Map)
- [Transform](#Transform)
- [Vector Displacement](#Vector-Displacement)
- [Vector Rotate](#Vector-Rotate)
- [Color](#Color)
- [Display Settings](#Display-Settings)
- [Grease Pencil](#Grease-Pencil)
- [Index](#Index)
- [Introduction](#Introduction)
- [Lighting](#Lighting)
- [Options](#Options)
- [Performance](#Performance)
- [Sampling](#Sampling)


## Cameras

.. _bpy.types.Camera:
.. _bpy.ops.camera:

*******
Cameras
*******

A camera is an object that provides a means of rendering images from Blender.
It defines which portion of a scene is visible in the rendered image.

Cameras are invisible in renders, so they do not have any material or texture settings.
However, they do have *Object* and *Editing* setting panels available which are displayed
when a camera is the active object.

.. seealso::

   :doc:`3D Viewport Camera Navigation </editors/3dview/navigate/camera_view>`
   for documentation about managing cameras in the viewport.


Properties
==========

.. reference::

   :Mode:      Object Mode
   :Editor:    :menuselection:`Properties --> Camera`


.. _camera-lens-type:

Lens
----

.. rubric:: Type

The camera lens options control the way 3D objects are represented in a 2D image.

Perspective
   This matches how you view things in the real world.
   Objects in the distance will appear smaller than objects in the foreground,
   and parallel lines (such as the rails on a railroad) will appear to converge as they get farther away.

   Focal Length/Field of View
      The :term:`Focal Length` controls the amount of zoom, i.e.
      the amount of the scene which is visible all at once.
      Longer focal lengths result in a smaller :abbr:`FOV (Field of View)` (more zoom),
      while short focal lengths allow you to see more of the scene at once
      (larger :abbr:`FOV (Field of View)`, less zoom).

      .. list-table::

         * - .. figure:: /images/render_cameras_traintracks-perspective-BI.jpg

                Perspective camera with 35 mm focal length.

           - .. figure:: /images/render_cameras_traintracks-perspective-telephoto-BI.jpg

                Perspective camera with 210 mm focal length instead of 35 mm.

      Lens Unit
         The focal length can be set either in terms of millimeters or the actual :term:`Field of View` as an angle.

         .. figure:: /images/render_cameras_perspective.svg
            :align: center
            :width: 50%

      .. hint::

         While the camera is moving towards an object the *Focal Length* property can be decreased
         to produce a *Dolly Zoom* camera effect, or vice versa.

         `This video <https://vimeo.com/15837189>`__ demonstrates the *Dolly Zoom* camera effect.

Orthographic
   With *Orthographic* perspective objects always appear at their actual size, regardless of distance.
   This means that parallel lines appear parallel, and do not converge like they do with *Perspective*.

   .. figure:: /images/render_cameras_traintracks-orthographic-BI.jpg
      :width: 50%

      Render from the same camera angle as the previous examples, but with orthographic perspective.

   Orthographic Scale
      This controls the apparent size of objects projected on the image.

      Note that this is effectively the only setting which applies to orthographic perspective.
      Since parallel lines do not converge in orthographic mode (no vanishing points),
      the lens shift settings are equivalent to translating the camera in the 3D Viewport.

      .. figure:: /images/render_cameras_orthographic.svg
         :align: center
         :width: 50%

Panoramic
   Panoramic cameras only work in Cycles. See the Cycles
   :ref:`panoramic camera <bpy.types.Camera.panorama_type>` settings for more information.

Shift
   Allows for the adjustment of *vanishing points*.
   *Vanishing points* refer to the positions to which parallel lines converge.
   In these render examples, the most obvious vanishing point is at the end of the railroad.

   .. list-table::

      * - .. figure:: /images/render_cameras_traintracks-perspective-lens-shift-BI.jpg

             Horizontal lens shift of 0.330.

        - .. figure:: /images/render_cameras_traintracks-perspective-rotate-BI.jpg

             Rotation of the camera object instead of a lens shift.

   Notice how the horizontal lines remain perfectly horizontal when using the lens shift,
   but do get skewed when rotating the camera object.

   .. note::

      Using lens shift is equivalent to rendering an image with a larger
      :abbr:`FOV (Field of View)` and cropping it off-center.

.. _camera-clipping:

Clip Start and End
   The interval in which objects are directly visible.
   Any objects outside this range still influence the image indirectly,
   as further light bounces are not clipped.

   .. note::

      For viewport rendering, setting clipping distances to limited values
      is important to ensure sufficient rasterization precision.
      Ray tracing renders do not suffer from this issue so much,
      and as such more extreme values can safely be set.

   .. tip::

      When *Limits* in the *Viewport Display* panel is enabled,
      the clip bounds will be visible as two yellow connected dots on the camera's line of sight.

   .. seealso::

      - :doc:`3D Viewport clipping </editors/3dview/sidebar>`


.. _bpy.types.CameraDOFSettings:

Depth of Field
--------------

Real-world cameras transmit light through a lens that bends and focuses it onto the sensor.
Because of this, objects that are a certain distance away are in focus,
but objects in front and behind that are blurred.

.. figure:: /images/render_cameras_dof-bokeh.jpg
   :align: center
   :width: 50%

   Example of DOF bokeh effect.

The area in focus is called the *focal point* and can be set using either an exact value,
or by using the distance between the camera and a chosen object:

Focus Object
   Choose an object which will determine the focal point. Linking an object will deactivate the distance parameter.
Focal Distance
   Sets the distance to the focal point when no *Focus Object* is specified.
   This distance can be visualized in the 3D Viewport by enabling *Limits* in the camera's
   :ref:`Viewport Display <bpy.types.Camera.show>` panel.

   .. hint::

      Hover the mouse over the *Distance* property and press :kbd:`E` to use a special *Depth Picker*,
      then click on a point in the 3D Viewport to sample the distance from that point to the camera.


Aperture
^^^^^^^^

F-Stop
   F-Stop ratio that defines the amount of blurring.
   Lower values give a strong depth of field effect.
Blades
   Total number of polygonal blades used to alter the shape of the blurred objects
   in the render, and render preview. As with the viewport, the minimum amount of
   blades to enable the bokeh effect is 3, resulting in a triangular-shaped blur.
Rotation
   Rotate the polygonal blades along the facing axis, and will rotate in a clockwise,
   and counter-clockwise fashion.
Ratio
   Change the amount of distortion to simulate the anamorphic bokeh effect.
   A setting of 1.0 shows no distortion, where a number below 1.0 will cause a horizontal distortion,
   and a higher number will cause a vertical distortion.


Camera
------

These settings adjusts properties that relate to a physical camera body.
Several :ref:`Presets <ui-presets>` can be chosen to match real-world cameras.

.. _bpy.types.Camera.sensor_fit:

Sensor Fit
   Adjusts how the camera's sensor fits within the outputs dimension adjusting the angular field of view.

   :Auto:
      Calculates a square sensor size based on the larger of
      the :ref:`Resolution <bpy.types.RenderSettings.resolution_y>` dimensions.
   :Horizontal:
      Manually adjust the *Width* of the sensor, the *Height* is calculated based on
      the aspect ratio of the output's :ref:`Resolution <bpy.types.RenderSettings.resolution_y>`.
   :Vertical:
      Manually adjust the *Height* of the sensor, the *Width* is calculated based on
      the aspect ratio of the output's :ref:`Resolution <bpy.types.RenderSettings.resolution_y>`.

.. _bpy.types.Camera.sensor_height:
.. _bpy.types.Camera.sensor_width:

Size / Width, Height
   This setting is an alternative way to control the field of view, as opposed to modifying the focal length.
   It is useful to match a camera in Blender to a physical camera and lens combination,
   e.g. for :doc:`motion tracking </movie_clip/index>`.


.. _bpy.ops.safe_areas:
.. _bpy.types.DisplaySafeAreas:

Safe Areas
----------

Safe areas are guides used to position elements to ensure that
the most important parts of the content can be seen across all screens.

Different screens have varying amounts of :term:`Overscan` (especially older TV sets).
That means that not all content will be visible to all viewers,
since parts of the image surrounding the edges are not shown.
To work around this problem TV producers defined two areas where content is guaranteed to be shown:
action safe and title safe.

Modern LCD/plasma screens with purely digital signals have no :term:`Overscan`,
yet safe areas are still considered best practice and may be legally required for broadcast.

In Blender, safe areas can be set from the Camera and Sequencer views.

.. figure:: /images/render_cameras_safe-areas-main-BI.png
   :align: center
   :width: 50%

   Red line: Action safe. Green line: Title safe.

The Safe Areas can be customized by their outer margin,
which is a percentage scale of the area between the center and the render size.
Values are shared between the Video Sequence editor and camera view.

Title Safe Margins X/Y
   Also known as *Graphics Safe*.
   Place all important information (graphics or text) inside this area to
   ensure it can be seen by the majority of viewers.
Action Safe Margins X/Y
   Make sure any significant action or characters in the shot are inside this area.
   This zone also doubles as a sort of "margin" for the screen which can be used
   to keep elements from piling up against the edges.

.. tip::

   Each country sets a legal standard for broadcasting.
   These include, among other things, specific values for safe areas.
   Blender defaults for safe areas follow the EBU (European Union) standard.
   Make sure you are using the correct values when working for broadcast to avoid any trouble.


Center-Cut Safe Areas
^^^^^^^^^^^^^^^^^^^^^

Center-cuts are a second set of safe areas to ensure content
is seen correctly on screens with a different aspect ratio.
Old TV sets receiving ``16:9`` or ``21:9`` video will cut off the sides.
Position content inside the center-cut areas to make sure the most important elements
of your composition can still be visible in these screens.

Blender defaults show a ``4:3`` (square) ratio inside ``16:9`` (widescreen).

.. figure:: /images/render_cameras_safe-areas-cuts-BI.png
   :align: center
   :width: 50%

   Cyan line: action center safe. Blue line: title center safe.


.. _bpy.types.CameraBackgroundImage:

Background Images
-----------------

A background picture in your camera can be very helpful in many situations:
modeling is obviously one, but it is also useful when painting
(e.g. you can have reference pictures of faces when painting textures directly on your model...),
or animation (when using a video as background), etc.

Background Source
   The source of the background image.

   :Image:
      Use an external image, image sequence, video file or generated texture.
   :Movie Clip:
      Use one of the Movie Clip data-blocks.

      Active Clip
         Display a Movie Clip from the scene's :ref:`Active Clip <bpy.types.Scene.active_clip>`.
      Render Undistorted
         Display the background image using undistorted proxies when available.
      Proxy Render Size
         Select between full (non-proxy) display or a proxy size to draw the background image.

         .. seealso::

            To build a proxy, the :ref:`Movie Clip Editor Proxy settings <bpy.types.MovieClipProxy>` have to be used.
            Otherwise the proxy settings here have no effect.

Color Space
   The color space the image or video file uses within Blender.

View as Render
   Apply the :ref:`color management <render-post-color-management>` settings when displaying this image on the screen.

Opacity
   Controls the transparency of the background image.

Depth
   Choose whether the image is shown behind all objects, or in front of everything.

Frame Method
   Controls how the image is placed in the camera view.

   :Stretch: Forces the image dimensions to match the camera bounds (may alter the aspect ratio).
   :Fit: Scales the image down to fit inside the camera view without altering the aspect ratio.
   :Crop:
      Scales the image up so that it fills the entire camera view,
      but without altering the aspect ratio (some of the image will be cropped).

Offset X, Y
   Positions the background image using these offsets.

   In orthographic views, this is measured in the normal scene units.
   In the camera view, this is measured relative to the camera bounds
   (0.1 will offset it by 10% of the view width/height).

Rotation
   Rotates the image around its center.

Scale
   Scales the image up or down from its center.

Flip
   X
      Swaps the image around, such that the left side is now on the right, and the right now on the left.
   Y
      Swaps the image around, such that the top side is now on the bottom, and the bottom now on the top.

.. note::

   Movie Clips or images with view as render are only visible behind objects
   when :ref:`film transparency <bpy.types.RenderSettings.film_transparent>`
   is enabled or the :ref:`scene world <bpy.types.View3DShading.use_scene_world>`
   is disabled in the viewport.

.. _bpy.types.Camera.show:
.. _bpy.types.Camera.display_size:

Viewport Display
----------------

.. figure:: /images/render_cameras_display-view-BI.png
   :align: center
   :width: 50%

   Camera view displaying safe areas, sensor and name.

Size
   Size of the camera visualization in the 3D Viewport. This setting has **no** effect on
   the render output of a camera. The camera visualization can also be scaled using
   the standard Scale :kbd:`S` transform key.

Show
   Limits
      Shows an orange line indicating the *Clip Start* and *End* values,
      as well as a yellow cross indicating the *Focus Distance*.
      If the *Focus Distance* gizmo is enabled in the 3D Viewport's
      :doc:`gizmo settings </editors/3dview/display/gizmo>`,
      this cross can also be dragged with the mouse to adjust the distance.
   Mist
      Toggles viewing of the mist limits on and off.
      The limits are shown as two connected white dots on the camera line of sight.
      The mist limits and other options are set in the *World* panel,
      in the :ref:`Mist section <bpy.types.WorldMistSettings>`.
   Sensor
      Displays a dotted frame in camera view.
   Name
      Toggle name display on and off in camera view.


.. _bpy.types.Camera.show_composition:

Composition Guides
^^^^^^^^^^^^^^^^^^

*Composition Guides* enable overlays onto the camera display that can help when framing a shot.

Thirds
   Adds lines dividing the frame in thirds vertically and horizontally.

Center
   Center
      Adds lines dividing the frame in half vertically and horizontally.
   Diagonal
      Adds lines connecting opposite corners.

Golden
   Ratio
      Divides the width and height into Golden proportions (about 0.618 of the size from all sides of the frame).
   Triangle A
      Displays a diagonal line from the lower left to upper right corners,
      then adds perpendicular lines that pass through the top left and bottom right corners.
   Triangle B
      Same as A, but with the opposite corners.

Harmony
   Triangle A
      Displays a diagonal line from the lower left to upper right corners,
      then lines from the top left and bottom right corners to 0.618 the lengths of the opposite side.
   Triangle B
      Same as A, but with the opposite corners.

.. _bpy.types.Camera.show_passepartout:
.. _bpy.types.Camera.passepartout_alpha:

Passepartout
   This option darkens the area outside of the camera's field of view.
   The opacity of the passepartout can be adjusted using the value slider.

   .. tip::

      If the Passepartout is fully opaque, Blender will make optimizations
      to speed up the rendering of areas inside the camera view.


## Color Management

.. _bpy.types.ColorManagedSequencerColorspaceSettings:
.. _bpy.types.ColorManagedDisplaySettings:
.. _bpy.types.ColorManagedViewSettings:

****************
Color Management
****************

Color management is important to create renders and assets that are physically accurate and look great
on multiple display devices. It is used both to ensure all parts of the pipeline interpret colors correctly,
and to make artistic changes like exposure and color grading.

.. figure:: /images/render_color-management_different-exposures.jpg
   :width: 300px
   :align: right

   Different views and exposures of the same render.

Blender's color management is based on the `OpenColorIO <https://opencolorio.org/>`__ library.
By using the same OpenColorIO configuration in multiple applications,
the same color spaces and transforms will be available for consistent results.


Workflow
========

.. _color-management-linear-space:

Scene Linear Color Space
------------------------

For correct results, different :term:`Color Spaces <Color Space>`
are needed for rendering, display and storage of images.
Rendering and compositing is best done in *scene linear* color space,
which corresponds more closely to nature, and makes computations more physically accurate.

.. figure:: /images/render_color-management_linear-workflow.svg
   :width: 100%
   :align: center

   An example of a linear workflow.

If the colors are linear, it means that if in reality, we double the number of photons,
the color values are also doubled. Put another way,
if we have two photos/renders each with one of two lights on, and add those images together,
the result would be the same as a render/photo with both lights on. It follows that such
a radiometrically linear space is best for photorealistic rendering and compositing.

However, these values do not directly correspond to human perception or the way display devices
work. and image files are often stored in different color spaces.
So we have to take care to do the right conversion into and out of this scene linear color space.


Display Transforms
------------------

Transforming scene linear colors to display involves both technical and artistic choices.

Correct display of renders requires a conversion to the display device color space.
A computer monitor works differently from a digital cinema projector or HDTV,
and so needs a different conversion.

There is also an artistic choice to be made.
Partially that is because display devices cannot display the full spectrum of colors and
only have limited brightness, so we can squeeze the colors to fit in the gamut of the device.
Besides that, it can also be useful to give the renders a particular look,
e.g. as if they have been printed on real camera film.
The default Filmic transform does this.

.. figure:: /images/render_color-management_linear-display-space.svg
   :width: 100%
   :align: center

   Conversion from linear to display device space.


Image Color Spaces
------------------

When loading and saving media formats it is important to have color management in mind.
File formats such as PNG or JPEG will typically store colors in a color space ready for
display, not in a linear space. When they are used as textures in renders,
they need to be converted to linear first, and when saving renders for display on the web,
they also need to be converted to a display space.

For intermediate files in production, it is recommended to use *OpenEXR* files.
These are always stored in scene linear color spaces, without any data loss.
That makes them suitable to store renders that can later be composited, color graded and
converted to different output formats.

Images can also contain data that is not actually a color. For example normal or displacement maps
merely contain vectors and offsets. Such images should be marked as *Non-Color Data* so
that no color space conversion happens on them.


.. _render-post-color-management:

Render Settings
===============

.. reference::

   :Editor:    Properties
   :Panel:     :menuselection:`Render Properties --> Color Management`

.. figure:: /images/render_color-management_panel.png
   :align: right

   Color Management properties.

These are color management settings that are used across Blender.
These color management settings are Scene specific so settings can be customized per Scene.
Color management can also be overridden when saving images;
this behavior can be set in the :ref:`Output Color Management Properties <render-output-color-management-panel>`.

.. _bpy.types.ColorManagedDisplaySettings.display_device:

Display Device
   The color space for the display that Blender is being viewed on.

   Most displays are sRGB by default with some newer displays having the option to use Rec. 2020.
   These displays have a wider color gamut and can display high dynamic range content.
   If you have an Apple display you probably will want to use Display P3.

   It is important to check your OS and display setting to make sure
   they all match the display in use to view the most accurate image.

   :sRGB: Used by most displays.
   :Display P3: Used by most Apple devices.
   :Rec. 1886: Used by many older TVs.
   :Rec. 2020: Used for newer wide gamut HDR displays.

.. _bpy.types.ColorManagedViewSettings.view_transform:

View Transform
   These are different ways to view the image on the same display device.

   :Standard:
      Does no extra conversion besides the conversion for the display device. Often used for
      non-photorealistic results or video editing where a specific look is already baked into
      the input video.
   :AgX:
      A tone mapping transform that improves on *Filmic*, giving more photorealistic results.
      AgX offers 16.5 stops of dynamic range and unsaturates highly
      exposed colors to mimic film's natural response to light.
   :Filmic:
      A tone mapping transform designed to handle high dynamic range colors.
      Filmic is deprecated and is superseded by AgX which improves handling of saturated colors.
   :Filmic Log:
      Converts to Filmic log color space. This can be used for export to color grading applications,
      or to inspect the image by flattening out very dark and light areas.
   :False Color:
      Shows a heat map of image intensities, to visualize the dynamic range, and help properly expose an image.

      Below is a table that represents how normalized linear color data is represented with False Color.

      .. list-table::
         :header-rows: 1

         * - Luminance Value
           - Color
         * - Low Clip
           - Black
         * - 0.0001% to 0.05%
           - Blue
         * - 0.05% to 0.5%
           - Blue-Cyan
         * - 0.5% to 5%
           - Cyan
         * - 5% to 16%
           - Green-Cyan
         * - 16% to 22%
           - Grey
         * - 22% to 35%
           - Green-Yellow
         * - 35% to 55%
           - Yellow
         * - 55% to 80%
           - Orange
         * - 80% to 97%
           - Red
         * - High Clip
           - White
   :Raw:
      Intended for inspecting the image but not for final export.
      Raw gives the image without any color space conversion.

.. _bpy.types.ColorManagedViewSettings.look:

Look
   Choose an artistic effect from a set of measured film response data
   which roughly emulates the look of certain film types. Applied before color space conversion.

.. _bpy.types.ColorManagedViewSettings.exposure:

Exposure
   Used to control the image brightness (in stops) applied before color space conversion.
   It is calculated as follows: :math:`output\_value = render\_value × 2^{(exposure)}`

.. _bpy.types.ColorManagedViewSettings.gamma:

Gamma
   Extra gamma correction applied after color space conversion.
   Note that the default display transforms already perform the appropriate conversion,
   so this mainly acts as an additional effect for artistic tweaks.

.. _bpy.types.ColorManagedSequencerColorspaceSettings.name:

Sequencer
   The color space that the :doc:`Sequencer </editors/video_sequencer/index>` operates in.
   By default, the Sequencer operates in sRGB space,
   but it can also be set to work in Linear space like the Compositing nodes, or another color space.
   Different color spaces will give different results for color correction, crossfades, and other operations.

   The list of color spaces depends on the active :ref:`OCIO config <ocio-config>`.
   The default supported color spaces are described in detail here:
   :ref:`Default OpenColorIO Configuration <ocio-config-default-color-spaces>`


Display
-------

High Dynamic Range
   Enable high dynamic range display in rendered viewport, uncapping display brightness.
   This requires a monitor with HDR support and a view transform designed for HDR
   (Filmic does not generate HDR colors).

   This feature is currently only supported on macOS.


.. _bpy.types.ColorManagedViewSettings.use_curve_mapping:

Use Curves
----------

Adjust RGB Curves to control the image colors before the color space conversion.
Read more about using the :ref:`ui-curve-widget`.


Image Files
===========

When working with image files, the default color space is usually the right one.
If this is not the case, the color space of the image file can be configured in the image settings.
A common situation where manual changes are needed is when working with or baking normal maps or displacement maps,
for example. Such maps do not actually store colors, just data encoded as colors.
Those images should be marked as *Non-Color Data*.

Image data-blocks will always store float buffers in memory in the scene linear color space,
while a byte buffer in memory and files in a drive are stored in the specified
:ref:`color space <bpy.types.ColorManagedInputColorspaceSettings.name>` setting.

By default, only renders are displayed and saved with the render *View Transformation* applied.
These images are the "Render Result" and "Viewer" image data-blocks,
and the files saved directly to a drive with the Render Animation operator.
However, when loading a render saved to an intermediate OpenEXR file,
Blender cannot detect automatically that this is a render
(it could be e.g. an image texture or displacement map).
We need to specify that this is a render and that we want the transformations applied,
with these two settings:

View as Render
   Display the image data-block (not only renders) with view transform, exposure, gamma, RGB curves applied.
   Useful for viewing rendered frames in linear OpenEXR files the same as when rendering them directly.
Save as Render
   Option in the image save operator to apply the view transform, exposure, gamma, RGB curves.
   This is useful for saving linear OpenEXR to e.g. PNG or JPEG files in display space.


.. _ocio-config:

OpenColorIO Configuration
=========================

Blender comes with a standard OpenColorIO configuration that
contains a number of useful display devices and view transforms.
The reference linear :term:`Color Space` used is the linear color space
with Rec. 709 chromaticities and D65 white point.

However, OpenColorIO is also designed to give a consistent user experience across
`multiple applications <https://opencolorio.org/#supported_apps>`__,
and for this, a single shared configuration file can be used.
Blender will use the standard OCIO environment variable to read an OpenColorIO configuration
other than the default Blender one. More information about how to set up such a workflow
can be found on the `OpenColorIO website <https://opencolorio.org/>`__.

Blender currently use the following color space rules:

``scene_linear``
   Color space used for rendering, compositing, and storing all float precision images in memory.
``data``
   Color space for non-color data.
``aces_interchange``
   ACES2065-1 color space. Used to derive chromaticities of the *scene_linear* color space, for
   effects such as blackbody emission.
``color_picking``
   Defines the distribution of colors in color pickers. It is expected to
   be approximately perceptually linear, have the same gamut as the *scene_linear* color space,
   map 0..1 values to 0..1 values in the scene linear color space for predictable editing of materials' albedo.
``default_sequencer``
   Default color space for the Sequencer, *scene_linear* if not specified.
``default_byte``
   Default color space for byte precision images and files, *texture_paint* if not specified.
``default_float``
   Default color space for float precision images and files, *scene_linear* if not specified.

The standard Blender configuration includes support for saving and loading images in
`ACES <https://www.oscars.org/science-technology/sci-tech-projects/aces>`__
(`code and documentation <https://github.com/ampas/aces-dev>`__) color spaces.
However, the ACES gamut is larger than the Rec. 709 gamut, so for best results,
an ACES specific configuration file should be used. OpenColorIO provides
an `ACES configuration <https://opencolorio.readthedocs.io/en/latest/configurations/_index.html>`__ file,
though it may need a few more tweaks to be usable in production.


Default OpenColorIO Configuration
=================================

.. _ocio-config-default-color-spaces:

Color Spaces
   Blender's OCIO configuration file is equipped by default to read/write files in these color spaces:

   :sRGB: Standard RGB display space using Rec. 709 chromaticities and a D65 white point.
   :Rec.2020: BT.2020 2.4 Exponent EOTF Display.
   :Rec.1886: BT.1886 2.4 Exponent EOTF Display, commonly used for TVs.
   :Non-Color: Generic data that is not color, will not apply any color transform (e.g. normal maps).
   :Linear Rec.709: Linear BT.709 chromaticities with illuminant D65 white point.
   :Linear Rec.2020: Linear BT.2020 with illuminant D65 white point.
   :Linear FilmLight E-Gamut: Linear E-Gamut with illuminant D65 white point.
   :Linear DCI-P3 D65: Linear DCI-P3 with illuminant D65 white point.
   :Linear CIE-XYZ E: 1931 CIE XYZ standard with assumed illuminant E white point.
   :Linear CIE-XYZ D65: 1931 CIE XYZ with adapted illuminant D65 white point.
   :Filmic sRGB: Similar to *sRGB* but uses the Filmic view transform.
   :Filmic Log: Intermediate log color space of Filmic view transform.
   :Display P3: Apple's Display P3 with sRGB compound (piece-wise) encoding transfer function, common on Mac devices.
   :ACEScg:
      An ACES color space that is designed to be used for rendering and compositing.
      It uses the AP1 color primaries, a D60 white point, and a linear transfer function.
      While similar to ACES2065-1, this color space has a smaller color gamut.
      The smaller gamut allow it to better represent the colors that fit inside the CIE 1931 chromaticities diagram.
      Colors that lie outside the CIE 1931 chromaticities are generally not important to rendering and compositing
      because the human stimulus cannot represent these colors.
   :ACES2065-1:
      An ACES color space using the AP0 color primaries, a D60 white point and a linear transfer function.
      This color space is meant to store and transfer data with the most amount of possible color information.


## Index

.. _render-index:
.. _bpy.types.Render:
.. _bpy.types.RenderSettings:
.. _bpy.ops.render:

#############
  Rendering
#############

.. toctree::
   :maxdepth: 2

   introduction.rst
   eevee/index.rst
   cycles/index.rst
   workbench/index.rst
   cameras.rst
   lights/index.rst
   materials/index.rst
   shader_nodes/index.rst
   color_management.rst
   freestyle/index.rst
   layers/index.rst
   output/index.rst


## Introduction


************
Introduction
************

Rendering is the process of turning a 3D scene into a 2D image.
Blender includes three render engines with different strengths:

- :doc:`EEVEE </render/eevee/index>` is a physically based realtime renderer.
- :doc:`Cycles </render/cycles/index>` is a physically based path tracer.
- :doc:`Workbench </render/workbench/index>` is designed for layout, modeling and previews.

More renderers from third-party developers are available as
:doc:`add-ons </editors/preferences/extensions>`.
Each renderer has its own render settings to control render quality and performance.

What the render looks like is defined by :doc:`cameras </render/cameras>`,
:doc:`lights </render/lights/index>` and :doc:`materials </render/materials/index>`.
These are shared between EEVEE and Cycles, however some features are only supported in one or the other.

Renders can be split up into :doc:`layers and passes </render/layers/index>`, which can then
be :doc:`composited </compositing/index>` together for creative control, or to combine
with real footage. :doc:`Freestyle </render/freestyle/index>` can be used to
add non-photorealistic line rendering.

Blender supports interactive 3D viewport rendering for all render engines, for quick iteration
on lighting and shading. Once this is done, the final quality image or animation can
be rendered and :doc:`output </render/output/index>`.


## Baking

.. _bpy.types.BakeSettings:

*************
Render Baking
*************

Cycles shaders and lighting can be baked to image textures.
This has a few different purposes, most commonly:

- Baking textures like base color or normal maps for export to game engines.
- Baking ambient occlusion or procedural textures,
  as a base for texture painting or further edits.
- Creating light maps to provide global illumination or speed up rendering in games.


Setup
=====

Baking requires a mesh to have a UV map, and either a Color Attribute
or an Image Texture node with an image to be baked to.
The :term:`Active` :doc:`Image Texture </render/shader_nodes/textures/image>`
node or :doc:`Color Attribute </sculpt_paint/vertex_paint/index>` is used as the baking target.

Use Render Bake in intensive light/shadow solutions,
such as AO or soft shadows from area lights. If you bake AO for the main objects,
you will not have to enable it for the full render, saving render time.
Cycles uses the render settings (samples, bounces, ...) for baking.
This way the quality of the baked textures should match the result you get from the rendered scene.


Settings
========

.. reference::

   :Panel:     :menuselection:`Render --> Bake`

.. _bpy.ops.object.bake:

Bake
   Perform the baking operation.

.. _bpy.types.RenderSettings.use_bake_multires:

Bake from Multires
   Bake directly from multires object.

Bake Type
   Type of pass to bake.

   :Combined:
      Bakes all materials, textures, and lighting except specularity.
      The passes that contribute to the combined pass can be toggled individually to form the final map.
   :Ambient Occlusion:
      Bakes ambient occlusion as specified in the World panels. Ignores all lights in the scene.
   :Shadow:
      Bakes shadows and lighting.
   :Normal:
      Bakes normals to an RGB image.
   :UV:
      Mapped UV coordinates, used to represent where on a mesh a texture gets mapped too.
      This is represented through the red and green channels of the image,
      the blue channel is encoded with a constant value of 1 but does not hold any information.
   :Roughness:
      Bakes the roughness pass of a material.
   :Emit:
      Bakes Emission, or the Glow color of a material.
   :Environment:
      Bakes the environment (i.e. the world surface shader defined for the scene) onto
      the selected object(s) as seen by rays cast from the world origin.
   :Diffuse:
      Bakes the diffuse pass of a material.
   :Glossy:
      Bakes the glossiness pass of a material.
   :Transmission:
      Bakes the transmission pass of a material.

.. _bpy.types.BakeSettings.view_from:

View From
   Source of reflection ray directions.

   :Above Surface: Cast rays from above the surface.
   :Active Camera: Use the active camera's position to cast rays.


Influence
---------

.. rubric:: Combined

.. _bpy.types.BakeSettings.use_pass_direct:

Lighting
   Direct
      Add direct lighting contribution.

   .. _bpy.types.BakeSettings.use_pass_indirect:

   Indirect
      Add indirect lighting contribution.

Contributions
   .. _py.types.BakeSettings.use_pass_diffuse:

   Diffuse
      Add diffuse contribution.

   .. _bpy.types.BakeSettings.use_pass_glossy:

   Glossy
      Add glossy contribution.

   .. _bpy.types.BakeSettings.use_pass_transmission:

   Transmission
      Add transmission contribution.

   .. _bpy.types.BakeSettings.use_pass_ambient_occlusion:

   Ambient Occlusion
      Add ambient occlusion contribution.

   .. _bpy.types.BakeSettings.use_pass_emit:

   Emit
      Add emission contribution.


.. rubric:: Diffuse, Glossy, Transmission

Contributions
   Direct
      See :ref:`above <bpy.types.BakeSettings.use_pass_direct>`.
   Indirect
      See :ref:`above <bpy.types.BakeSettings.use_pass_indirect>`.

   .. _bpy.types.BakeSettings.use_pass_color:

   Color
      Colorize the pass.

      - If only *Color* is selected you get the pass color,
        which is a property of the surface and independent of sampling refinement.
      - If *Color* is not selected, you get the direct and/or indirect contributions in gray-scale.
      - If *Color* and either *Direct* or *Indirect* are selected,
        you get the direct and/or indirect contributions colored.


.. rubric:: Normal

.. _bpy.types.BakeSettings.normal_space:

Space
   Normals can be baked in different spaces:

   For materials, the same spaces can be chosen in the image texture options
   next to the existing *Normal Map* setting. For correct results,
   the setting here should match the setting used for baking.

   :Object:
      Normals in object coordinates, independent of object transformation, but dependent on deformation.
   :Tangent:
      Normals in tangent space coordinates, independent of object transformation and deformation.
      This is the default, and the right choice in most cases, since then the normal map can be used for
      animated objects too.

.. _bpy.types.BakeSettings.normal_r:
.. _bpy.types.BakeSettings.normal_g:
.. _bpy.types.BakeSettings.normal_b:

Swizzle R, G, B
   Axis to bake into the red, green and blue channel.


.. _bpy.types.BakeSettings.use_selected_to_active:

Selected to Active
------------------

Bake shading on the surface of selected objects to the active object.
The rays are cast from the low-poly object inwards towards the high-poly object.
If the high-poly object is not entirely involved by the low-poly object, you can tweak the rays start point with
*Max Ray Distance* or *Extrusion* (depending on whether or not you are using cage).
For even more control you can use a *Cage Object*.

.. note::

   There is a CPU fixed memory footprint for every object used to bake from.
   In order to avoid crashes due to lack of memory, the high-poly objects can be joined before the baking process.

.. _bpy.types.BakeSettings.use_cage:

Cage
   Cast rays to active object from a cage.
   A cage is a ballooned-out version of the low-poly mesh created either automatically
   (by adjusting the ray distance) or manually (by specifying an object to use).
   When not using a cage the rays will conform to the mesh normals. This produces glitches on the edges,
   but it is a preferable method when baking into planes to avoid the need of adding extra loops around the edges.

   .. _bpy.types.BakeSettings.cage_object:

   Cage Object
      Object to use as cage instead of calculating the cage from the active object with the *Cage Extrusion*.

.. _bpy.types.BakeSettings.cage_extrusion:

Cage Extrusion
   Distance to use for the inward ray cast when using *Selected to Active* and *Cage*.
   The inward rays are cast from a version of the active object with disabled Edge Split Modifiers.
   Hard splits (e.g. when the Edge Split Modifier is applied) should be avoided because they will lead to non-smooth
   normals around the edges.

   .. note::

      When the base mesh extruded does not give good results,
      you can create a copy of the base mesh and modify it to use as a *Cage*.
      Both meshes need to have the same :term:`Topology` (number of faces and face order).

.. _bpy.types.BakeSettings.max_ray_distance:

Max Ray Distance
   Distance to use for the inward ray cast when using *Selected to Active*.
   Ray distance is only available when not using *Cage*.


Output
------

.. _bpy.types.BakeSettings.target:

Target
   Where to output the baked map.

   :Image Textures:
      Bake to the image data-block associated with the :term:`Active`
      :doc:`Image Texture </render/shader_nodes/textures/image>` node.

      .. _bpy.types.BakeSettings.use_clear:

      Clear Image
         If selected, clears the image before baking render.

   :Color Attributes:
      Bake to the :term:`Active` :doc:`Color Attributes </sculpt_paint/vertex_paint/index>` layer on the active mesh.
      Note, the active object must be a mesh as other object types do not have Color Attributes.

Margin
------

When baking to images, by default a margin is generated around UV "islands".
This is important to avoid discontinuities at UV seams, due to texture filtering and mip-mapping.

.. _bpy.types.BakeSettings.margin_type:

Type
   Method to generate the margin.

   :Extend:
      Extend border pixels outwards.

   :Adjacent Faces:
      Fill margin using pixels from adjacent faces across UV seams.

.. _bpy.types.BakeSettings.margin:

Size
   Size of the margin in pixels.


## Features

.. |tick|  unicode:: U+2714
.. |cross| unicode:: U+2717

*********************
Experimental Features
*********************

.. reference::

   :Panel:     :menuselection:`Render`

.. _cycles-experimental-features:

Experimental features are disabled / hidden by default,
but can be enabled by setting *Feature Set* to *Experimental* in the Render properties.
Enabling the *Experimental Feature Set* will use experimental
and incomplete features that might be broken or change in the future.

Adaptive subdivision is currently the only experimental feature.


## Gpu Rendering

*************
GPU Rendering
*************

:abbr:`GPU (Graphics Processing Unit)` rendering makes it possible to use your
graphics card for rendering, instead of the CPU. This can speed up rendering
because modern GPUs are designed to do quite a lot of number crunching.
On the other hand, they also have some limitations in rendering complex scenes, due to more limited memory,
and issues with interactivity when using the same graphics card for display and rendering.

To enable GPU rendering, go into the :menuselection:`Preferences --> System --> Cycles Render Devices`,
and select either *CUDA*, *OptiX*, *HIP*, *oneAPI*, or *Metal*. Next, you must configure each scene to
use GPU rendering in :menuselection:`Properties --> Render --> Device`.


Rendering Technologies
======================

Blender supports different technologies to render on the GPU depending on the particular GPU manufacturer
and operating system.

CUDA -- NVIDIA
--------------

:abbr:`CUDA (Compute Unified Device Architecture)` is supported on Windows and Linux and requires a
Nvidia graphics cards with compute capability 3.0 and higher. To make sure your GPU is supported,
see the `list of Nvidia graphics cards <https://developer.nvidia.com/cuda-gpus#compute>`__
with the compute capabilities and supported graphics cards.


.. _render-cycles-gpu-optix:

OptiX -- NVIDIA
---------------

OptiX is supported on Windows and Linux and requires a Nvidia graphics cards with compute capability 5.0 and higher
and a driver version of at least 470. To make sure your GPU is supported,
see the `list of Nvidia graphics cards <https://developer.nvidia.com/cuda-gpus#compute>`__.

OptiX takes advantage of hardware ray-tracing acceleration in RTX graphics cards, for improved performance.

GPU acceleration for OpenImageDenoise is available for compute capability 7.0 and higher, which includes
all NVIDIA RTX cards.


HIP -- AMD
----------

:abbr:`HIP (Heterogeneous-compute Interface for Portability)` is supported on Windows and Linux and requires a
AMD graphics card with the Vega architecture or newer. Both discrete GPUs and APUs are supported.

Supported GPUs include:

 - Radeon RX Vega Series (Excluding the Radeon VII)
 - Radeon RX 5000 Series
 - Radeon RX 6000 Series
 - Radeon RX 7000 Series
 - Radeon Pro WX 9100
 - Radeon Pro W6000 Series

Minimum driver versions:

 - Windows: Radeon Software 21.12.1 or Radeon PRO Software 21.Q4
 - Linux: Radeon Software 22.10 or ROCm 5.3

Please refer to `AMD's website <https://www.amd.com/en/products/specifications>`__ for more
information about AMD graphics cards and their architectures.

On Windows, experimental hardware ray-tracing support is available with the most recent drivers.
This can be enabled in the preferences.
However there are currently known issues regarding motion blur, hair rendering and degenerate triangle shapes.

GPU accelerated denoising is available on discrete Radeon RX 6000 and Radeon RX 7000 GPUs.


oneAPI -- Intel
---------------

oneAPI is a computation library that is supported on Windows and Linux and requires a
Intel® Arc™ graphics card with the Xe HPG architecture.
Hardware acceleration for ray-tracing and denoising is supported.

Supported GPUs include:

 - Intel® Arc™ A-Series

Minimum driver versions:

 - Windows: Intel Graphics Driver XX.X.101.5518
 - Linux: ``intel-level-zero-gpu`` package 1.3.27642,
   typically available through the ``intel-compute-runtime`` package XX.XX.27642.38

Please refer to `Intel's website <https://www.intel.com/content/www/us/en/products/details/discrete-gpus.html>`__
for more information about Intel graphics cards and their architectures.

GPU accelerated denoising is available on all supported GPUs.


Metal -- Apple (macOS)
----------------------

Metal is supported on Apple computers with Apple Silicon, AMD and Intel graphics cards.
macOS 13.0 or newer is required to support all features and graphics cards.

Using AMD graphics cards with Metal has a number of limitations. :ref:`Light Trees
<bpy.types.CyclesRenderSettings.use_light_tree>` and :ref:`Shadow Caustics
<bpy.types.CyclesObjectSettings.is_caustics_caster>` are not supported, and the Principled Hair BSDF causes poor
rendering performance.

GPU accelerated denoising is available on Apple Silicon.


Limitations
===========

 - :ref:`Path Guiding <bpy.types.CyclesRenderSettings.use_guiding>` is not supported on any GPU.
 - :doc:`/render/shader_nodes/osl` is only supported for OptiX, with some limitations listed in the documentation.


Frequently Asked Questions
==========================

Why is Blender unresponsive during rendering?
---------------------------------------------

On older GPU generations, graphics cards can only either render or draw the user interface.
This can make Blender unresponsive while it is rendering.
Heavy scenes can also make Blender unresponsive on newer GPUs,
when using a lot of memory or executing expensive shaders, however this is generally less of a problem.

The only complete solution for this is to use a dedicated GPU for rendering, and another for display.


Why does a scene that renders on the CPU not render on the GPU?
---------------------------------------------------------------

There may be multiple causes,
but the most common one is that there is not enough memory on your graphics card.
Typically, the GPU can only use the amount of memory that is on the GPU
(see `Would multiple GPUs increase available memory?`_ for more information).
This is usually much smaller than the amount of system memory the CPU can access.
With CUDA, OptiX, HIP and Metal devices, if the GPU memory is full Blender will automatically
try to use system memory. This has a performance impact, but will usually still result in a faster render
than using CPU rendering.


Can multiple GPUs be used for rendering?
----------------------------------------

Yes, go to :menuselection:`Preferences --> System --> Compute Device Panel`, and configure it as you desire.


Would multiple GPUs increase available memory?
----------------------------------------------

Typically, no, each GPU can only access its own memory.

The exception is NVIDIA GPUs connected with NVLink, where multiple GPUs can share memory at a small performance cost.
This is can be enabled with :ref:`Distributed Memory Across Devices <prefs-system-cycles-distributive-memory>`
in the preferences.


What renders faster?
--------------------

This varies depending on the hardware used. Different technologies also have different compute times
depending on the scene tested. For the most up to date information on the performance of different devices,
browse the `Blender Open Data <https://opendata.blender.org/>`__ resource.


Error Messages
==============

In case of problems, be sure to install the official graphics drivers from the GPU manufacturers website,
or through the package manager on Linux.
The graphics drivers provided by the computer manufacturer can sometimes be outdated or incomplete.


Error: Out of memory
--------------------

This usually means there is not enough memory to store the scene for use by the GPU.

.. note::

   One way to reduce memory usage is by using smaller resolution textures.
   For example, 8k, 4k, 2k, and 1k image textures take up respectively 256MB, 64MB, 16MB and 4MB of memory.


The NVIDIA OpenGL driver lost connection with the display driver
----------------------------------------------------------------

If a GPU is used for both display and rendering,
Windows has a limit on the time the GPU can do render computations.
If you have a particularly heavy scene, Cycles can take up too much GPU time.
Reducing Tile Size in the Performance panel may alleviate the issue,
but the only real solution is to use separate graphics cards for display and rendering.

Another solution can be to increase the time-out,
although this will make the user interface less responsive when rendering heavy scenes.
`Learn More Here <https://learn.microsoft.com/en-us/windows-hardware/drivers/display/timeout-detection-and-
recovery>`__.


Unsupported GNU version
-----------------------

On Linux, depending on your GCC version you might get this error.
See the `Nvidia CUDA Installation Guide for Linux
<https://docs.nvidia.com/cuda/archive/10.2/cuda-installation-guide-linux/index.html>`__
for a list of supported GCC versions. There are two possible solutions to this error:

Use an alternate compiler
   If you have an older GCC installed that is compatible with the installed CUDA toolkit version,
   then you can use it instead of the default compiler.
   This is done by setting the ``CYCLES_CUDA_EXTRA_CFLAGS`` environment variable when starting Blender.

   Launch Blender from the command line as follows:

   .. code-block:: sh

      CYCLES_CUDA_EXTRA_CFLAGS="-ccbin gcc-x.x" blender

   (Substitute the name or path of the compatible GCC compiler).

Remove compatibility checks
   If the above is unsuccessful, delete the following line in
   ``/usr/local/cuda/include/host_config.h``:

   .. code-block:: c

      #error -- unsupported GNU version! gcc x.x and up are not supported!

   This will allow Cycles to successfully compile the CUDA rendering kernel the first time it
   attempts to use your GPU for rendering. Once the kernel is built successfully, you can
   launch Blender as you normally would and the CUDA kernel will still be used for rendering.


CUDA Error: Kernel compilation failed
-------------------------------------

This error may happen if you have a new NVIDIA graphics card that is not yet supported by
the Blender version and CUDA toolkit you have installed.
In this case Blender may try to dynamically build a kernel for your graphics card and fail.

In this case you can:

#. Check if the latest Blender version
   (official or `experimental builds <https://builder.blender.org/download/>`__)
   supports your graphics card.
#. If you build Blender yourself, try to download and install a newer CUDA developer toolkit.

Normally users do not need to install the CUDA toolkit as Blender comes with precompiled kernels.


## Index

.. _bpy.types.Cycles:
.. _bpy.ops.cycles:

##########
  Cycles
##########

.. toctree::
   :titlesonly:
   :maxdepth: 2

   introduction.rst
   render_settings/index.rst
   world_settings.rst
   object_settings/index.rst
   material_settings.rst
   light_settings.rst
   gpu_rendering.rst
   features.rst
   baking.rst
   optimizations/index.rst


## Introduction


************
Introduction
************

.. figure:: /images/render_cycles_introduction_overview.jpg

Cycles is Blender's physically-based path tracer for production rendering.
It is designed to provide physically based results out-of-the-box,
with artistic control and flexible shading nodes for production needs.

To use Cycles, select it as the *Render Engine* in the Render properties.
For :doc:`GPU accelerated rendering </render/cycles/gpu_rendering>`,
enable compatible devices in :menuselection:`Preferences --> System --> Cycles Render Devices`.

.. seealso::

   The `Cycles website <https://www.cycles-renderer.org/>`__ with more information and a gallery.


## Light Settings


**************
Light Settings
**************

.. reference::

   :Panel:     :menuselection:`Properties --> Light` and :menuselection:`Shader Editor --> Sidebar --> Settings`

Next to lighting from the background and any object with an emission shader,
lights are another way to add light into the scene.
The difference is that they are not directly visible in the rendered image,
and can be more easily managed as objects of their own type.


Common
======

:doc:`Light settings </render/lights/light_object>` for all renderers.


Cycles
======

.. _bpy.types.CyclesLightSettings.max_bounces:

Max Bounces
   Maximum number of times light from the light is allowed to :term:`Bounce <Light Bounces>`.
   Limited by :ref:`scene-wide bounce settings <bpy.types.CyclesRenderSettings.max_bounces>`.

.. _bpy.types.CyclesLightSettings.cast_shadow:

Cast Shadow
   By disabling this option, light from lights will not be blocked by objects in between.
   This can speed up rendering by not having to trace rays to the light source.

.. _bpy.types.CyclesLightSettings.use_multiple_importance_sampling:

Multiple Importance Sample
   By default lights use only direct light sampling. For area lights and sharp glossy reflections, however,
   this can be noisy,
   and enabling this option will enable indirect light sampling to be used in addition to reduce noise.

.. _bpy.types.CyclesLightSettings.is_caustics_light:

Shadow Caustics
   Mark a light as a refractive caustic caster. This setting can be used in conjunction with the
   :ref:`Cast and Receive caustics object settings <bpy.types.CyclesObjectSettings.is_caustics_caster>`
   to selectively speed up refractive caustic rendering of select objects.


Area Lights
===========

.. _render-cycles-lights-area-portals:

Portals
   Area lights can also function as light portals to help sample the environment light,
   and significantly reduce noise in interior scenes.
   Note that rendering with portals is usually slower, but as it converges more quickly, less samples are required.

   Light portals work by enabling the *Portal* option, and placing areas lights in
   windows, door openings, and any place where light will enter the interior.

   In outdoor scenes most rays do not bounce much and just fly off into the sky and therefore,
   light portals are not helpful for outdoor scenes.

   .. figure:: /images/render_cycles_light-settings_portals2.jpg
   .. figure:: /images/render_cycles_light-settings_portals.jpg

      White Room model by Jay Hardy.


Beam Shape
----------

.. _bpy.types.AreaLight.spread:

Spread
   How wide the emitted light fans out controlling how diffused the light is.
   Larger values create soft shadows while smaller values create sharper light
   simulating a `gridded softbox <https://en.wikipedia.org/wiki/Softbox>`__.

   .. figure:: /images/render_lights_light-object-area-spread.png

      Example of Spread at different angles.


## Material Settings

.. _bpy.types.CyclesMaterialSettings:

*****************
Material Settings
*****************

.. reference::

   :Panel:     :menuselection:`Material --> Settings` and :menuselection:`Shader Editor --> Sidebar --> Settings`


Surface
=======

.. _bpy.types.Material.displacement_method:

Displacement
   Method used to perform :doc:`Displacement </render/materials/components/displacement>` on materials.

   :Displacement Only:
      Mesh vertices will be displaced before rendering, modifying the actual mesh.
      This gives the best quality results, if the mesh is finely subdivided.
      As a result, this method is also the most memory intensive.
   :Bump Only:
      When executing the surface shader, a modified surface normal is used instead of the true normal.
      This is a less memory intensive alternative to actual displacement, but only an approximation.
      Surface silhouettes will not be accurate and there will be no self-shadowing of the displacement.
   :Displacement and Bump:
      Both methods can be combined, to do displacement on a coarser mesh,
      and use bump mapping for the final detail.

.. _bpy.types.CyclesMaterialSettings.emission_sampling:

Emission Sampling
   The method used for sampling the emissive component of the material.
   This option will only have an influence if the material contains an emissive material node,
   otherwise it will be ignored.

   :None:
      Do not use this surface as a light for sampling.
   :Auto:
      Automatically determine if the surface should be treated as a light for sampling based on
      emission intensity.
   :Front:
      Treat only the front side of the surface as a light, useful for closed meshes whose interior
      is not visible.
   :Back:
      Treat only the back side of the surface as a light for sampling.
   :Front and Back:
      Treat surface as a light for sampling, emitting from both the front and back side.

.. _bpy.types.CyclesMaterialSettings.use_transparent_shadow:

Transparent Shadows
   Use transparent shadows if it contains a :doc:`Transparent BSDF </render/shader_nodes/shader/transparent>`,
   disabling will render faster but will not give accurate shadows.

.. _bpy.types.CyclesMaterialSettings.use_bump_map_correction:

Bump Map Correction
   Applies corrections to solve shadow terminator artifacts caused by bump mapping.


Volume
======

.. _bpy.types.CyclesMaterialSettings.volume_sampling:

Sampling Method
   :Distance: For dense volumes lit from far away *Distance* sampling is usually more efficient.
   :Equiangular: If you have got a light inside or near the volume then *equiangular* sampling is better.
   :Multiple Importance: If you have a combination of both, then the multiple importance sampling will be better.

.. _bpy.types.CyclesMaterialSettings.volume_interpolation:

Interpolation
   Interpolation method to use for the volume objects and smoke simulation grids.

   :Linear: Simple interpolation which gives good results for thin volumes.
   :Cubic: Smoothed high-quality interpolation needed for more dense volumes, but slower.

.. _bpy.types.CyclesMaterialSettings.homogeneous_volume:

Homogeneous
   Assume volume has the same density everywhere (not using any textures), for faster rendering.
   For example absorption in a glass object would typically not have any textures,
   and so the renderer can be set to avoid taking small steps to sample the volume shader.
   Usually this is automatically determined by the renderer.
   This setting provides a manual control for cases where it is not detected.

.. _bpy.types.CyclesMaterialSettings.volume_step_rate:

Step Rate
   Adjust distance between volume shader samples for volume shaders.
   This is typically used to reduce the step size for procedural shaders that add more detail
   with procedural textures, when it is not captured by the default step size.
   See :doc:`Volume Render Settings </render/cycles/render_settings/volumes>` for more information.


## World Settings

.. _bpy.types.CyclesWorldSettings:
.. _bpy.types.WorldLighting:

**************
World Settings
**************

Mist Pass
=========

.. reference::

   :Panel:     :menuselection:`World --> Mist Pass`

.. note::

   The mist pass must be enabled in the View Layer tab
   of the :doc:`Properties Editor </editors/properties_editor>`
   before the settings below are available in the World tab.

Mist can greatly enhance the illusion of depth in your rendering. To create mist,
Blender generates a render layer with a depth map ranging between 0.0 and 1.0
that can be used in the Compositor to generate a mist effect.

Start
   The distance from the camera at which the mist starts to fade in.

Depth
   The distance from *Start* of the mist, that it fades in over.
   Objects further from the camera than *Start + Depth* are completely hidden by the mist.

Falloff
   The curve function that controls the rate of change of the mist's strength further and further into the distance.

   :Quadratic:
      Uses the same calculation as light falloff (:math:`1\over{x^2}`) and provides the smoothest
      transition from transparent (0.0) to opaque (1.0).
   :Linear: Has a steeper start than quadratic (:math:`1\over{x}`).
   :Inverse Quadratic:
      Has the steepest start (:math:`1\over{\sqrt{x}}`) and approaches 1.0 faster than the other two functions.

.. tip::

   A visualization can be activated in the :menuselection:`Camera --> Viewport Display` panel.

.. figure:: /images/render_cycles_world-settings_mist-example1-BI.jpg

   Mist example
   (`blend-file <https://archive.blender.org/wiki/2015/index.php/File:25-Manual-World-Mist-Example1.blend>`__).


.. _bpy.types.CyclesVisibilitySettings.camera:

Ray Visibility
==============

.. reference::

   :Panel:     :menuselection:`World --> Ray Visibility`

As with other objects,
*Ray Visibility* allows you to control which other shaders can "see" the environment.


Tricks
------

Sometimes it may be useful to have a different background that is directly visible versus one
that is indirectly lighting the objects. A simple solution to this is to add a Mix node,
with the *Blend Factor* set to *Is Camera Ray*. The first input color is then the indirect color,
and the second the directly visible color. This is useful when using a high-res image for
the background and a low-res image for the actual lighting.

Similarly, adding the *Is Camera* and *Is Glossy* rays will mean that the high-res image
will also be visible in reflections.

.. figure:: /images/render_cycles_world-settings_tricks.png

   Nodes for the trick above.


.. _render-cycles-integrator-world-settings:

Settings
========

.. reference::

   :Panel:     :menuselection:`World --> Settings`


Surface
-------

.. _bpy.types.CyclesWorldSettings.sampling_method:

Sampling
   Controls the sampling method for the world material. Selecting Auto or Manual enables
   *Multiple Importance Sampling* while None disables it. *Multiple Importance Sampling*
   is a method to sample the background texture such that lighter parts are favored,
   creating an importance map. It will produce less noise in the render in trade of artifacts (:term:`Fireflies`).
   Enable this when using an image texture with small area lights (like the sun),
   otherwise noise can take a long time to converge.

   Below is a comparison between *Multiple Importance Sample* off and on.
   Both images are rendered for 25 seconds (Off: 1,500 samples, On: 1,000 samples).

   .. list-table::

      * - .. figure:: /images/render_cycles_world-settings_mis-off.jpg

             Multiple Importance Sample off.

        - .. figure:: /images/render_cycles_world-settings_mis-on.jpg

             Multiple Importance Sample on.

.. _bpy.types.CyclesWorldSettings.sample_map_resolution:

Map Resolution
   Sets the resolution of the importance map.
   A higher resolution will better detect small features in the map and give more accurate sampling
   but conversely will take up more memory and render slightly slower.
   Higher values also may produce less noise when using high-res images.

.. _bpy.types.CyclesWorldSettings.max_bounces:

Max Bounces
   Maximal number of bounces the background light will contribute to the render.

.. seealso::

   See :doc:`Reducing Noise </render/cycles/optimizations/reducing_noise>`
   for more information on how to reduce noise.

.. _bpy.types.CyclesWorldSettings.is_caustics_light:

Shadow Caustics
   Mark the World Shader as a refractive caustic caster. This setting can be used in conjunction with the
   :ref:`Cast and Receive caustics object settings <bpy.types.CyclesObjectSettings.is_caustics_caster>`
   to selectively speed up refractive caustic rendering of select objects.


Volume
------

.. _bpy.types.CyclesWorldSettings.volume_sampling:

Sampling Method
   :Distance:
      For dense volumes lit from far away *Distance* sampling is more efficient in most cases.
      Usually this shouldn't be used for World volumes.
   :Equiangular:
      If you have got a light inside or near the volume then *equiangular* sampling is better.
   :Multiple Importance:
      If you have a combination of both, then the multiple importance sampling will be better.

.. _bpy.types.CyclesWorldSettings.volume_interpolation:

Interpolation
   Interpolation method to use for the volume.

   :Linear: Simple interpolation which gives good results for thin volumes.
   :Cubic: Smoothed high-quality interpolation needed for more dense volumes, but slower.

.. _bpy.types.CyclesWorldSettings.homogeneous_volume:

Homogeneous
   Assume volume has the same density everywhere (not using any textures), for faster rendering.
   Usually this is automatically determined by the renderer.
   This settings provides a manual control for cases where it is not detected.

.. _bpy.types.CyclesWorldSettings.volume_step_size:

Step Size
   Distance between volume shader samples for world volume shaders.
   See :doc:`Volume Render Settings </render/cycles/render_settings/volumes>` for more information.


Light Group
-----------

.. _bpy.types.World.lightgroup:

Light Group :guilabel:`Cycles only`
   Select the :ref:`Light Group <bpy.types.ViewLayer.active_lightgroup_index>` to add the
   current *World Surface Shader* too.

   Add Light Group
      If the name input into the *Light Group* field does not align with an existing
      Light Group, then pressing this button will create a *Light Group* with that name
      and assign this *World Shader* to it.


## Adaptive Subdiv

.. _render-cycles-settings-object-subdivision:

********************
Adaptive Subdivision
********************

.. reference::

   :Panel:     :menuselection:`Modifier --> Subdivision Surface`

.. note::

   Implementation not finished yet, marked as an :ref:`Experimental Feature Set <cycles-experimental-features>`.

When using the *Experimental Feature Set*
the :doc:`Subdivision Surface Modifier </modeling/modifiers/generate/subdivision_surface>`
gets changed to control the subdivision of a mesh at the time of rendering.
For this, all the other settings are the same except the *View* and *Render* settings.
These before mentioned settings get removed/renamed and the following settings are added:

.. _bpy.types.CyclesObjectSettings.use_adaptive_subdivision:

Adaptive Subdivision
   Use OpenSubdiv to give different subdivision levels to near and far objects automatically.
   This allows nearer objects to get more subdivisions and far objects to get less.

   .. _bpy.types.CyclesObjectSettings.dicing_rate:

   Dicing Scale
      Multiplier of the :ref:`scene dicing rate <bpy.types.CyclesRenderSettings.dicing_rate>`
      to determine the final size of :term:`Micropolygons` in pixels.

      .. figure:: /images/render_cycles_object-settings_adaptive-subdiv_displacement-dicing.png

         Subdivision off/on, Dicing Scale: 1.0 - 0.3 - 0.05 (monkeys look identical in viewport, no modifiers).


Known Limitations
=================

- Multi-user object data are currently made single users, leading to increased memory usage.
  For those it is better to use non-adaptive subdivision still.
- Multi-view renders can have some inconsistencies between views.

.. warning::

   Instances are not tessellated individually.
   Instead, the original object is tessellated and then duplicated on all instances.
   To take advantage of both adaptive subdivision and instancing you should place
   the original object at the position of the instance that is closest from the camera.


## Cameras


*******
Cameras
*******

.. _bpy.types.Camera.panorama_type:

Panoramic Cameras
=================

Cycles supports several types of panoramic cameras which are described in detail below.
Note that these cannot be displayed in non-rendered modes in the viewport,
i.e. *Solid* mode; they will only work for the final render.


Equirectangular
---------------

Render a panoramic view of the scenes from the camera location and use an equirectangular projection,
always rendering the full 360° over the X axis and 180° over the Y axis.

This projection is compatible with the environment texture as used for world shaders,
so it can be used to render an environment map. To match the default mapping,
set the camera object rotation to (90, 0, -90) or pointing along the positive X axis.
This corresponds to looking at the center of the image using the default environment texture mapping.

.. _bpy.types.Camera.latitude:

Latitude Min, Max
   Limits of the vertical field of view angles.

.. _bpy.types.Camera.longitude:

Longitude Min, Max
   Limits of the horizontal field of view angles.


Equiangular Cubemap Face
------------------------

Improves on *Equirectangular* by providing a more uniform distribution of rendered pixel of the spherical environment.
This results in an image that has little variation in visual resolution for the entire spherical projection.
This is in contrast to *Equirectangular* which can lose detail in the poles of the image.
This is also in contrast to cube map projections which lose detail near the edges of each face.

This panorama type is great for virtual reality use cases
where providing as much visual detail for a limited resolution is important.

A limitation over *Equirectangular* is that this method does not have longitude or latitude limits.


Fisheye
-------

Fisheye lenses are typically wide angle lenses with strong distortion,
useful for creating panoramic images for e.g. dome projection, or as an artistic effect.

The *Fisheye Equisolid* lens will best match real cameras.
It provides a lens focal length and field of view angle,
and will also take the sensor dimensions into account.

The *Fisheye Equidistant* lens does not correspond to any real lens model;
it will give a circular fisheye that does not take any sensor information into account
but rather uses the whole sensor. This is a good lens for full-dome projections.

.. _bpy.types.Camera.fisheye_lens:

Lens
   Lens focal length in millimeters.

.. _bpy.types.Camera.fisheye_fov:

Field of View
   Field of view angle, going to 360 and more to capture the whole environment.


Fisheye Lens Polynomial
-----------------------

Match a real world camera by specifying the coordinates of a 4th degree polynomial.

The projection works as follows.
Pixels in the image are mapped to positions :math:`(x, y)` on the camera sensor in mm.
A position on the sensor is mapped to a direction with spherical coordinates
:math:`(1, \theta, \phi)` in radians as follows:

.. math::

  & r = \sqrt{x^2 + y^2}\\
  & \theta = k_0 + k_1 r + k_2 r^2 + k_3 r^3 + k_4 r^4\\
  & \phi = acos(x/r)

Incoming light from this direction is then projected onto the corresponding pixel.

This can be used to model both fisheye and perspective cameras.


Mirror Ball
-----------

Render as if taking a photo of a reflective mirror ball.
This can be useful in rare cases to compare with a similar photo taken to capture an environment.


## Index


###################
  Object Settings
###################

Settings for objects and object data.

.. toctree::
   :maxdepth: 2

   object_data.rst
   light_linking.rst
   adaptive_subdiv.rst
   cameras.rst


## Light Linking


*************
Light Linking
*************

.. _bpy.types.Object.light_linking:

With light linking, lights can be set to affect only specific objects in the scene.
Shadow linking additionally gives control over which objects acts as shadow blockers for a light.

This adds more artistic control for lighting by breaking the laws of physics.
For example the environment and characters in a shot might have different light setups.
A character could have a dedicated linked rim light to make it stand out,
and shadow linking could be used to ensure no objects from the environment block it.

Setup
=====

* Select the light or emissive mesh object and go to the :ref:`Shading panel
  <render-cycles-object-light-linking-settings>`.
* Create a new light or shadowing linking collection.
* Drag & drop objects or collection from the outliner.

Links can also be set up in the 3D viewport, with the Link Data operator.
The active light object is linked to selected receiver or blocker object.

A light or shadow linking collection can be assigned to and shared between multiple light objects.
While a scene collection can be directly assigned as a light or shadow linking collection,
it is recommended to instead create a dedicated collection and link any scene collection inside it instead.
This way it's easy to include or exclude additional objects without affecting the scene layout.

Include & Exclude
=================

Light receiver objects can be set to be either included or excluded.
The behavior is as follows:

* If only included objects are specified, the light only affects those objects.
* If only excluded objects are specified, the light affects all objects in the scene except those specified.
* If both included and excluded objects are specified, the light affects only included objects minus the excluded
  objects. This can be used to for example set a character collection to be included, and then exclude specific
  objects part of the character.

Performance
===========

Sampling for light linking is most efficient with the light tree enabled,
where a specialized acceleration structure is built for light linking.

When using shadow linking, renders can be slower and trace additional rays,
as direct and indirect lighting take different paths.


## Object Data


******
Object
******

.. _render-cycles-object-settings-visibility:

Visibility
==========

.. reference::

   :Panel:     :menuselection:`Object Properties --> Visibility`

.. seealso::

   There are several other :doc:`general visibility </scene_layout/object/properties/visibility>` properties.

.. _bpy.types.Object.is_shadow_catcher:

Mask
   Shadow Catcher
      Enables the object to only receive shadow rays. It is to be noted that,
      shadow catcher objects will interact with other CG objects via indirect light interaction.
      This simplifies compositing :abbr:`CGI (Computer-Generated Imagery)` elements into real-world footage.

      .. note::

         The *Shadow Catcher* outputs different results depending on if the *Shadow Catcher* pass is enabled in
         :ref:`Render Layer <render_layers_passes_data>` settings. With the *Shadow Catcher* pass enabled, all
         indirect light interactions are captured. With it disabled, a simple approximation is used instead.
         The simple approximation is used in viewport rendering.

      .. figure:: /images/render_cycles_object-settings_object-data_shadow-catcher.jpg

         Example of the shadow catcher. Note how the material of the plane can still be viewed in the spheres.


.. _cycles-ray-visibility:

Ray Visibility
--------------

Objects can be set to be invisible to particular ray types.
This can be used, for example, to make an emitting mesh invisible to camera rays.
For instanced objects, visibility is inherited; if the parent object is hidden for some ray types,
the children will be hidden for these too.

In terms of performance, using these options is more efficient that using a shader node setup
that achieves the same effect.
Objects invisible to a certain ray will be skipped in ray traversal already,
leading to fewer ray casts and shader executions.

.. _bpy.types.Object.visible_camera:

Camera
   Makes the object visible in camera rays.

.. _bpy.types.Object.visible_diffuse:

Diffuse
   Makes the object visible in diffuse rays.

.. _bpy.types.Object.visible_glossy:

Glossy
   Makes the object visible in glossy rays.

.. _bpy.types.Object.visible_transmission:

Transmission
   Makes the object visible in transmission rays.

.. _bpy.types.Object.visible_volume_scatter:

Volume Scatter
   Makes the object visible in volumetric scatter rays.

Shadow
   Enables the object to cast shadows.


Culling
-------

In order to activate these options the respectively camera cull options have to be enabled
in the scene :ref:`simplify panel <render-cycles-settings-scene-simplify-culling>`.

.. _bpy.types.CyclesObjectSettings.use_camera_cull:

Use Camera Cull
   Ignore and this way make objects invisible to rays outside of the camera frustum.

.. _bpy.types.CyclesObjectSettings.use_distance_cull:

Use Distance Cull
   Will cull any objects further from the camera than a given distance. When used in combination with
   camera frustum culling, this can be used to avoid culling nearby objects that are outside the camera frustum,
   but still visible in reflections. It is also useful to cull small objects far from the camera.


.. _bpy.types.CyclesObjectSettings.use_motion_blur:

Motion Blur
===========

.. reference::

   :Panel:     :menuselection:`Properties --> Object Properties --> Motion Blur`

Each object has its own motion blur settings along with
the :doc:`Scene Level Motion Blur </render/cycles/render_settings/motion_blur>`.
These settings can be found in the Object Properties tab of the Properties.

.. _bpy.types.CyclesObjectSettings.motion_steps:

Steps
   Controls accuracy of deformation motion blur, more steps uses more memory.
   The actual number of time steps is :math:`2^{steps -1}`.

.. _bpy.types.CyclesObjectSettings.use_deform_motion:

Deformation
   Enables motion blur for deformed meshes such as animated characters, including hair.

   .. warning::

      An object modifier setup that changes mesh topology over time can not render
      deformation motion blur correctly. Deformation blur should be disabled for such objects.
      Common examples of this are animated Booleans, Deformation
      before Edge Split, Remesh, Skin or Decimate modifiers.


Shading
=======

.. reference::

   :Panel:     :menuselection:`Properties --> Object Properties --> Shading`


Shadow Terminator
-----------------

.. _bpy.types.CyclesObjectSettings.shadow_terminator_geometry_offset:

Geometry Offset
   Offset rays from the surface to reduce shadow terminator artifacts on low-poly geometry.
   Higher values affect more triangles, a value of one affecting all triangles and zero having no affect.
   The default value only affects triangles at grazing angles to light and should eliminate most artifacts.

   Unlike the *Shading Offset*, this option has little affect on the lighting
   making it the preferable method to handle shadow terminator artifacts.

.. _bpy.types.CyclesObjectSettings.shadow_terminator_offset:

Shading Offset
   Pushes the shadow terminator (the line that divides the light and dark) towards the light
   to hide artifacts on low-poly geometry such as the ones below:

   .. list-table::

      * - .. figure:: /images/render_cycles_object-settings_object-data_shading-terminator1.jpg

             Shadow Terminator Artifacts.

        - .. figure:: /images/render_cycles_object-settings_object-data_shading-terminator2.jpg

             Result of using an offset of 0.15.

   .. note::

      This property artificially alters the scene's lighting
      and is not energy conserving and consequently not physically accurate (see *Geometry Offset* instead).


Fast GI Approximation
---------------------

.. _bpy.types.CyclesObjectSettings.ao_distance:

AO Distance
   Override for the world's :ref:`AO Distance <bpy.types.CyclesRenderSettings.use_fast_gi>`,
   if the value is zero the world's distance is used.


Caustics
--------

Mark objects as caustic casters or receivers. This is used in conjunction with a
:ref:`Light <bpy.types.CyclesLightSettings.is_caustics_light>` or
:ref:`World Shader <bpy.types.CyclesWorldSettings.is_caustics_light>` with *Shadow Caustics* enabled
to selectively speed up caustic rendering of objects in your scene.

.. note::

   The rendering technique used to speed up the rendering of caustics is based on
   :abbr:`MNEE (Manifold Next Event Estimation)`. There are a number of limitations with this technique
   and it's implementation in Cycles:

   - Only refractive caustics in the shadows of objects work. Caustics from reflections or caustics that
     fall outside shadows are not rendered with this technique.

   - MNEE Caustics are an approximation of caustics and will produce physically inaccurate results
     in many situations. Examples include incorrect brightnesses and the incorrect representation of
     caustics caused by rough or curved surfaces.

   - In complex materials with multiple refractive BSDFs, MNEE will only generate caustics for one of
     the BSDFs.

   - :ref:`Filter Glossy <bpy.types.CyclesRenderSettings.blur_glossy>` settings are ignored when using
     MNEE for refractive caustics.

   - MNEE Caustic rays can pass through up to 6 Caustic Caster surfaces between a Caustic Receiver and a
     Shadow Caustic light before the ray is terminated and caustics are ignored.

   - The :ref:`Ambient Occlusion <bpy.types.ShaderNodeAmbientOcclusion>` and
     :ref:`Bevel <bpy.types.ShaderNodeBevel>` nodes will not produce a valid result on objects that are
     a Caustic caster or Caustic receiver while the scene contains a active Caustic caster,
     Caustic receiver, and :ref:`Shadow Caustic Light <bpy.types.CyclesLightSettings.is_caustics_light>`.

   - MNEE Caustics only work if the caustic caster has smooth normals.

   - Volumetric materials are not considered when calculating MNEE caustics.

   - Bump and normal maps are ignored when calculating caustics.

.. _bpy.types.CyclesObjectSettings.is_caustics_caster:

Cast Shadow Caustics
   Mark an object as a caustic caster.

.. _bpy.types.CyclesObjectSettings.is_caustics_receiver:

Receive Shadow Caustics
   Mark an object as a caustic receiver.

.. list-table::

   * - .. figure:: /images/render_cycles_object-settings_caustics-example1.png

          Rendering caustics inside an eye without MNEE at 32 samples per pixel.

     - .. figure:: /images/render_cycles_object-settings_caustics-example2.png

          Rendering caustics inside an eye using MNEE at 32 samples per pixel.


Light Group
-----------

.. _bpy.types.Object.lightgroup:

Light Group
   Select the :ref:`Light Group <bpy.types.ViewLayer.active_lightgroup_index>` to add the
   current *Object* or *Light* too.

   Add Light Group
      If the name input into the *Light Group* field does not align with an existing
      Light Group, then pressing this button will create a *Light Group* with that name
      and assign the selected *Object* or *Light* to it.

Light Linking
-------------

.. _render-cycles-object-light-linking-settings:

Limit light influence to specified objects, with :ref:`Light Linking <bpy.types.Object.light_linking>`.

Receiver Collection
   Collection of objects that will receive light emitted from the object.

Shadow Linking
--------------

.. _render-cycles-object-shadow-linking-settings:

Limit shadows to specified objects, with :ref:`Light Linking <bpy.types.Object.light_linking>`.

Shadow Blocker Collection
   Collection of objects that will act as shadow blockers for light emitted from the object.


## Index


######################
  Optimizing Renders
######################

.. toctree::
   :maxdepth: 2

   reducing_noise.rst
   nodes.rst


## Nodes


************
Shader Nodes
************

Cycles applies a number of shader node optimizations both at compile time and run-time.
By exploiting them it is possible to design complicated "Uber Shader"
style node groups that incur minimal render time overhead for unused features.


.. is this information needed? Sounds more developer related.

Node Optimizations
==================

As the first step in preparing a node shader for execution,
Cycles expands all node groups, as if using the Ungroup tool,
and discards UI only features like frames and reroute nodes.

After that, it applies some obvious transformations,
for example, it can (the list is not exhaustive):

- Replace the following nodes with the constant result of their evaluation,
  if all their inputs are determined to be constant:

  RGB, Value, Mix RGB, Math, Vector Math, RGB to BW, Gamma, Bright Contrast,
  Invert, Separate/Combine RGB/XYZ/HSV, Blackbody, RGB Curves, Vector Curves, Color Ramps.

- Detect Mix RGB, Math and Vector Math nodes that become no-op (without Clamp)
  or evaluate to 0 as a result of addition, subtraction, multiplication,
  division or dot/cross product with a known constant 0 or 1 input,
  and replace with the appropriate input link or constant result.
- Eliminate Mix RGB Mix (without Clamp) and Mix Shader nodes when
  Factor is known to be 0 or 1 by replacing with the appropriate input value or link.
- Eliminate no-op Mix RGB (except Color Burn, Color Dodge, Lighten, or enabled Clamp),
  Invert, RGB Curves and Vector Curves nodes with known zero Factor.
- Eliminate Emission and Background shader nodes that do not emit any light,
  and Add Shader nodes with one or both input arguments missing.
- Eliminate :doc:`Bump </render/shader_nodes/vector/bump>` with constant Height input, using its Normal input or
  Geometry Normal instead. This is useful for implementing node group inputs that default to normal via routing
  through a no-op Bump before doing math.
- Replace :doc:`Attribute </render/shader_nodes/input/attribute>` nodes of the View Layer type with the
  evaluated attribute value (it is constant within the whole Render Layer).
- Combine multiple copies of the same node with the same inputs into only one instance.

Finally, any nodes that end up not connected either directly or indirectly to the Output node are removed.


Run-Time Optimizations
======================

When executing shaders, a special optimization is applied to Mix Shader nodes.
If Factor evaluates to 0 or 1, any nodes that are only reachable via the unused branch of the mix are not evaluated.

This can substantially reduce the performance cost of combining multiple materials
in one shader with a Color Attribute, texture, or other input used as a switch.


Open Shading Language
=====================

If Open Shading Language is chosen as the rendering backend,
node shaders are translated to OSL code and then compiled and executed by the OSL runtime.
In the process it applies its own extensive set of optimizations, both at compile time and run-time.

Open Shading Language can optimize out Script nodes if their outputs are unused or constant,
even if their OSL shaders have side effects like debug tracing and message passing,
which may be confusing. For that reason message passing with ``setmessage`` and ``getmessage``
should generally not be used for passing information forward in the graph;
explicitly passing information through sockets should be preferred.


## Reducing Noise


**************
Reducing Noise
**************

When performing a final render, it is important to reduce noise as much as possible.
Here we will discuss a number of tricks that, while breaking the laws of physics,
are particularly important when rendering animations within a reasonable time.
Click to enlarge the example images to see the noise differences well.


Path Tracing
============

Cycles uses path tracing with next event estimation,
which is not good at rendering all types of light effects, like caustics,
but has the advantage of being able to render more detailed and
larger scenes compared to some other rendering algorithms.
This is because we do not need to store,
for example, a photon map in memory,
and because we can keep rays relatively coherent to use an on-demand image cache,
compared to e.g. bidirectional path tracing.

.. figure:: /images/render_cycles_render-settings_light-paths_rays.svg
   :align: center

We do the inverse of what reality does,
tracing light rays from the camera into the scene and onto lights,
rather than from the light sources into the scene and then into the camera.
This has the advantage that we do not waste light rays that will not end up in the camera,
but also means that it is difficult to find some light paths that may contribute a lot.
Light rays will be sent either according to the surface BRDF,
or in the direction of known light sources.

.. seealso::

   For more details, see
   the :doc:`Light Paths </render/cycles/render_settings/light_paths>` and
   :doc:`Sampling </render/cycles/render_settings/sampling>` documentation.


The Source of the Noise
=======================

To understand where noise can come from, take for example the scene below.
When we trace a light ray into the location marked by the white circle on a red dot,
the second image below gives an impression of what the diffuse shader "sees".

To find the light that is reflected from this surface,
we need to find the average color from all these pixels.
Note the glossy highlight on the sphere,
and the bright spot the light casts on the nearby wall. These hot-spots are much brighter than
other parts of the image and will contribute significantly to the lighting of this pixel.

.. list-table::

   * - .. figure:: /images/render_cycles_optimizations_reducing-noise_fisheye-reference.jpg
          :width: 180px

          The scene.

     - .. figure:: /images/render_cycles_optimizations_reducing-noise_fisheye.jpg
          :width: 180px

          Irradiance at the shading point.

     - .. figure:: /images/render_cycles_optimizations_reducing-noise_fisheye-hotspot.jpg
          :width: 180px

          The detected highlights.

The light is a known light source, so its location is already known,
but the glossy highlight(s) that it causes are a different matter.
The best we can do with path tracing is to distribute light rays randomly over the hemisphere,
hoping to find all the important bright spots. If for some pixels we miss some bright spot,
but we do find it for another, that results in noise. The more samples we take,
the higher the probability that we cover all the important sources of light.

With some tricks we can reduce this noise. If we blur the bright spots,
they become bigger and less intense, making them easier to find and less noisy.
This will not give the same exact result,
but often it's close enough when viewed through a diffuse or soft glossy reflection.
Below is an example of using :ref:`Glossy Filter <bpy.types.CyclesRenderSettings.blur_glossy>`
and :doc:`Light Falloff </render/shader_nodes/color/light_falloff>`.

.. list-table::

   * - .. figure:: /images/render_cycles_optimizations_reducing-noise_fisheye-blur-reference.jpg
          :width: 180px

          Using Glossy Filter & Light Falloff.

     - .. figure:: /images/render_cycles_optimizations_reducing-noise_fisheye-blur.jpg
          :width: 180px

          Irradiance at the shading point.

     - .. figure:: /images/render_cycles_optimizations_reducing-noise_fisheye-blur-hotspot.jpg
          :width: 180px

          The detected highlights.


Bounces
=======

In reality light will bounce a huge number of times due to the speed of light being very high.
In practice more bounces will introduce more noise, and it might be good to use something like
the Limited Global Illumination preset in the :ref:`Light Paths <render-cycles-integrator-light-paths>`
Section that uses *fewer* bounces for different shader types.
Diffuse surfaces typically can get away with fewer bounces,
while glossy surfaces need a few more,
and transmission shaders such as glass usually need the most.

.. list-table::

   * - .. figure:: /images/render_cycles_optimizations_reducing-noise_0bounce.jpg
          :width: 180px

          No bounces.

     - .. figure:: /images/render_cycles_optimizations_reducing-noise_2bounce.jpg
          :width: 180px

          Two bounces at max.

     - .. figure:: /images/render_cycles_optimizations_reducing-noise_4bounce.jpg
          :width: 180px

          Four bounces at max.

Also important is to use shader colors that do **not** have components of value 1.0 or
values near that; try to keep the maximum value to 0.8 or less and make your lights brighter.
In reality, surfaces are rarely perfectly reflecting all light,
but there are of course exceptions; usually glass will let most light through,
which is why we need more bounces there. High values for the color components tend to
introduce noise because light intensity then does not decrease much as it bounces off each
surface.


Caustics and Filter Glossy
==========================

Caustics are a well-known source of noise, causing :term:`Fireflies`.
They happen because the renderer has difficulty finding specular highlights
viewed through a soft glossy or diffuse reflection.
There is a :ref:`No Caustics <bpy.types.CyclesRenderSettings.caustics>` option
to disable glossy behind a diffuse reflection entirely.
Many renderers will typically disable caustics by default.

.. list-table::

   * - .. figure:: /images/render_cycles_optimizations_reducing-noise_reference.jpg
          :width: 180px

          Default settings.

     - .. figure:: /images/render_cycles_optimizations_reducing-noise_no-caustics.jpg
          :width: 180px

          Caustics disabled.

     - .. figure:: /images/render_cycles_optimizations_reducing-noise_filter-glossy.jpg
          :width: 180px

          Filter Glossy greater than zero.

However, using *No Caustics* will result in missing light,
and it still does not cover the case where a sharp glossy reflection is viewed through a soft glossy reflection.
There is a :ref:`Filter Glossy <bpy.types.CyclesRenderSettings.blur_glossy>` option
to reduce the noise from such cases at the cost of accuracy.
This will blur the sharp glossy reflection to make it easier to find, by increasing the shader Roughness.

The above images show default settings, no caustics, and filter glossy set to 1.0.


Light Falloff
=============

In reality light in a vacuum will always fall off at a rate of 1/(distance^2).
However, as distance goes to zero,
this value goes to infinity and we can get very bright spots in the image.
These are mostly a problem for indirect lighting, where the probability of hitting such
a small but extremely bright spot is low and so happens only rarely.
This is a typical recipe for :term:`Fireflies`.

.. list-table::

   * - .. figure:: /images/render_cycles_optimizations_reducing-noise_falloff-hard.jpg
          :width: 180px

          Hard Falloff.

     - .. figure:: /images/render_cycles_optimizations_reducing-noise_falloff-soft.jpg
          :width: 180px

          Soft Falloff.

To reduce this problem, the :doc:`Light Falloff </render/shader_nodes/color/light_falloff>`
node has a *Smooth factor*, that can be used to reduce the maximum intensity
a light can contribute to nearby surfaces. The images above show default falloff and smooth value 1.0.


.. _render-cycles-reducing-noise-mis:

Multiple Importance Sampling
============================

Materials with emission shaders can be configured to use
Multiple Importance Sampling (:doc:`/render/cycles/material_settings`).
This means that they will get rays sent directly towards them,
rather than ending up there based on rays randomly bouncing around.
For very bright mesh light sources, this can reduce noise significantly.
However, when the emission is not particularly bright,
this will take samples away from other brighter light sources for which it is important to find them this way.

The optimal setting here is difficult to guess; it may be a matter of trial and error,
but often it is clear that a somewhat glowing object may be only contributing light locally,
while a mesh light used as a light would need this option enabled.
Here is an example where the emissive spheres contribute little to the lighting,
and the image renders with slightly less noise by disabling Multiple Importance on them.

.. list-table::

   * - .. figure:: /images/render_cycles_optimizations_reducing-noise_sample-lamp.jpg
          :width: 180px

          Multiple Importance off.

     - .. figure:: /images/render_cycles_optimizations_reducing-noise_no-sample-lamp.jpg
          :width: 180px

          Multiple Importance on.

The world background also has a *Multiple Importance* (:ref:`render-cycles-integrator-world-settings`) option.
This is mostly useful for environment maps that have small bright spots in them, rather than being smooth.
This option will then, in a preprocess, determine the bright spots, and send light rays directly towards them. Again,
enabling this option may take samples away from more important light sources if it is not needed.


.. _render-cycles-reducing-noise-glass-and-transp-shadows:

Glass and Transparent Shadows
=============================

With caustics disabled, glass shadows may appear too dark,
and with filter glossy the caustics might be too soft.
We can make a glass shader that will use a Glass BSDF when viewed *directly*,
and a Transparent BSDF when viewed *indirectly*. The Transparent BSDF can be used for
transparent shadows to find light sources straight through surfaces,
and will give properly-colored shadows, but without the caustics.
The Light Path node is used to determine when to use which of the two shaders.

.. figure:: /images/render_cycles_optimizations_reducing-noise_glass-group.png

   Optimized glass shader.

Above we can see the node setup used for the glass transparency trick;
on the left the render has dark shadows due to missing caustics,
and on the right the render with the trick.

.. list-table::

   * - .. figure:: /images/render_cycles_optimizations_reducing-noise_glass-too-much-shadow.jpg
          :width: 180px

          Default Glass BSDF.

     - .. figure:: /images/render_cycles_optimizations_reducing-noise_glass-trick.jpg
          :width: 180px

          Optimized Glass Shader.


Light Portals
=============

When rendering a daylight indoor scene where most of the light is coming in through a window
or door opening, it is difficult for the integrator to find its way to them.
To fix this, use :ref:`Light Portals <render-cycles-lights-area-portals>`.
You then will need to modify its shape to match that of the opening that you are trying to fill.

.. figure:: /images/render_cycles_light-settings_portals2.jpg
.. figure:: /images/render_cycles_light-settings_portals.jpg


Denoising
=========

Even with all the settings described above there will always end
up being some render noise no matter how many samples you use.
To fix this there is a post-processing technique to cleanup the final bit of noise.
To use this enable :ref:`Denoising <render-cycles-settings-viewport-denoising>`
in the *Render* tab of the Properties.

Below is an example render by
`The Pixelary <https://blog.thepixelary.com/post/160451378592/denoising-in-cycles-tested>`__.

.. list-table::

   * - .. figure:: /images/render_layers_denoising_example1.jpg

          Example render before denoising.

     - .. figure:: /images/render_layers_denoising_example2.jpg

          Example render after denoising.


.. _render-cycles-reducing-noise-clamp-samples:

Clamp Fireflies
===============

Ideally with all the previous tricks, :term:`Fireflies` would be eliminated, but they could still happen.
For that, the *intensity* that any individual light ray sample will contribute to a pixel can be *clamped*
to a maximum value with the integrator :ref:`Clamp setting <render-cycles-integrator-clamp-samples>`.

If set too low this can cause missing highlights in the image,
which might be useful to preserve for camera effects such as bloom or glare.
To mitigate this conundrum it's often useful to clamp only indirect bounces,
leaving highlights directly visible to the camera untouched.

.. list-table::

   * - .. figure:: /images/render_cycles_optimizations_reducing-noise_no-clamp.jpg
          :width: 180px

          No Clamp (0).

     - .. figure:: /images/render_cycles_optimizations_reducing-noise_clamp4.jpg
          :width: 180px

          With Clamp set to 4.


## Film


****
Film
****

.. reference::

   :Panel:     :menuselection:`Render --> Film`

.. _bpy.types.CyclesRenderSettings.film_exposure:

Exposure
   This can be used to change the brightness of an image.
   Different than the *Exposure* option found in the :ref:`Color management <render-post-color-management>` panel,
   this exposure option works *on the data* while the Color management exposure is on the *view transform*.


Pixel Filter
============

Due to limited resolution of images and computer screens, pixel filters are needed to avoid :term:`Aliasing`.
This is achieved by slightly blurring the image to soften edges.

.. _bpy.types.CyclesRenderSettings.pixel_filter_type:

Type
   Pixel Filtering algorithm to use.

   :Box: No filter.
   :Gaussian: Smooth filter.
   :Blackman-Harris: Default filter with a better balance between smoothness and detail preservation.

.. _bpy.types.CyclesRenderSettings.filter_width:

Width
   Lower values give more crisp renders, higher values are softer and reduce aliasing.


.. _bpy.types.RenderSettings.film_transparent:

Transparent
===========

Render the background transparent, for compositing the image over another background after rendering.

.. _bpy.types.CyclesRenderSettings.film_transparent_glass:

Transparent Glass
   Render transmissive surfaces as transparent, for compositing glass over another background.

.. _bpy.types.CyclesRenderSettings.film_transparent_roughness:

Transparent Roughness
   For transparent glass, keep surfaces with roughness above the threshold opaque.


## Grease Pencil


*************
Grease Pencil
*************

.. reference::

   :Panel:     :menuselection:`Render --> Grease Pencil`

This panel is comprised of settings to control the rendering of :doc:`Grease Pencil Lines </grease_pencil/index>`.

.. _bpy.types.SceneGpencil.antialias_threshold:

Anti-Aliasing Threshold
   Threshold for the edge detection algorithm used to correct aliasing,
   higher values might over blur some part of the image.


## Hair


******
Curves
******

.. reference::

   :Panel:     :menuselection:`Render --> Curves`

These are global settings that apply to all instances of particle hair systems.
The resolution of the strands is controlled by the step values in particle settings.
Each hair system uses the material identified in the particle settings.

.. _bpy.types.CyclesCurveRenderSettings.shape:

Shape
   :Rounded Ribbons:
      Render curves as flat ribbon with rounded normals, for fast rendering.
      Curves are subdivided with a fixed number of specified subdivisions.

      .. _bpy.types.CyclesCurveRenderSettings.subdivisions:

      Curve Subdivisions
         Number of subdivisions used in cardinal curve intersection (power of 2).

   :3D Curves:
      Render curves as circular 3D geometry, for accurate results when viewing curves close up.
      Curves are automatically subdivided until the curve is smooth.


Viewport Display
----------------

These settings control the curve rendering settings used when the 3D viewport is set to
:ref:`Material Preview <3dview-material-preview>`

.. include:: /render/eevee/render_settings/curves.rst
   :start-after: .. --- copy below this line ---


## Index


###################
  Render Settings
###################

.. toctree::
   :maxdepth: 2

   grease_pencil.rst
   sampling.rst
   light_paths.rst
   volumes.rst
   subdivision.rst
   hair.rst
   simplify.rst
   motion_blur.rst
   film.rst
   performance.rst


## Light Paths

.. _render-cycles-integrator-light-paths:

***********
Light Paths
***********

.. reference::

   :Panel:     :menuselection:`Render --> Light Paths`


Ray Types
=========

Ray types can be divided into four categories:

#. Camera: the ray comes straight from the camera.
#. Reflection: the ray is generated by a reflection off a surface.
#. Transmission: the ray is generated by a transmission through a surface.
#. Shadow: the ray is used for (transparent) shadows.

Reflection and transmission rays can further have these properties:

- Diffuse: the ray is generated by a diffuse reflection or transmission (translucency).
- Glossy: the ray is generated by a glossy specular reflection or transmission.
- Singular: the ray is generated by a perfectly sharp reflection or transmission.

The Light Path node can be used to find out the type of ray the shading is being computed for.

.. figure:: /images/render_cycles_render-settings_light-paths_rays.svg
   :align: center

.. seealso::

   The object :ref:`ray visibility <cycles-ray-visibility>` settings.


Bounce Control
==============

The maximum number of light bounces can be controlled manually.
While ideally this should be infinite,
in practice a smaller number of bounces may be sufficient,
or some light interactions may be intentionally left out for faster convergence.
The number of diffuse reflection,
glossy reflection and transmission bounces can also be controlled individually.

Light paths are terminated probabilistically when specifying a minimum number of light bounces
lower than the maximum. In that case, paths longer than minimum will be randomly stopped when
they are expected to contribute less light to the image.
This will still converge to the same image, but renders faster while possibly being noisier.


.. _render-cycles-light-paths-transparency:

Transparency
============

The :doc:`/render/shader_nodes/shader/transparent` shader is given
special treatment. Rays pass straight through it, changing neither direction nor type
as if there were no geometry at all.

Alpha pass output is also different for the transparent
:abbr:`BSDF (Bidirectional Scattering Distribution Function)`.
Other transmission BSDFs are considered opaque,
because they change the light direction. As such they cannot be used for
alpha-over compositing, while this is possible with the transparent BSDF.

Note that, while semantically the ray passes through as if no geometry was hit,
rendering performance is affected as each transparency step requires executing the shader and tracing a ray.


Settings
========

Max Bounces
-----------

.. _bpy.types.CyclesRenderSettings.max_bounces:

Total
   Maximum number of light bounces. For best quality, this should be set to the maximum.
   However, in practice, it may be good to set it to lower values for faster rendering.
   A value of 0 bounces results in direct lighting only.

.. _bpy.types.CyclesRenderSettings.diffuse_bounces:

Diffuse
   Maximum number of diffuse bounces.

.. _bpy.types.CyclesRenderSettings.glossy_bounces:

Glossy
   Maximum number of glossy bounces.

.. _bpy.types.CyclesRenderSettings.transmission_bounces:

Transmission
   Maximum number of transmission bounces.

.. _bpy.types.CyclesRenderSettings.volume_bounces:

Volume
   Maximum number of volume scattering bounces.

.. _bpy.types.CyclesRenderSettings.max_transparent_bounces:

Transparent
   Maximum number of transparency bounces.

   Note, the maximum number of transparent bounces is controlled separately from other bounces.
   It is also possible to use probabilistic termination of transparent bounces,
   which might help rendering many layers of transparency.


.. _render-cycles-integrator-clamp-samples:

Clamping
--------

.. _bpy.types.CyclesRenderSettings.sample_clamp_direct:

Direct Light
   This option limits the maximum intensity a sample from rays which have not yet bounced can contribute to a pixel.
   It reduces noise at the cost of accuracy. Setting this option to 0.0 disables clamping altogether.
   Lower have a greater affect (dimmer samples) on the resulting image than higher values.

   .. note::

      This option provides a way to limit :term:`Fireflies`. However, note that as you clamp out such values,
      other bright lights/reflections will be dimmed as well.

      Care must be taken when using this setting to find a balance between mitigating fireflies and
      losing intentionally bright parts. It is often useful to clamp indirect bounces separately,
      as they tend to cause more fireflies than direct bounces. See the *Clamp Indirect* setting.

.. _bpy.types.CyclesRenderSettings.sample_clamp_indirect:

Indirect Light
   The same as *Direct Light*, but for rays which have bounced multiple times.


Caustics
--------

A common source of noise is :term:`Caustics`.

.. seealso::

   See :ref:`Reducing Noise <render-cycles-reducing-noise-clamp-samples>`
   for examples of the clamp settings in use.

.. _bpy.types.CyclesRenderSettings.blur_glossy:

Filter Glossy
   When using a value higher than 0.0, this will blur glossy reflections after blurry bounces,
   to reduce noise at the cost of accuracy. 1.0 is a good starting value to tweak.

   Some light paths have a low probability of being found while contributing much light to the pixel.
   As a result these light paths will be found in some pixels and not in others, causing :term:`Fireflies`.
   An example of such a difficult path might be a small light that is causing a small specular highlight
   on a sharp glossy material, which is observed through a rough glossy material.
   In fact in such a case there practically occurs a caustic.

   With path tracing it is difficult to find the specular highlight,
   but if you increase the roughness on the material, the highlight gets bigger and softer, and so easier to find.
   Often this blurring will hardly be noticeable, because it is blurred by the material anyway,
   but there are also cases where this will lead to a loss of detail in lighting.

.. _bpy.types.CyclesRenderSettings.caustics:

Caustics
   Reflective
      While in principle path tracing supports rendering of caustics with a sufficient number of samples,
      in practice it may be inefficient to the point that there is just too much noise.
      This option can be unchecked, to disable reflective caustics.
   Refractive
      The same as above, but for refractive caustics.


.. _bpy.types.CyclesRenderSettings.use_fast_gi:

Fast GI Approximation
---------------------

.. reference::

   :Panel:     :menuselection:`Render --> Light Paths --> Fast GI Approximation`

Approximate diffuse indirect light with background tinted ambient occlusion.
This provides fast alternative to full global illumination (GI),
for interactive viewport rendering or final renders with reduced quality.

.. _bpy.types.CyclesRenderSettings.fast_gi_method:

Method
   Fast GI approximation method.

   :Replace: Replace global illumination with ambient occlusion after a specified number of bounces.
   :Add: Add ambient occlusion to diffuse surfaces.

.. _bpy.types.WorldLighting.ao_factor:

AO Factor
   The strength of the ambient occlusion.

.. _bpy.types.WorldLighting.distance:

AO Distance
   Distance from shading point to trace rays. A shorter distance emphasizes nearby features,
   while longer distances make it also take objects farther away into account.

   This option can also be overridden per object
   in the :ref:`Object Properties <bpy.types.CyclesObjectSettings.ao_distance>`,
   which is useful when you have both small and large scale objects in the same scene.

.. _bpy.types.CyclesRenderSettings.ao_bounces:

Viewport Bounces
   Replace global illumination with ambient occlusion after the specified number of bounces
   when rendering in the 3D Viewport. This can reduce noise in interior scenes with little visual difference.

.. _bpy.types.CyclesRenderSettings.ao_bounces_render:

Render Bounces
   Number of bounces when rendering final renders.


## Motion Blur

.. _bpy.types.RenderSettings.use_motion_blur:

***********
Motion Blur
***********

.. reference::

   :Panel:     :menuselection:`Render --> Motion Blur`

Blender's animations are by default rendered as a sequence of *perfectly still* images.
While great for stop-motion and time-lapses, this is unrealistic, since fast-moving
objects do appear to be blurred in the direction of motion,
both in a movie frame and in a photograph from a real-world camera.

.. figure:: /images/render_cycles_render-settings_motion-blur_example-cubes.jpg

   Motion blur example.
   (`blend-file <https://archive.blender.org/wiki/2015/uploads/0/03/Blender2.65_motion_blur.blend>`__)

.. _bpy.types.RenderSettings.motion_blur_position:

Position
   Controls at what point the shutter opens in relation to the current frame.

   :Start on Frame: Shutter is starting to open at the current frame.
   :Center on Frame: Shutter is fully opened at the current frame.
   :End on Frame: Shutter is fully closed at the current frame.

.. _bpy.types.RenderSettings.motion_blur_shutter:

Shutter
   Time (in frames) between when the shutter starts to open and fully closed.
   For example, shutter time 1.0 blurs over the length of 1 frame.

.. _bpy.types.CyclesRenderSettings.rolling_shutter_type:

Rolling Shutter
   Creates a :term:`Rolling Shutter` effect.

   :None: No rolling shutter effect.
   :Top-Bottom: Renders rolling shutter from the top of the image to the bottom.

.. _bpy.types.CyclesRenderSettings.rolling_shutter_duration:

Rolling Shutter Duration
   Controls balance between pure rolling shutter effect (if the value is zero)
   and pure motion blur effect (if the value is one).

.. note::

   If there are particles or other physics system in a scene,
   be sure to bake them before rendering,
   otherwise you might not get correct or consistent motion blur.

.. seealso::

   Each object has its own settings to control motion blur.
   These options can be found in the Object tab of the Properties.
   See :ref:`object setting <bpy.types.CyclesObjectSettings.use_motion_blur>` for more information.


.. _bpy.ops.render.shutter_curve_preset:

Shutter Curve
=============

Curve defining how the shutter opens and closes.
The X axis is time, Y values of 0 mean fully closed shutter, Y values of 1 mean fully opened shutter.
The default mapping is set to when shutter opens and closes instantly.


Limitations
===========

- Camera motion blur does not work for :doc:`Orthographic Cameras </render/cameras>`.
- Motion blur does not take into account the movement of :doc:`Lights </render/lights/light_object>`.


## Performance


***********
Performance
***********

.. reference::

   :Panel:     :menuselection:`Render --> Performance`

Properties that affect render speeds or memory consumption.
There are several presets available to help choose between different trade offs:

:Default: Balances memory saving and faster rendering settings.
:Faster Render: Uses settings to render faster at the cost of higher memory consumption.
:Lower Memory: Uses settings to decrease memory considered at the cost of slower renders.


Threads
=======

.. _bpy.types.RenderSettings.threads_mode:

Threads Mode
   Method to determine the maximum number of CPU cores to use while rendering.

   :Auto-Detect:
      Automatically chooses the amount of threads to match the number of logical processors on your computer.
   :Fixed:
      Manually choose the maximum number threads to use for rendering.
      This can be useful for example, if you want to use your computer while rendering,
      you can set the property to a thread count lower the amount of logical processors on your computer.

.. _bpy.types.RenderSettings.threads:

Threads
   The maximum number of CPU cores to use simultaneously while rendering.


Tiles
=====

.. _bpy.types.RenderSettings.use_auto_tile:

Use Tiling
   Render high resolution images in tiles to reduce memory usage.
   Tiles are cached to disk while rendering to save memory.

.. _bpy.types.RenderSettings.tile_size:

Tile Size
   This value is used to control the size of the tile used for rendering;
   decreasing the size reduces memory usage.

   .. note::

      In some cases changing the *Tile Size* can result in increased performance.
      For example when a small object renders slowly compared to other objects,
      using a small *Tiles Size* can lead to an increase in performance.


Acceleration Structure
======================

.. _bpy.types.CyclesRenderSettings.debug_use_spatial_splits:

Use Spatial Splits
   Spatial splits improve the rendering performance in scenes with a mix of large and small polygons.
   The downsides are longer BVH build times and slightly increased memory usage.

.. _bpy.types.CyclesRenderSettings.debug_use_hair_bvh:

Use Curves BVH
   Use a special type of :term:`BVH` for rendering curves.
   The bounding boxes are not axis aligned allowing a spatially closer fit to the curve geometry.
   Disabling this option will reduce memory, at the cost of increasing curve render time.

.. _bpy.types.CyclesRenderSettings.debug_bvh_time_steps:

BVH Time Steps
   Split BVH primitives by this number of time steps to speed up render time at the expense of memory.

Use Compact BVH
   Use a more compact BVH structure, which can reduce RAM usage but render slower.


Final Render
============

.. _bpy.types.RenderSettings.use_persistent_data:

Persistent Data
   Keep render data in memory after rendering for faster re-renders and animation renders
   at the cost of extra memory usage while performing other tasks in Blender.

   When using multiple :doc:`View Layers </render/layers/view_layer>`,
   only data from a single view layer is preserved to keep memory usage within bounds;
   however, objects shared between view layers are preserved.


Viewport
========

.. _bpy.types.RenderSettings.preview_pixel_size:

Pixel Size
   Option to control the resolution for viewport rendering.
   Allows you to speed up viewport rendering, which is especially useful for displays with high DPI.


## Sampling

********
Sampling
********

.. reference::

   :Panel:     :menuselection:`Render --> Sampling`

The integrator is the rendering algorithm used to compute the lighting.
Cycles currently supports a path tracing integrator with direct light sampling.
It works well for various lighting setups,
but is not as suitable for caustics and some other complex lighting situations.

Rays are traced from the camera into the scene,
bouncing around until they find a light source such as a light, an object emitting light,
or the world background. To find lights and surfaces emitting light,
both indirect light sampling (letting the ray follow the surface BSDF)
and direct light sampling (picking a light source and tracing a ray towards it) are used.

.. _bpy.types.CyclesRenderSettings.preview_samples:

Viewport Samples
   Number of samples for viewport rendering. Setting this value to zero
   enables indefinite sampling of the viewport.

.. _bpy.types.CyclesRenderSettings.samples:

Render Samples
   Number of paths to trace for each pixel in the final render. As more samples are taken,
   the solution becomes less noisy and more accurate.

.. _bpy.types.CyclesRenderSettings.time_limit:

Time Limit
   Renders scene until time limit or sample count is reached. When the time is set to 0,
   the sample count is used to determine when the render stops.

   .. note:: The time limit does not include pre-render processing time, only render time.


.. _bpy.types.CyclesRenderSettings.use_adaptive_sampling:

Adaptive Sampling
=================

With adaptive sampling Cycles automatically reduces the number of samples in areas that have little noise,
for faster rendering and more even noise distribution.
For example hair on a character may need many samples, but the background may need very few.

With adaptive sampling it is also possible to render images with a target amount of noise.
This is done by settings the *Noise Threshold*, typical values are in the range from 0.1 to 0.001.
Then render samples can then be set to a high value,
and the renderer will automatically choose the appropriate amount of samples.

.. _bpy.types.CyclesRenderSettings.adaptive_threshold:
.. _bpy.types.CyclesRenderSettings.preview_adaptive_threshold:

Noise Threshold
   The error threshold to decide whether to continue sampling a pixel or not.
   Typical values are in the range from 0.1 to 0.001, with lower values meaning less noise.
   Setting it to exactly 0 lets Cycles guess an automatic value for it based on the total sample count.

.. _bpy.types.CyclesRenderSettings.adaptive_min_samples:
.. _bpy.types.CyclesRenderSettings.preview_adaptive_min_samples:

Min Samples
   The minimum number of samples a pixel receives before adaptive sampling is applied.
   When set to 0 (default), it is automatically set to a value determined by the *Noise Threshold*.


.. _render-cycles-settings-viewport-denoising:

Denoising
=========

Denoising removes noise while previewing scenes in *Rendered* mode in the 3D Viewport or for final renders.

.. _bpy.types.CyclesRenderSettings.use_denoising:
.. _bpy.types.CyclesRenderSettings.denoiser:

Render
   Denoising for the final render can be enabled or disabled with the checkbox.
   For denoising the image after rendering with the :doc:`Denoising node </compositing/types/filter/denoise>`,
   the :ref:`Data Render Passes <render_layers_passes_data>` also adapt to the selected denoiser.

   :OpenImageDenoise:
      Uses Intel's `Open Image Denoise <https://www.openimagedenoise.org/>`__,
      an AI denoiser. Typically provides the highest quality, and is the default.

   :OptiX:
      Uses NVIDIA's OptiX AI denoiser. Supports GPU acceleration on some
      older NVIDIA GPUs where OpenImageDenoise does not.

.. _bpy.types.CyclesRenderSettings.use_preview_denoising:
.. _bpy.types.CyclesRenderSettings.preview_denoiser:

Viewport
   Denoising for the *Rendered* mode in the 3D Viewport can be enabled or disabled for with the checkbox.

   :Automatic:
      Uses GPU accelerated denoising if supported, for best performance.
      Prefers OpenImageDenoise over OptiX.

   :OpenImageDenoise:
      Uses Intel's `Open Image Denoise <https://www.openimagedenoise.org/>`__,
      an AI denoiser. Typically provides the highest quality.

   :OptiX:
      Uses NVIDIA's OptiX AI denoiser. Supports GPU acceleration on some
      older NVIDIA GPUs where OpenImageDenoise does not.

.. _bpy.types.CyclesRenderSettings.preview_denoising_start_sample:

Start Sample
   Sample to start :ref:`denoising <render-cycles-settings-viewport-denoising>` in the 3D Viewport.

.. _bpy.types.CyclesRenderSettings.preview_denoising_input_passes:
.. _bpy.types.CyclesRenderSettings.denoising_input_passes:

Input Passes
   Controls which :doc:`Render Pass </render/layers/passes>` the denoiser should use as input,
   which can have different effects on the denoised image.
   Generally, the more passes the denoiser has to denoise the better the result.
   It is recommended to at least use *Albedo* as *None* can blur out details,
   especially at lower sample counts.

   :None: Denoises the image using color data.
   :Albedo: Denoises the image using color and albedo data.
   :Albedo + Normal: Denoises the image using color, albedo, and normal pass data.

.. _bpy.types.CyclesRenderSettings.preview_denoising_prefilter:
.. _bpy.types.CyclesRenderSettings.denoising_prefilter:

Prefilter
   Controls whether or not prefiltering is applied to *Input Passes* for use when denoising.
   Visible only when using *OpenImageDenoise*.

   :None:
      Does not apply any prefiltering to the input passes. This option retains the most detail and
      is the fastest, but assumes the input passes are noise free which may require a high sample count.
      If the input passes aren't noise free, then noise will remain in the image after denoising.
   :Fast:
      Assumes the input passes are not noise free, yet does not apply prefiltering to the input passes.
      This option is faster than *Accurate* but produces a blurrier result.
   :Accurate:
      Prefilters the input passes before denoising to reduce noise. This option usually produces
      more detailed results than *Fast* with increased processing time.

.. _bpy.types.CyclesRenderSettings.preview_denoising_use_gpu:
.. _bpy.types.CyclesRenderSettings.denoising_use_gpu:

Use GPU
   Perform denoising on the GPU.
   This is significantly faster than on CPU, but requires additional GPU memory.
   When large scenes need more GPU memory, this option can be disabled.

   See :doc:`GPU Rendering </render/cycles/gpu_rendering>` for details on
   supported GPU.


.. _bpy.types.CyclesRenderSettings.use_guiding:

Path Guiding
============

Path guiding helps reduce noise in scenes where finding a path to light is difficult for
regular path tracing, for example when a room is lit through a small door opening.
Important light directions are learned over time, improving as more samples are taken.
Guiding is supported for surfaces with diffuse BSDFs and volumes with isotropic
and anisotropic scattering.

.. note::

   - Path guiding is only available when rendering on a CPU.

   - While path guiding helps render caustics in some scenes, it is not designed for complex caustics
     as they are harder to learn and guide.

.. _bpy.types.CyclesRenderSettings.guiding_training_samples:

Training Samples
   The maximum number of samples to use for training. A value of 0 will keep training until
   the end of the render. Usually 128 to 256 training samples is enough for accurate guiding.
   Higher values can lead to a minor increases in guiding quality but with increased render times.

.. _bpy.types.CyclesRenderSettings.use_surface_guiding:

Surface
   Enable path guiding for the diffuse and glossy components of surfaces.

.. _bpy.types.CyclesRenderSettings.use_volume_guiding:

Volume
   Enable path guiding inside volumes.


Lights
======

.. _bpy.types.CyclesRenderSettings.use_light_tree:

Light Tree
   Use a light tree to more effectively sample lights in the scene, taking into account
   distance and estimated intensity.
   This can significantly reduce noise, at the cost of a somewhat longer render time per sample.

   Certain lighting properties are not accounted for in the light tree. This include custom
   falloff, ray visibility, and complex shader node setups including textures.
   This can result in an increase in noise in some scenes that make use of these features.

   Note, this feature is currently disabled for AMD GPUs on macOS.

.. _bpy.types.CyclesRenderSettings.light_sampling_threshold:

Light Threshold
   Probabilistically terminates light samples when the light contribution
   is below this threshold (more noise but faster rendering).
   Zero disables the test and never ignores lights.
   This is useful because in large scenes with many light sources,
   some lights might only contribute a small amount to the final image,
   and increase render times. Using this setting can decrease the render times
   needed to calculate the rays which in the end have very little effect on the image.


Advanced
========

.. _bpy.types.CyclesRenderSettings.seed:

Seed
   Seed value for integrator to get different noise patterns.

   .. _bpy.types.CyclesRenderSettings.use_animated_seed:

   Use Animated Seed (clock icon)
      Changes the seed for each frame. It is a good idea to enable this
      when rendering animations because a varying noise pattern is less noticeable.

.. _bpy.types.CyclesRenderSettings.sample_offset:

Sample Offset
   The number of samples to skip when starting render.
   This can be used to distribute a render across multiple computers
   then combine the images with ``bpy.ops.cycles.merge_images``

Scrambling Distance
   .. _bpy.types.CyclesRenderSettings.adaptive_scrambling_distance:

   Automatic
      Uses a formula to adapt the scrambling distance strength based on the sample count.

   .. _bpy.types.CyclesRenderSettings.preview_scrambling_distance:

   Viewport
      Uses the *Scrambling Distance* value for the viewport rendering.
      This will make the rendering faster but may cause flickering.

   .. _bpy.types.CyclesRenderSettings.scrambling_distance:

   Multiplier
      Lower values Reduce randomization between pixels to improve GPU rendering performance,
      at the cost of possible rendering artifacts if set too low.


.. _bpy.types.CyclesRenderSettings.min_light_bounces:

Min Light Bounces
   Minimum number of light bounces for each path,
   after which the integrator uses Russian Roulette to terminate paths that contribute less to the image.
   Setting this higher gives less noise, but may also increase render time considerably. For a low number of bounces,
   it is strongly recommended to set this equal to the maximum number of bounces.

.. _bpy.types.CyclesRenderSettings.min_transparent_bounces:

Min Transparent Bounces
   Minimum number of :ref:`transparent <render-cycles-light-paths-transparency>` bounces
   (more specifically "passthroughs"). Setting this higher reduces noise in the first bounces,
   but can also be less efficient for more complex geometry like hair and volumes.

.. _bpy.types.CyclesRenderSettings.use_layer_samples:

Layer Samples
   When render layers have per layer number of samples set, this option specifies how to use them.

   :Use: The render layer samples will override the set scene samples.
   :Bounded: Bound render layer samples by scene samples.
   :Ignore: Ignore render layer sample settings.


## Simplify


********
Simplify
********

.. reference::

   :Menu:      :menuselection:`Render --> Simplify`


.. rubric:: Common Settings

.. _bpy.types.RenderSettings.simplify_subdivision:

Max Subdivision
   Maximum number of subdivision by the Subdivision Surface modifiers.

.. _bpy.types.RenderSettings.simplify_child_particles:

Child Particles
   Show only a subset of all child hairs and particles.

.. _bpy.types.CyclesRenderSettings.texture_limit:

Texture Limit
   Automatically scales textures down so that they are no larger than the values chosen.
   This can help reduce computer memory resources when rendering large scenes with huge textures.


Viewport
========

See Common Settings above.

.. _bpy.types.RenderSettings.simplify_volumes:

Volume Resolution
   Resolution percentage of :doc:`volume objects </modeling/volumes/index>` in the viewport.
   This mostly affects memory usage rather than computation times.

.. _bpy.types.RenderSettings.use_simplify_normals:

Normals
   Skip computing custom normals and face corner normals for displaying meshes in the viewport.


Render
======

See Common Settings above.


.. _render-cycles-settings-scene-simplify-culling:

Culling
=======

.. _bpy.types.CyclesRenderSettings.camera_cull_margin:
.. _bpy.types.CyclesRenderSettings.use_camera_cull:

Camera Cull
   Automatically culls objects based on the camera frustum defined by the *Margin*.

.. _bpy.types.CyclesRenderSettings.distance_cull_margin:
.. _bpy.types.CyclesRenderSettings.use_distance_cull:

Distance Cull
   Automatically culls objects based on their distance from the active camera.
   This is set via the *Distance* property.


Grease Pencil
=============

.. _bpy.types.RenderSettings_simplify_gpencil_onplay:

Playback Only
   Activates the simplification process only during animation playback.

.. _bpy.types.RenderSettings_simplify_gpencil_view_fill:

Fill
   Shows the fill component in Grease Pencil materials.

.. _bpy.types.RenderSettings_simplify_gpencil_view_modifier:

Modifiers
   Shows Grease Pencil :doc:`modifiers </grease_pencil/modifiers/index>`.

.. _bpy.types.RenderSettings_simplify_gpencil_shader_fx:

Shader Effects
   Shows Grease Pencil :doc:`visual effects </grease_pencil/modifiers/index>`.

.. _bpy.types.RenderSettings_simplify_gpencil_tint:

Layer Tinting
   Shows layers tint overrides.

.. _bpy.types.RenderSettings.simplify_gpencil_antialiasing:

Anti-Aliasing
   Use :term:`Anti-Aliasing` to smooth stroke edges. The amount of anti-aliasing can be adjusted by
   the :ref:`Anti-Aliasing Threshold <bpy.types.SceneGpencil.antialias_threshold>`.


## Subdivision


***********
Subdivision
***********

.. reference::

   :Panel:     :menuselection:`Render --> Subdivision`

.. note::

   These settings are only available if :ref:`Experimental Feature Set <cycles-experimental-features>` is turned on.

These settings are used to control :doc:`Adaptive Subdivision </render/cycles/object_settings/adaptive_subdiv>`.

.. _bpy.types.CyclesRenderSettings.preview_dicing_rate:
.. _bpy.types.CyclesRenderSettings.dicing_rate:

Dicing Rate Render, Viewport
   Size of :term:`Micropolygons` in pixels for the final/viewport render.

.. _bpy.types.CyclesRenderSettings.offscreen_dicing_scale:

Offscreen Scale
   Multiplier for dicing rate of geometry outside of the camera view.
   The dicing rate of objects is gradually increased the further they are outside the camera view.
   Lower values provide higher quality reflections and shadows for off screen objects,
   while higher values use less memory.

.. _bpy.types.CyclesRenderSettings.max_subdivisions:

Max Subdivisions
   Stop subdividing when this level is reached even if the dicing rate would produce finer :term:`Tessellation`.

.. _bpy.types.CyclesRenderSettings.dicing_camera:

Dicing Camera
   Camera to use as reference point when subdividing geometry,
   useful to avoid crawling artifacts in animations when the scene camera is moving.


## Volumes


*******
Volumes
*******

.. reference::

   :Panel:     :menuselection:`Render --> Volumes`

Volume *Step* size is the distance between volume shader samples.
Cycles automatically estimates this distance based on voxel size in
volume objects and smoke simulations.

Render time can be reduced by increasing the step size, at the cost of
potentially losing some volume detail. For procedural volume shaders
that add detail, step size can be increased per object, material or world.

.. _bpy.types.CyclesRenderSettings.volume_step_rate:

Step Rate Render
   Global multiplier on the step size for all volumes in renders.
   Increase to reduce render time, at the cost of losing detail.

.. _bpy.types.CyclesRenderSettings.volume_preview_step_rate:

Viewport
   Global multiplier on the step size for all volumes in the viewport.
   Increase for more responsive viewport rendering.

.. _bpy.types.CyclesRenderSettings.volume_max_steps:

Max Steps
   Maximum number of steps through the volume before giving up,
   to protect from extremely long render times with big objects or small step sizes.


## Index

.. _bpy.types.SceneEEVEE:

#########
  EEVEE
#########

.. toctree::
   :titlesonly:
   :maxdepth: 2

   introduction.rst
   render_settings/index.rst
   scene_settings.rst
   world_settings.rst
   object_settings/index.rst
   material_settings.rst
   light_settings.rst
   light_probes/index.rst
   limitations/index.rst


## Introduction


************
Introduction
************

EEVEE is Blender's realtime render engine focused on speed and interactivity while achieving the
goal of rendering :abbr:`PBR (Physically Based Rendering)` materials.
EEVEE can be used interactively in the 3D Viewport but also produce high quality final renders.

.. figure:: /images/render_eevee_introduction_viewport.png

   EEVEE in the 3D Viewport -- "Tiger" by Daniel Bystedt.

EEVEE materials are created using the same shader nodes as Cycles, making it easy to render existing
scenes. For Cycles users, this makes EEVEE work great for previewing materials in realtime.

EEVEE is based on rasterization and is not a path tracer.
Instead of computing each ray of light, rasterization determines what surface is visible from the camera.
It then estimates the way light interacts with these surfaces and materials using numerous algorithms.
While EEVEE is designed to use :abbr:`PBR (Physically Based Rendering)` principles,
it is not perfect and Cycles will always provide more physically accurate renders.
For these reasons, EEVEE has a set of :doc:`limitations </render/eevee/limitations/index>`.

.. figure:: /images/render_eevee_introduction_final-render.png

   EEVEE final render -- "Temple" by Dominik Graf.


## Light Settings


**************
Light Settings
**************

.. reference::

   :Panel:     :menuselection:`Properties --> Light`
               :menuselection:`Shader Editor --> Sidebar --> Options`

Besides lighting from the background and materials with emission shaders,
lights are another way to add light into the scene.
The difference is that they are not directly visible in the rendered image,
and can be more easily managed as objects of their own type.

See :doc:`Light settings </render/lights/light_object>` for settings common to all renderers.

.. _bpy.types.Light.shadow:

Shadow
======

EEVEE uses a technique called *Virtual Shadow Mapping* along with *Shadow Map Raytracing*.
*Virtual Shadow Mapping* produces more accurate results than traditional shadow mapping by putting resolution
only where it is needed. It also includes a very efficient caching mechanism.
This technique offers better performance than ray tracing and is compatible
with any :ref:`Render Method <bpy.types.Material.render_method>`.

.. tip::
   - The error message *Shadow buffer full* means that the system cannot allocate enough shadow memory.
     Increasing the :ref:`Shadow Pool Size <bpy.types.SceneEEVEE.shadow_pool_size>` or
     the :ref:`Resolution Limit <bpy.types.Light.shadow_maximum_resolution>` on some lights
     can fix the issue. Otherwise, the only workaround is to disable shadow casting on some lights.
   - *Shadow Map Raytracing* can be tweaked in the :ref:`Render Settings <bpy.types.SceneEEVEE.use_shadows>`.
   - Turning on :ref:`Jitter <bpy.types.Light.use_shadow_jitter>` can reduce the light leaking artifacts
     caused by large lights and *Shadow Map Raytracing*.

.. seealso:: :ref:`Limitations <eevee-limitations-shadows>`.

.. _bpy.types.Light.use_shadow_jitter:

Jitter
   Enable jittered soft shadows to increase shadow precision.
   Has a high performance impact as the shadow map cannot be cached and needs to be updated for each render sample.

   .. note::

      The effect isn't visible by default in the viewport.
      See :ref:`render settings <bpy.types.SceneEEVEE.use_shadow_jitter_viewport>`.

.. _bpy.types.Light.shadow_jitter_overblur:

Overblur
   Apply shadow tracing to each jittered sample to reduce under-sampling artifacts.

   .. note::

      Any value higher than zero will result in a blurrier shadow and is not physically correct.

.. _bpy.types.Light.shadow_filter_radius:

Filter
   Blur shadow aliasing using :abbr:`PCF (Percentage Closer Filtering)` with a circular kernel.
   The effective world scale of the filter depends on the shadow map resolution at the shadowed pixel position.

   .. note::

      Any value bigger than 1px will increase the chances of light leaking artifacts.

.. _bpy.types.Light.shadow_maximum_resolution:

Resolution Limit
   Minimum size of a shadow map pixel. Higher values use less memory at the cost of shadow quality.
   Higher values also speed-up rendering of heavy scenes.
   Each shadow is scaled depending on the shadowed pixel on screen. This can create very sharp shadows
   but also requires a lot of memory if the shadowed pixel is close to the camera.
   This property limits the maximum amount of detail that the shadow map can capture.

   .. note::

      Reducing the shadow map resolution will increase the chances of light leaking artifacts.

.. _bpy.types.Light.use_absolute_resolution:

Absolute Resolution Limit
   Limit the resolution at 1 unit from the light origin instead of relative to the shadowed pixel.
   This makes :ref:`Resolution Limit <bpy.types.Light.shadow_maximum_resolution>`
   act as a regular shadow map pixel size.

   .. hint::

      With this option enabled, the following equation can be used to set the *Resolution Limit*
      with a desired resolution:

      .. math::

         resolution\_limit = 2 * \sqrt{2} / resolution

      The :math:`2 * \sqrt{2}` refers to the unit cube diagonal and
      :math:`resolution` refers to the desired resolution (e.g. 1024px).

   .. note::

      The setting :ref:`Absolute Resolution Limit <bpy.types.Light.use_absolute_resolution>`
      does not exist for Sun Light.


Influence
=========

These parameters modulate the intensity of the light depending on the shader type.
These are meant for artistic control, and any value other than 1.0 breaks
:abbr:`PBR (Physically Based Rendering)` rules.

.. _bpy.types.Light.diffuse_factor:

Diffuse
   Diffuse reflection intensity multiplier.

.. _bpy.types.Light.specular_factor:

Glossy
   Glossy light intensity multiplier.

.. _bpy.types.Light.transmission_factor:

Transmission
   Transmission light intensity multiplier.

.. _bpy.types.Light.volume_factor:

Volume Scatter
   Volume light intensity multiplier.


.. _bpy.types.Light.use_custom_distance:

Custom Distance
===============

If enabled, uses :ref:`Distance <bpy.types.Light.cutoff_distance>` as the custom attenuation distance
instead of global Light Threshold. In order to avoid long setup times, this distance is first computed
automatically based on a light threshold.
The distance is computed at the light origin and using the inverse square falloff.

.. _bpy.types.Light.cutoff_distance:

Distance
   Specifies where light influence will be set to 0.

.. note::

   The setting :ref:`Custom Distance <bpy.types.Light.use_custom_distance>` does not exist for Sun Light.

.. seealso::

   Global :ref:`Light Threshold <bpy.types.SceneEEVEE.light_threshold>`.


## Material Settings

*********
Materials
*********

.. seealso::

   While EEVEE shares the same material node system as Cycles, not all features are supported.
   See :ref:`Shader nodes limitations <eevee-limitations-materials>`.


Thickness
=========

.. reference::

   :Panel:     :menuselection:`Properties --> Material --> Thickness`

This feature is used to approximate the inner geometry structure of the object without heavy computation.
This is currently used for Subsurface, Translucent BSDF, Refraction BSDF and the nodes containing them.

If no value is plugged into the output node, a default thickness based on the smallest dimension of the object
is computed.
If a value is connected it will be used as object space thickness (i.e. scaled by object transform).
A value of zero will disable the thickness approximation and treat the object as having only one interface.

.. note::
   - The thickness is used to skip the inner part of the object.
   - Refraction will not refract objects inside the thickness distance.
   - Shadow casting object will not cast shadow within the thickness distance.

.. tip::
   - For large or compound meshes (e.g. vegetation),
     the thickness should be set to the thickness of individual parts (e.g. leaves, grass blades).
   - Thickness can be baked to textures or custom attributes for more accurate result.


Material Settings
=================

.. reference::

   :Panel:     :menuselection:`Properties --> Material --> Settings`

Pass Index
   Index number for the *Material Index* :doc:`render pass </render/layers/passes>`.
   This can be used to give a mask to a material which then can be read with
   the :doc:`ID Mask Node </compositing/types/mask/id_mask>` in the Compositor.

   .. note::

      :doc:`Volume Objects </modeling/volumes/introduction>` do not support the pass index.


.. _bpy.types.Material.surface:

Surface
=======

Backface Culling
   Backface Culling hides the back side of faces.
   This option should be turned on whenever it is possible, as it has an impact on performance.

   Camera
      Use back face culling to hide the back side of the face.

   Shadow
      Use back face culling when casting shadows.

   Light Probe Volume
      Use back face culling when baking :doc:`Light Probe Volumes </render/eevee/light_probes/volume>`.
      Additionally helps rejecting capture point inside the object to avoid light leaking

.. _bpy.types.Material.displacement:

Displacement
   Controls how the displacement output from the shader node tree is used.

   :Bump Only:
      Use Bump Mapping to simulated the appearance of displacement.
      This only modifies the shading normal of the object. Vertex position is not affected.
   :Displacement Only:
      This mode is not supported and falls back to *Displacement and Bump*.
   :Displacement and Bump:
      Combination of true displacement and bump mapping for finer details.
      Vertex position is modified.

   .. note::

      This type of displacement is not precomputed. It has a performance impact multiplied by the
      render sample count. However, the evaluation is much faster than doing it using geometry
      nodes or a displacement modifier.

   .. note::

      Displacing flat shaded geometry will split adjacent faces.
      This can be worked around by passing the vertex normals as a custom attribute.

Max Displacement
   The maximum distance a vertex can be displaced when using true displacement.
   Displacements over this threshold may cause visibility issues.
   These visibility issues can be observed when the object is out of view at the edge of screen
   with parts being displaced inside the view. The object would then disappear because of camera culling.
   This can also produce missing shadow updates where the displaced geometry is.

.. _bpy.types.Material.use_transparent_shadow:

Transparent shadows
   Use transparent shadows for this material if it contains a Transparent BSDF.
   Disabling will render faster but not give accurate shadows.

.. _bpy.types.Material.render_method:

Render Method
   Controls the blending and the compatibility with certain features.

   :Dithered:
      Allows for grayscale hashed transparency, and compatible with render passes and raytracing.
      Also know as deferred rendering.

      When using *Dithered* render method, the materials are rendered in layers.
      Each layer can only transmit (e.g. refract) light emitted from previous layers.
      If no intersection with the layers below exists, the transmissive BSDFs will fallback to light probes.

      Raytraced Transmission
         Use raytracing to determine transmitted color instead of using only light probes.
         This prevents the surface from contributing to the lighting of surfaces not using this setting.

   :Blended:
      Allows the colored transparency, but incompatible with render passes and raytracing.
      Also known as forward rendering.

      .. admonition:: Sorting Problem
         :class: important

         When using *Blended* render method, the order in which the color blending
         happens is important as it can change the final output color.
         EEVEE does not support per-fragment (pixel) sorting or per-triangle sorting.
         Only per-object sorting is available and is automatically done on all
         transparent surfaces based on object origin.
         Opaque surfaces (i.e. that have no transparency)
         will still have correct sorting regardless of the render method.

         .. tip::

            Face order can be adjusted in edit mode by using
            :doc:`sort element </modeling/meshes/editing/mesh/sort_elements>` or using a
            :doc:`geometry node </modeling/geometry_nodes/geometry/operations/sort_elements>`.

      .. note::

         Per-object sorting has a performance cost and having thousands of
         objects in a scene will greatly degrade performance.

.. _bpy.types.Material.use_transparency_overlap:

Transparency Overlap
   If enabled, all transparent fragments will be rendered.
   If disabled, only the front-most surface fragments will be rendered.
   This option can be disabled to fix sorting issues caused by blending order.
   Only available for the *Blended* render method.

.. _bpy.types.Material.thickness:

Thickness
   Determines what model to use to approximate the object geometry.

   :Sphere:
      Approximate the object as a sphere whose diameter is equal to the thickness defined by the node tree.
      This is more suited to objects with rounder edges (e.g. a monkey head), and is perfectly suited to spheres.
   :Slab:
      Approximate the object as an infinite slab of thickness defined by the node tree.
      This is more suited to very flat or thin objects (e.g. glass panels, grass blades).

From Shadow
   Use the shadow maps from shadow casting lights to refine the thickness defined by the material node tree.
   This takes the minimum thickness between the shadow map and the material node tree value.
   This is useful for objects where pre-computation is difficult (e.g. complex meshes), impossible
   (e.g. procedural geometry with displacement) or just impractical.
   However, this will have a performance impact that scale with the number of render samples.


Volume
======

.. _bpy.types.Material.volume_intersection_method:

Intersection
   Determines which inner part of the mesh will produce volumetric effect.

   :Fast:
      Each face is considered as a medium interface. Gives correct results for manifold geometry
      that contains no inner part.
   :Accurate:
      Faces are considered as medium interface only when they have different consecutive facing.
      Gives correct results as long as the max ray depth is not exceeded. Has significant memory
      overhead compared to the fast method.


## Scene Settings


**************
Scene Settings
**************

Light Probes
============

Light Probe Spheres Resolution
    Defines the resolution of every light probe sphere in the scene.


## World Settings


**************
World Settings
**************

The world environment can emit light, ranging from a single solid color
to arbitrary textures.

In EEVEE, the world lighting contribution is stored into an internal
:doc:`Light Probe </render/eevee/light_probes/index>`.
This makes the lighting less precise than Cycles.


.. _bpy.types.WorldMistSettings:

Mist Pass
=========

.. reference::

   :Panel:     :menuselection:`World --> Mist Pass`

.. note::

   The mist pass must be enabled in the View Layer tab
   of the :doc:`Properties Editor </editors/properties_editor>`
   before the settings below are available in the World tab.

Mist can greatly enhance the illusion of depth in your rendering. To create mist,
Blender generates a render layer with a depth map ranging between 0.0 and 1.0
that can be used in the Compositor to generate a mist effect.

.. _bpy.types.WorldMistSettings.start:

Start
   The distance from the camera at which the mist starts to fade in.

.. _bpy.types.WorldMistSettings.depth:

Depth
   The distance from *Start* of the mist, that it fades in over.
   Objects further from the camera than *Start + Depth* are completely hidden by the mist.

.. _bpy.types.WorldMistSettings.falloff:

Falloff
   The curve function that controls the rate of change of the mist's strength further and further into the distance.

   :Quadratic:
      Uses the same calculation as light falloff (:math:`1\over{x^2}`) and provides the smoothest
      transition from transparent (0.0) to opaque (1.0).
   :Linear: Has a steeper start than quadratic (:math:`1\over{x}`).
   :Inverse Quadratic:
      Has the steepest start (:math:`1\over{\sqrt{x}}`) and approaches 1.0 faster than the other two functions.

.. tip::

   A visualization can be activated in the :menuselection:`Camera --> Viewport Display` panel.

.. figure:: /images/render_cycles_world-settings_mist-example1-BI.jpg

   Mist example
   (`blend-file <https://archive.blender.org/wiki/2015/index.php/File:25-Manual-World-Mist-Example1.blend>`__).


Settings
========

.. reference::

   :Panel:     :menuselection:`World --> Light Probe`


Light Probe
-----------

.. _bpy.types.World.probe_resolution:

Resolution
   The resolution used to store the light from the world.
   This is equivalent to the resolution for light probe spheres.

.. seealso::

   :doc:`Light Probe Sphere </render/eevee/light_probes/sphere>`.

Sun
---

EEVEE can separate the light from intense light sources
(e.g. a sun from an outdoor :abbr:`HDRI (High Dynamic Range Imaging)`) and
replace them with a sun light. This increases the quality of the lighting as the internal light probes
alone cannot reproduce this type of lighting with enough precision.

Threshold
   If non-zero, the maximum value for world contribution that will be recorded inside the world light probe.
   The excess contribution is converted to a sun light.
   This reduces the light bleeding caused by very bright light sources.
   A value of zero will disable this feature and all lighting will be stored inside the internal light probes.

Angle
   Angular diameter of the extracted sun light as seen from the Earth.

Use Shadow
   Enable shadow casting on the extracted sun light.

.. seealso::

   The shadow properties control the extracted sun shadows.
   They are exactly the same as for a sun light object.
   :doc:`Light Properties </render/eevee/light_settings>`.


## Index

.. _bpy.ops.object.lightprobe:
.. _bpy.types.LightProbe:

################
  Light Probes
################

Light probe objects are used by EEVEE as support objects.

There are three different types of light probes.
Each type of light probe records the lighting at a different resolution and frequency.
Probes are used together to recover incoming light information when using ray tracing is not possible
(either for performance or for technical limitations).

These types of objects are only useful for EEVEE (and by extension, the Material Preview mode).


Types
=====

.. toctree::
   :maxdepth: 2

   Sphere <sphere.rst>
   Plane <plane.rst>
   Volume <volume.rst>


## Plane


*****************
Light Probe Plane
*****************

A light probe plane records the light incoming from a single direction for all visible points on a plane.
The specular reflection direction is the only one currently available.

This type of light probe is suited to smooth planar surfaces.

Each visible planar light probe increases the render time as the scene needs to be rendered for
each of them.

Light probe planes only work when the ray tracing method is set to ``Screen-Trace``.
When enabled, they accelerate the tracing process and complete the missing data from the screen space ray tracing.

.. note::

   Reflections and volumetrics are not supported inside Light probe planes.


Placement
=========

If Backface Culling is not enabled, snapping the light probe plane to the planar surface
will effectively capture the underside of the surface.

You can manually move the light probe plane above the surface enough for it to not appear in the capture.
Alternatively you can disable the light probe visibility in the object visibility panel.


Properties
==========

.. reference::

   :Panel:     :menuselection:`Object Data --> Probe`

.. _bpy.types.LightProbe.influence_distance:

Distance
   A probe object only influences the lighting of surfaces inside its influence zone.
   This influence zone is defined by the distance parameter and the object's scale.

   For light probe planes, the influence distance is the distance from the plane.
   Only surfaces whose normals are aligned with the Reflection Plane will receive the captured reflection.


Capture
-------

.. _bpy.types.LightProbe.clip_start:

Clipping Offset
   Define how far below the plane the near clip is when capturing the scene.
   Increasing this can fix reflection contact problems.


Viewport Display
----------------

.. reference::

   :Panel:     :menuselection:`Object Data --> Viewport Display`

Arrow Size
   Size of the arrow showing the reflection plane normal.

Capture
   Show the captured reflected image onto a fully reflective plane in the 3D Viewport.

Influence
   Show the influence bounds in the 3D Viewport.


## Sphere


******************
Light Probe Sphere
******************

A light probe sphere records the light incomming from many directions at a single location.

They are used for smooth and semi-rough reflections.
Sphere probes smoothly blend to light probe volume lighting for completely diffuse reflections.

If *Raytracing* is turned on, they are used as a fallback if a ray misses.

.. note::

   In both usages, the light probe spheres are shadowed by light probe volume.
   This is done in order to reduce light leaking in shadowed areas and reduce the need to
   setup more light probe spheres.

Adjusting their resolution is done inside the :doc:`Scene data </render/eevee/scene_settings>` panel.

The world also has an internal light probe sphere with a resolution that can be adjusted
in the :doc:`World data </render/eevee/world_settings>` panel.


Properties
==========

.. reference::

   :Panel:     :menuselection:`Object Data --> Probe`

Type
   Shape of the influence volume. Can be set to Sphere or Box.

Radius
   A probe object only influences the lighting of nearby surfaces.
   This influence zone is defined by the size parameter and object scaling.

Falloff
   Percentage of the influence distance in which the influence of a probe fades linearly.


Capture
-------

.. note::

   In the viewport, capture only happens if an update is detected on the light probe data or position.
   For renders, the capture happens at the start of each frame.

Clipping
   Define the near and far clip distances when capturing the scene.


Custom Parallax
---------------

.. reference::

   :Panel:     :menuselection:`Object Data --> Custom Parallax`

By default, the influence volume is also the parallax volume.
The parallax volume is a volume on which the recorded light is projected.
It should roughly fit it surrounding area. In some cases it may be better to
adjust the parallax volume without touching the influence parameters.
In this case, enable the *Custom Parallax* and
change the shape and radius of the parallax volume independently.


Viewport Display
----------------

Data
   Show the captured light using a reflective sphere of the given size.

Clipping
   Show the clipping distance in the 3D Viewport.

Influence
   Show the influence bounds in the 3D Viewport. The inner sphere is where the falloff starts.

Parallax
   Show the *Custom Parallax* shape in the 3D Viewport.


## Volume


******************
Light Probe Volume
******************

A volume probe records the light incomming from all directions at many locations inside a volume.

The light is then filtered and only the diffuse light is recorded.
The capture point positions are visible as an overlay when the Irradiance Volume object is selected.

If an object is not inside any Irradiance Volume, or if the indirect lighting has not been baked,
the world's diffuse lighting will be used to shade it.

.. tip::

   - When lighting indoor environments, try to align grids with the room shape.
   - Try not to put too much resolution in empty areas or areas with a low amount of lighting variation.
   - Bad samples can be fixed by adding a smaller grid near the problematic area.
   - Large scenes may require using many volumes with different level of details.


Properties
==========

.. reference::

   :Panel:     :menuselection:`Object Data --> Probe`

Intensity
   Intensity factor of the recorded lighting.
   Making this parameter anything other than 1.0 is not physically correct.
   To be used for tweaking, animating or artistic purposes.

.. rubric:: Sampling Bias

Normal Bias
   Offset sampling of the irradiance grid in the surface normal direction to reduce light bleeding.
   Can lead to specular appearance of diffuse surface if set too high.

View Bias
   Offset sampling of the irradiance grid in the viewing direction to reduce light bleeding.
   Can lead to view dependant result if set too high. Prefer this if camera is static.

Facing Bias
   When set to zero, avoids capturing points behind the shaded surface to bleed light onto
   the shaded surface. This produces non-smooth interpolation when the capture resolution is high.
   Increasing this bias will make the interpolation smoother but also introduce some light bleeding.


.. rubric:: Validity & Dilation

During the baking process, a validity score is assigned to each capture point.
This score is based on the number of back-faces hit when capturing the incoming lighting.
Only materials with *Single Sided* turned on for Light Probe Volumes will reduce the validity score.

Validity Threshold
   Capture points with validity below this threshold will be ignored during lighting interpolation.
   This remove the influence of capture points trapped inside closed geometry, reducing the artifacts they produced.

Dilation Threshold
   Capture points with validity below this threshold will have their data replaced using valid neighbors.

Radius
   Radius in capture points in which to search for a valid neighbor.


.. _eevee-lightprobe-volume-bake:

Bake
----

Light probe volume light data is static and needs to be manually baked.
Once baked, the data is stored inside the object data-block and can be moved, animated and linked
between blender files.

.. note::

   Baking uses the render visibility of the objects in the scene.

During baking, the scene is converted into a different representation to accelerate light transport.
This representation can be very memory intensive and prevents baking if it cannot fit inside the GPU memory.
There are a few way to deal with this issue:

- Larger scenes should be divided into smaller sections or use different level of details.
- Reduce *Surfel Resolution*.
- Turn off the light probe volume visibility option on objects that have little to no effect in the bake.

.. tip::

   The internal scene representation can be inspected using the ``Debug Value`` 3, 4 and 5.

Resolution
----------

Resolution X, Y, Z
   Spatial resolution for volumetric light probes is determined per probe.
   The local volume is divided into a regular grid of the specified dimensions.
   The lighting will be captured for each cell in this grid.

Bake Samples
   Number of ray directions to evaluate when baking.
   This increases the baking time proportionally to the size of the scene representation.

Surfel Resolution
   Number of surfels to spawn in one local unit distance.
   Higher values increase quality, but have a huge impact on memory usage.

   .. tip::

      A good value is twice the maximum *Resolution*.


Capture
-------

Capture Distance
   Distance around the light probe volume that will be captured during the bake.
   A distance of 0 will only considered the inside of the volume.

World Contribution
   Bake incoming light from the world instead of just visibility for more accurate lighting,
   but lose correct blending to surrounding irradiance volumes.

Indirect Light Contribution
   Capture light bounces from light source.

Emission Contribution
   Capture emissive surfaces when baking.


Clamping
^^^^^^^^

Direct Light
   Clamp incoming direct light. 0.0 disables direct light clamping.
   Here direct light refers to the light that bounces only once (from the light object)
   or light coming from emissive materials.

Indirect Light
   Clamp incoming indirect light. 0.0 disables indirect light clamping.
   Here indirect light refers to the light that bounces off a surface after the first bounce (from the light object)
   or during the first bounce if the light comes from emissive materials.

.. tip::

   Setting *Clamp Indirect* to a very non-zero value will effectively only record the first light bounce leading.


Offset
^^^^^^

In order to reduce artifacts caused by bad capture point positioning,
the bake process adjusts their location before capturing light.
It moves the capture points slightly away from surrounding surfaces and tries to move them out of objects
if they are not too far bellow the surface.

Surface Offset
   Distance to move the capture points away from surfaces.

Search Distance
   Distance to search for valid capture positions if the capture point is near the back-face of a single-sided object.

.. note::

   Only materials with *Single Sided* turned on for Light Probe Volumes will move capture point position.


Viewport Display
----------------

Data
   Show the captured light using small diffuse spheres of the given size.

Influence
   Show the influence bounds in the 3D Viewport. The inner sphere is where the falloff starts.

Clipping
   Show the clipping distance in the 3D Viewport.


## Index


###############
  Limitations
###############

.. toctree::
   :maxdepth: 2

   limitations.rst
   nodes_support.rst


## Limitations


***********
Limitations
***********

EEVEE's goal is to be an interactive render engine. Some features may not be there yet or
may be impossible to implement into EEVEE's architecture without compromising performance.

Here is a rather exhaustive list of all the limitations you can expect while working with EEVEE.


Attributes and Properties
=========================

- Only 14 attributes from Geometry Nodes are supported in a material
- Only 8 custom object properties are supported in a material


Cameras
=======

- Only perspective and orthographic projections are currently supported.


Lights
======

- Lights can only have one color and do not support light node trees.
- Unlike in Cycles, the :ref:`Size <bpy.types.SpotLight.shadow_soft_size>`
  of spot lights does not change the softness of the cone.
- The area light :ref:`Beam spread <bpy.types.SpotLight.spot_size>` option is not supported.


Light Probes
============

- EEVEE supports up to 128 active light probe spheres.
- EEVEE supports up to 16 active light probe planes inside the view frustum.
- Active light probe volumes must fit inside the
  :ref:`Light Probes Volume Memory Pool <bpy.types.SceneEEVEE.gi_irradiance_pool_size>`.

Indirect Lighting
=================

- Light probe capture does not support specular reflections. Specular energy is treated as diffuse.

.. _eevee-limitations-shadows:

Shadows
=======

- *Shadow Map Raytracing* can produce light leaking because of overlapping shadow casters.
  This can be mitigated by using lower :ref:`step count <bpy.types.SceneEEVEE.shadow_step_count>`, enabling
  :ref:`jitter <bpy.types.Light.use_shadow_jitter>`, or reducing the light shape size.
- Thin objects (e.g. walls without thickness) might have light leaking on the shadowed side.
  This can be mitigated by making the object have some thickness or lowering
  :ref:`Resolution Limit<bpy.types.Light.shadow_maximum_resolution>`.

.. _eevee-limitations-volumetrics:

Volumetrics
===========

- Only single scattering is supported.
- Volumetrics are rendered only for the camera "rays". They don't appear in reflections/refractions and probes.
- Volumetric shadowing only work in volumetrics. They won't cast shadows onto solid objects in the scene.
- Volumetric shadowing only work for volumes inside the view frustum.


.. _eevee-limitations-dof:

Depth of Field
==============

- Blended materials cannot be correctly handled by the post-processing blur,
  but will be correctly handled by the sample-based method. For this, you need to
  disable the post-process depth of field by setting the *Max Size* to 0.

.. _eevee-limitations-screenspace:

Screen Space Effects
====================

Ray-triangle intersection is not currently supported.
Instead of this, EEVEE uses the depth buffer as an approximated scene representation.
This reduces the complexity of scene scale effects and enables a higher performance.
However, only what is in inside the view can be considered when computing these effects.
Also, since it only uses one layer of depth, only the front-most pixel distance is known.

These limitations creates a few problems:

- The screen space effects disappear when reaching the screen border.
  This can be partially fixed by using the *overscan* feature.
- Screen space effects lack deep information (or the thickness of objects).
  This is why most effects have a thickness parameter to control how to consider potential intersected pixels.
- Objects behind other objects (occluded) are not considered by these effects.
- Blended surfaces are not considered by these effects.
  They are not part of the depth prepass and do not appear in the depth buffer.
- Objects that a part of :ref:`Holdout Collections <bpy.ops.outliner.collection_holdout_set>`
  will not be rendered with screen space effects.


.. _eevee-limitations-raytracing:

Raytracing
----------

- Blended materials and materials using raytrace refractions will not appear in dithered materials reflections.
- Blended materials are not compatible with raytracing.
- Only one refraction event is correctly modeled.
  An approximation of the second refraction event can be achieved using the
  :ref:`Thickness workflow <bpy.types.Material.thickness>`.
- Only dithered materials *not* using Raytrace Refractions can be refracted.


.. _eevee-limitations-materials:

Shader Nodes
============

- All BSDF's are using approximations to achieve realtime performance
  so there will always be small differences between Cycles and EEVEE.
- Some utility nodes are not yet compatible with EEVEE.
- Certain combinations of BSDF's will result in more noise than others.
  This is the case when mixing Diffuse BSDF and Refraction BSDF.
- Displacement of flat shaded surfaces will split the mesh into triangles.
  See :ref:`Displacement <bpy.types.Material.displacement>` for a workaround.

.. seealso::

   For a full list of unsupported nodes see :doc:`Nodes Support </render/eevee/limitations/nodes_support>`.


Memory Management
=================

In EEVEE, :abbr:`GPU (Graphic Processing Unit, also known as Graphics Card)`
Memory management is done by the GPU driver.
In theory, only the needed textures and meshes (now referred as "the resources") for a single draw call
(i.e. one object) needs to fit into the GPU memory.

So if the scene is really heavy,
the driver will swap things in and out to make sure all objects are rendered correctly.

In practice, using too much GPU memory can make the GPU driver crash, freeze, or kill the application.
So be careful of what you ask.

There is no standard way of estimating if the resources will fit into the GPU memory and/or
if the GPU will render them successfully.


CPU Rendering
=============

Being a rasterization engine, EEVEE only uses the power of
the :abbr:`GPU (Graphic Processing Unit, also known as Graphics Card)` to render.
There is no plan to support :abbr:`CPU (Central Processing Unit)` (software) rendering
as it would be very inefficient. CPU power is still needed to handle high scene complexity
as the geometry must be prepared by the CPU before rendering each frame.


Multiple GPU Support
====================

There is currently no support for
multiple :abbr:`GPU (Graphic Processing Unit, also known as Graphics Card)` systems.


Headless Rendering
==================

Headless rendering is not supported on headless Windows systems.


## Nodes Support

***************
Supported Nodes
***************

Most nodes are taken from Cycles. However, some features are missing and
may (or may not) be implemented in EEVEE in the future.

.. seealso::

   :doc:`Shader Nodes </render/shader_nodes/index>`.


EEVEE only Nodes
================

These nodes are only available if EEVEE is the active render engine. These nodes will not work in Cycles.


Shader to RGB
-------------

EEVEE supports the conversion of BSDF outputs into color inputs to make a wide variety of custom shading.
This is supported using the :doc:`Shader to RGB </render/shader_nodes/converter/shader_to_rgb>` node.
This node evaluates the lighting of the BSDFs connected to it just like a *Blended* material and inherits
its limitation.


Specular BSDF
-------------

This :doc:`node </render/shader_nodes/shader/specular_bsdf>` implements the specular workflow
found in other render engines.


Other Nodes Support
===================

If something is not listed here, it is supported.


Shader Nodes
------------

In the general case, shader nodes should behave more or less like in Cycles.
So be sure to check out the Cycles section of this manual for that.

.. seealso::

   :doc:`Materials </render/shader_nodes/shader/index>`.

Although most BSDFs are supported, many of them are approximations and are not feature complete.

Diffuse BSDF
   Roughness is not supported. Only Lambertian diffusion is supported.

Glass / Refraction BSDF
   Only supports GGX and Multiscatter GGX distribution.
   See :ref:`Raytracing limitations <eevee-limitations-raytracing>`.

Glossy BSDF
   Only supports GGX and Multiscatter GGX distributions.

Subsurface Scattering
   Random Walk sampling, IOR and Anisotropic are not supported.

Transparent BSDF
   Colored and additive transparency are only compatible with blended modes.

Translucent BSDF
   Does not diffuse the light inside the object. It only lights the object with reversed normals.

Principled BSDF
   Cumulative limitations from Diffuse BSDF, Glossy BSDF, Refraction BSDF and Subsurface Scattering.
   Anisotropy is not supported. The Sheen layer is a crude approximation.

Volume Absorption
   See :ref:`Volume Limitation <eevee-limitations-volumetrics>`.

Volume Scatter
   The anisotropy parameter will be mixed and averaged for all overlapping volumetric objects,
   which is not physically correct and differs from Cycles.
   Also see :ref:`Volume Limitation <eevee-limitations-volumetrics>`.

Principled Volume
   Same as Volume Scatter. See :ref:`Volume Limitation <eevee-limitations-volumetrics>`.

Holdout
   Partially supported, using dithered mode may give incorrect results.

Anisotropic BSDF
   Not supported.

Toon BSDF
   Not supported.

Hair BSDF
   Not supported.

Sheen BSDF
   Not supported.

Principled Hair BSDF
   Not supported.


Input Nodes
-----------

Ambient Occlusion
   The *Only Local* option is not supported.

Geometry
   Pointiness is not supported.

Random per Island
   Random per Island is not supported.

Attribute
   Defaults to active UV layer. Only "density", "color", "flame" and "temperature" built-in Geometry attributes
   are supported. UVs and Color Attributes are supported.
   Only up to 8 Object or Instancer attributes per material (both types share the same limit), and 512 View Layer
   attributes per scene are supported.

Bevel
   Not supported.

Curves Info
   The Random output uses a different :abbr:`RNG (Random Number Generator)` algorithm.
   Range and statistical distribution of the values should be the same but the values will be different.

Light Path
   EEVEE has no real concept of rays. But in order to ease the workflow between Cycles and EEVEE
   some of the outputs are only supported in particular cases.
   This node makes it possible to tweak indirect lighting in the shader.

   - *Is Camera*: Supported.
   - *Is Shadow*: Supported.
   - *Is Diffuse*: Set to 1.0 when baking light probe volume. Otherwise is set to 0.0.
   - *Is Glossy*: Set to 1.0 when baking light probe sphere or plane. Otherwise is set to 0.0.
   - *Is Singular*: Not supported. Same as Is Glossy.
   - *Is Reflection*: Not supported. Same as Is Glossy.
   - *Is Transmission*: Not supported. Same as Is Glossy.
   - *Ray Length*: Not supported. Defaults to 1.0.
   - *Ray Depth*: Not supported. Defaults to 0.0.
   - *Diffuse Depth*: Partially supported. Set to 1.0 when baking light probe volume. Otherwise is set to 0.0.
   - *Glossy Depth*: Partially supported. Set to 1.0 when baking light probe sphere or plane. Otherwise is set to 0.0.
   - *Transparent Depth*: Not supported. Defaults to 0.
   - *Transmission Depth*: Not supported. Same as Glossy Depth.

   .. note::

      *Is Glossy* does not work with Screen Space Reflections/Refractions
      but does work with reflection planes (whether used with SSR or not).

Particle Info
   Not supported.

Texture Coordinate
   *From Instancer* is not supported.

UV Map
   *From Instancer* is not supported.

Wireframe
   Pixel size option does not give exactly the same output as Cycles. The width can be a bit different.


Texture Nodes
-------------

Most texture nodes are supported except for the exceptions listed below:

:abbr:`IES (Illuminating Engineering Society)` Texture
   Not supported.

Image Texture
   Smart Interpolation always uses Cubic interpolation.
   Artifact present using Tube or Sphere projection with linear interpolation.
   This is due to hardware mip-mapping and Anisotropic filtering.
   This kind of artifact will be also visible if the texture coordinates provided are not continuous.
   Using Box projection with *Extend type* set to Clip or Extend is not supported.
   Instead, it will always use Repeat.

Point Density
   Not supported.

Sky Texture
   In Nishita mode, the *Sun Disc* property is not supported.


Other Nodes
-----------

Light Falloff
   Not supported.


## Index


###################
  Object Settings
###################

Settings for objects and object data.

.. toctree::
   :maxdepth: 2

   object_data.rst


## Object Data


******
Object
******

Ray Visibility
==============

Objects can be set to be invisible to particular ray types.
This can be used, for example, to make an emitting mesh invisible to camera rays.
For instanced objects, visibility is inherited; if the parent object is hidden for some ray types,
the children will be hidden for these too.

In terms of performance, using these options is more efficient that using a shader node setup
that achieves the same effect.

.. _bpy.types.Object.visible_shadow:

Shadow
   Enables the object to cast shadows. The object will not be capture inside the shadow maps.


Light Probes
============

Objects can be set to not be captured by certain :doc:`light probe </render/eevee/light_probes/index>`.
This can be used, for example, to avoid animated object being recorded into static light probes.
For instanced objects, visibility is inherited; if the parent object is hidden for some ray types,
the children will be hidden for these too.

.. _bpy.types.Object.hide_probe_volume:

Volume
   Makes the object visible during light probe volumes :ref:`baking <eevee-lightprobe-volume-bake>`.

.. _bpy.types.Object.hide_probe_sphere:

Sphere
   Makes the object visible during light probe sphere capture.

.. _bpy.types.Object.hide_probe_plane:

Plane
   Makes the object visible during light probe plane capture.


## Clamping


********
Clamping
********

.. reference::

   :Panel:     :menuselection:`Render --> Clamping`


Surface
=======

.. _bpy.types.SceneEEVEE.clamp_surface_direct:

Direct Light
   This option limits the maximum light intensity a surface can reflect.
   It reduces :term:`Aliasing` noise and :term:`Fireflies` at the cost of accuracy.
   Setting this option to 0.0 disables clamping altogether.
   Lower values have a greater effect on the resulting image than higher values.

.. _bpy.types.SceneEEVEE.clamp_surface_indirect:

Indirect Light
   Similar to **Direct Light** but limits the maximum light intensity reflected using ray-tracing and light-probes.

.. note::

    These options provide a way to limit :term:`Fireflies` and :term:`Aliasing`
    of highly reflective surfaces and dense volumes.
    However, note that as you clamp out such values, other bright lights will be dimmed as well.

    Care must be taken when using this setting to find a balance between mitigating fireflies and
    losing intentionally bright parts.


Volume
======

.. _bpy.types.SceneEEVEE.clamp_volume_direct:

Direct Light
   The same as *Surface Direct Light* but for volume direct lighting.

.. _bpy.types.SceneEEVEE.clamp_volume_indirect:

Indirect Light
   The same as *Surface Direct Light* but for volume indirect lighting.


## Curves


******
Curves
******

.. reference::

   :Panel:     :menuselection:`Render --> Curves`

.. _bpy.types.RenderSettings.hair_type:
.. _bpy.types.RenderSettings.hair_subdiv:

.. Editor's Note: This part of this page gets copied into:
.. - :doc:`</render/cycles/render_settings/hair>`

.. --- copy below this line ---

Shape
   :Strand:
      Render curves as a thin strand roughly a pixel wide.
      Curve diameter parameters are ignored with this setting.

   :Strip:
      Render curves as a flat ribbon with rounded normals.

Additional Subdivisions
   Additional subdivisions to be applied on top of the curve resolution set in the
   hair system settings. Increasing this value will smooth out the curves of the strands.


## Depth Of Field

.. _bpy.types.SceneEEVEE.bokeh:

**************
Depth of Field
**************

To render a scene, EEVEE uses a pinhole camera model which produces
a perfectly focused image of the scene. For an enhanced realism, EEVEE can simulate
the optical :term:`Depth of Field` using a post-process filter, and a sample-based method.
The optical settings are located in the :doc:`camera settings </render/cameras>` properties.
Whereas the quality of the effect can be controlled by the settings found in the present section.

.. note::

   In the 3D Viewport, depth of field only works while in Camera View.

The post-process method is computed in two passes.
The first pass is using a blur that fails to produce quality bokeh for highlights but works for the general case.
Followed by a second pass which is sprite-based and improves only the quality of very bright highlights.
That is because it is too slow to be applied on every part of the image.
So it just includes very bright isolated parts of the image such that are different from their surroundings.
Which pixels are being processed by second pass can be control with
the *Sprite Threshold* and *Neighbor Rejection* options.

Secondly the sample-based method works by randomizing the camera position for every sample.
It is more accurate but needs many samples to achieve a smooth result.
Accordingly the post-process blurring radius is scaled down to remove undersampling.
Yet some scenes might still need more post-process blur in order to remove the noticeable sample pattern.
This is exactly what the *Overblur* option does, but it will also reduce the bokeh shape sharpness.

.. reference::

   :Panel:     :menuselection:`Render --> Depth of Field`

Max Size
   Maximum size in pixels of the depth of field post-process effect (lower is faster).
   A value of 0 will disable the post-process effect but not the sample-based method.

Sprite Threshold
   Minimum brightness a pixel needs to have to be considered by the sprite-based depth of field.
   Higher values will improve the performance but will also reduce the quality of highlights.
   Brightness is in the scene's referred color space.

Neighbor Rejection
   Maximum intensity to consider when doing sprite neighborhood rejection.
   This should be set to a brightness value above which there is
   small visual differences to be noticeable after color management.
   Lower values will improve the performance but will also reduce the quality of highlights.
   Brightness is in the scene's referred color space.

.. _bpy.types.SceneEEVEE.use_bokeh_jittered:

Jitter Camera
   Randomize the camera position for every scene render sample to increase precision.
   Enabling this option can change the scene's actual sample count.

   .. note::

      Be aware that the actual sample count can grow quite rapidly.

   .. hint::

      The actual number of samples is computed by the following formula:

      .. math::

         sample\_count = (ring\_count^{2} + ring\_count) * 3 + 1

      where :math:`ring\_count` is the number of ring in the hexaweb pattern.
      The :math:`ring\_count` is chosen so that the entire pattern contains at least the number of
      samples set in the :doc:`Render Settings </render/eevee/render_settings/sampling>`.

Over-blur
   Scales the post-process depth of field radius to reduce artifacts. Higher values will soften the bokeh shape.

.. seealso:: :ref:`Limitations <eevee-limitations-dof>`.


## Film


****
Film
****

Filter Size
   Due to limited resolution of images and computer screens, pixel filters are needed to avoid :term:`Aliasing`.
   This is achieved by slightly blurring the image to soften edges.

   This Setting controls how much the image is softened;
   lower values give more crisp renders, higher values are softer and reduce aliasing.

Transparent
   Render the background transparent, for compositing the image over another background after rendering.

.. _bpy.types.SceneEEVEE.use_overscan:
.. _bpy.types.SceneEEVEE.overscan_size:

Overscan
   Percentage of the render size to add to the internal render buffer.
   This will have a serious impact on performance but can fix
   render glitches around the perimeter of the rendered image.


## Grease Pencil


*************
Grease Pencil
*************

.. reference::

   :Panel:     :menuselection:`Render --> Grease Pencil`

This panel is comprised of settings to control the rendering of :doc:`Grease Pencil Lines </grease_pencil/index>`.

Anti-Aliasing Threshold
   Threshold for the edge detection algorithm used to correct aliasing,
   higher values might over blur some part of the image.


## Index


###################
  Render Settings
###################

.. toctree::
   :titlesonly:
   :maxdepth: 2

   sampling.rst
   clamping.rst
   raytracing.rst
   volumes.rst
   curves.rst
   depth_of_field.rst
   motion_blur.rst
   film.rst
   performance.rst
   grease_pencil.rst


## Motion Blur

.. _bpy.types.SceneEEVEE.motion_blur:

***********
Motion Blur
***********

.. reference::

   :Panel:     :menuselection:`Render --> Motion Blur`

Blender's animations are by default rendered as a sequence of *perfectly still* images.
While great for stop-motion and time-lapses, this is unrealistic, since fast-moving
objects do appear to be blurred in the direction of motion,
both in a movie frame and in a photograph from a real-world camera.

.. note::

   Motion blur is only visible in the viewport during animation playback and uses a simpler
   algorithm than final render. Same thing applies to :ref:`Viewport Renders <bpy.ops.render.opengl>`.

Position
   Controls at what point the shutter opens in relation to the current frame.

   :Start on Frame: Shutter is starting to open at the current frame.
   :Center on Frame: Shutter is fully opened at the current frame.
   :End on Frame: Shutter is fully closed at the current frame.

Shutter
   Time (in frames) taken between shutter open and close.

Bleeding Bias
   Used by the post-process blur to avoid blurring the background over the foreground.
   Lower values will reduce background bleeding onto foreground elements.

Max Blur
   Max Blur is intended to act as an optimization tool by
   limiting the number of pixels across which the blur is calculated.

Steps
   This controls the number of steps used by the accumulation blur and thus its accuracy.
   More steps means longer render time.

   .. note::

      When using multiple time steps, the render sample count is rounded up to the next multiple
      of steps to ensure even distribution of samples across steps.

   EEVEE splits the render into multiple time steps and accumulates the result
   which is known as Accumulation Motion Blur.
   This technique is precise but requires many steps for clean gradients.
   This is used in combination with the post-process blur to handle the inter-step gaps.
   Each step corresponds to a full scene re-evaluation and can add a lot of overhead to the render time.
   By adding more steps you can also reduce the *Max Blur* options because the post-process blur
   has to cover a smaller distance.

Shutter Curve
   Use a custom shutter curve.


Example
=======

.. _fig-render-motion-blur-properties-example:

.. list-table::

   * - .. figure:: /images/render_eevee_render-settings_motion-blur_1step-nofx.png
          :width: 310px

          No motion blur.

     - .. figure:: /images/render_eevee_render-settings_motion-blur_1step-fx.png
          :width: 310px

          Only post-process blur.

   * - .. figure:: /images/render_eevee_render-settings_motion-blur_4step-nofx.png
          :width: 310px

          4 time steps without post-process blur.

     - .. figure:: /images/render_eevee_render-settings_motion-blur_4step-fx.png
          :width: 310px

          4 time steps with post-process blur.

   * - .. figure:: /images/render_eevee_render-settings_motion-blur_32step-nofx.png
          :width: 310px

          32 time steps without post-process blur.

     - .. figure:: /images/render_eevee_render-settings_motion-blur_32step-fx.png
          :width: 310px

          32 time steps with post-process blur.


## Performance


***********
Performance
***********

.. reference::

   :Panel:     :menuselection:`Properties --> Render --> Performance`

.. _bpy.types.RenderSettings.use_high_quality_normals:

High Quality Normals
   Uses higher precision normals and tangents which can improve
   visual quality for dense meshes with high frequency textures at the cost of memory.


Memory
======

.. _bpy.types.SceneEEVEE.shadow_pool_size:

Shadow Pool Size
   A bigger pool size allows for more shadows in the scene but might not fit into GPU memory and decreases
   performance.
   Increasing it might fix the *Shadow buffer full* error.

   .. seealso::
      :ref:`Shadow documentation <bpy.types.Light.shadow>`

.. _bpy.types.SceneEEVEE.gi_irradiance_pool_size:

Light Probes Volume Pool Size
   A bigger pool size allows for more irradiance grids in the scene but might not fit into GPU memory and
   decreases performance.


Viewport
========

.. _bpy.types.RenderSettings.preview_pixel_size:

Pixel Size
   Option to control the resolution for viewport rendering.
   Allows you to speed up viewport rendering, which is especially useful for displays with high DPI.


## Raytracing

.. _bpy.types.SceneEEVEE.raytracing:
.. _bpy.types.RaytraceEEVEE:
.. _bpy.types.SceneEEVEE.use_raytracing:

**********
Raytracing
**********

.. reference::

   :Panel:     :menuselection:`Render --> Raytracing`

The ray-tracing pipeline goal is to increase the accuracy of surface indirect lighting.
This is done by generating ray from each :abbr:`BSDF (Bidirectional Scattering Distribution Function)`
and finding their intersection with the scene individually.

When disabled, it is replaced by a faster pipeline that uses pre-filtered light-probes.
This fallback mode offers a more visually stable and optimized alternative when visual
fidelity is not the primary goal.

.. seealso::

   :ref:`Limitations <eevee-limitations-raytracing>`.

Method
   Determine the tracing method used to find scene-ray intersections and indirect lighting.

   :Light Probe:
      Use light-probe spheres and planes to find scene intersection.
      This option has the lowest tracing cost but relies on manually placed light-probes.
   :Screen-Trace:
      Trace ray against the screen depth buffer. Fallback to light-probes if ray exits the view.

Resolution
   Resolution at which the ray-tracing is performed.
   Lower options will be faster and use less memory but will produce blurrier results.

Max Roughness
   Maximum roughness a :abbr:`BSDF (Bidirectional Scattering Distribution Function)` can have to use ray-tracing.
   BSDFs with higher roughness will progressively use the *Fast GI Approximation*.
   A value of 1 will raytrace every surfaces and disable the Fast GI.


Screen Tracing
==============

These settings control the behavior of the screen space ray-tracing.
They are only visible if *Screen-Trace* is the active tracing *Method*.

Precision
   Higher values increase precision of the screen space ray-tracing but lower the maximum trace distance.
   Increased precision also increases performance cost.

Thickness
   How thick to consider the pixels of the depth buffer during the tracing.
   Higher values will stretch the reflections and add flickering. Lower values may make the ray miss surfaces.


.. _bpy.types.RaytraceEEVEE.use_denoise:

Denoising
=========

Denoising can be enabled to reduce the amount of noise from the raw ray-traced output.
This can help image stability but will also over-blur the final ray-traced output.

Spatial Reuse
   Reuse the rays from neighbor pixels.
   Can introduce some light leaks across surfaces.

Temporal Accumulation
   Accumulate samples by re-projecting the last ray tracing results.
   This removes :term:`Fireflies` but also introduces color bias.
   Useful for viewport temporal stability or making renders converge faster.

Bilateral Filter
   Blur the resolved ray-traced output using a bilateral filter.


.. _bpy.types.SceneEEVEE.fast_gi:

Fast GI Approximation
=====================

Fast GI Approximation is a fallback to the ray-tracing pipeline for
:abbr:`BSDF (Bidirectional Scattering Distribution Function)` with high roughness.
It produces a less noisy output and captures bounce lighting more efficiently than individually traced rays.

This is currently implemented as a screen space effect and will inherit all associated
:ref:`limitations <eevee-limitations-screenspace>`.

Method
   Determine the method used to compute the fast GI approximation.

   :Ambient Occlusion:
      Use scene intersections to shadow the distant lighting from light-probes.
      This is the fastest option.
   :Global Illumination:
      Compute global illumination taking into account light bouncing off surrounding objects.

Resolution
   Resolution at which the fast GI is computed.
   Lower options will be faster and use less memory but will produce blurrier results.

Rays
   Number of GI rays per pixel at the specified *Resolution*.
   Higher values will reduce noise.

Steps
   Number of screen samples per GI ray.
   Higher values will reduce the noise amount and increase the quality.

   .. tip::

      With a higher step count, there is less chance to miss other surfaces that could reflect or block the light.
      This means that the Fast GI *Thickness* parameters can be tweaked to lower values without losing too much light
      bounce energy.

Precision
   Higher values increase the precision of the scene intersections with the GI rays.
   Increased precision also increases performance cost.

Distance
   If non-zero, the maximum distance at which other surfaces will contribute to the fast GI approximation.

Thickness Near
   Geometric thickness of the surfaces when computing fast GI and ambient occlusion.
   Reduces light leaking and missing contact occlusion.
   The effectiveness decreases proportionally to the distance from the shading point,
   following the inverse square law.

Far
   Angular thickness of the surfaces when computing fast GI and ambient occlusion.
   Reduces energy loss and missing occlusion of far geometry.
   Higher values will make the very thin objects block or reflect too much light.

Bias
   Bias the shading normal to reduce self intersection artifacts.


## Sampling


********
Sampling
********

EEVEE uses a process called Temporal Anti-Aliasing (TAA) which reduces :term:`Aliasing`.
TAA is sample based so the more samples the more aliasing is reduced at the cost of performance.

.. reference::

   :Panel:     :menuselection:`Render --> Sampling`


Viewport
========

.. _bpy.types.SceneEEVEE.taa_samples:

Samples
   The number of samples to use in the 3D Viewport.
   When setting this to zero the viewport will be resampled continuously.

.. _bpy.types.SceneEEVEE.use_taa_reprojection:

Temporal Reprojection
   Reduces noise while moving the viewport or during animation playback. Can leave some ghosting.

.. _bpy.types.SceneEEVEE.use_shadow_jitter_viewport:

Jittered Shadows
   Enable jittered shadows on the viewport.
   Jittered shadows are always enabled for final renders.
   This also affects shadows casted by transparent shadows.


Render
======

.. _bpy.types.SceneEEVEE.taa_render_samples:

Samples
   The number of samples to use in the final render.


.. _bpy.types.SceneEEVEE.use_shadows:

Shadows
=======

.. _bpy.types.SceneEEVEE.shadow_ray_count:

Rays
   Number of rays to trace for each light.
   Higher values reduces the noise caused by random shadow sampling.

.. _bpy.types.SceneEEVEE.shadow_step_count:

Steps
   Number of shadow map sample per shadow ray.
   Higher step count results in softer shadows but have a higher cost.

.. _bpy.types.SceneEEVEE.use_volumetric_shadows:

Volumetric Shadows
   Approximate light absorption of the surrounding volume objects. This makes the volumes more opaque to light.
   This is a very computationally expensive option and has limitations.

   .. _bpy.types.SceneEEVEE.volumetric_shadow_samples:

   Steps
      Number of steps to compute volumetric shadowing.

   .. seealso:: :ref:`Volume Limitations <eevee-limitations-volumetrics>`.

.. _bpy.types.SceneEEVEE.shadow_resolution_scale:

Resolution
   Resolution percentage of shadow maps.


Advanced
========

.. _bpy.types.SceneEEVEE.light_threshold:

Light Threshold
   Minimum light intensity for a light to contribute to the lighting.
   Used to compute the distance at which to cut-off lights influence.
   Lower values improve performance.

   .. seealso::

      :ref:`Custom Distance <bpy.types.Light.use_custom_distance>` overrides this setting.


## Volumes

.. _bpy.types.SceneEEVEE.volumetric:

***********
Volumetrics
***********

EEVEE simulates volumetric scattering by evaluating all volume objects inside the view frustum.

To achieve this, EEVEE uses several 3D textures which have a high video memory usage.
The texture dimensions can be tweaked using the *Resolution* and *Steps* parameters.

.. reference::

   :Panel:     :menuselection:`Properties --> Render --> Volumetrics`

Resolution
   Controls the quality of the volumetric effects. Lower resolution increases video memory usage and quality.

Steps
   Number of steps to compute volumetric effects. Higher count increases video memory usage and quality.
   These samples are distributed along the view depth (view Z axis).

Distribution
   Blend between linear and exponential sample distribution. Higher values put more samples near the camera.

Max Depth
   Maximum surface intersection count used by accurate volume intersection method.
   Will create artifacts if it is exceeded.


Custom Range
============

When working with volume objects, EEVEE automatically computes the best depth range where to compute
the volume sampling and lighting.
In certain situations, this isn't enough and produces sub-optimal sampling which increases noise.
This is particularly the case when using a volume shader inside the *World* or when working with large
number of volume objects.
The custom depth range can be enabled to restrict the computation of volumes to a certain range along
the camera depth and thus increase precision.

Start
   Start distance of the volumetric effect.

End
   End distance of the volumetric effect.

.. seealso:: :ref:`Limitations <eevee-limitations-volumetrics>`.


## Index

.. _render-freestyle:
.. _bpy.types.Freestyle:

#############
  Freestyle
#############

.. toctree::
   :maxdepth: 2

   introduction.rst
   render.rst
   view_layer/index.rst
   material.rst
   python.rst


## Introduction


************
Introduction
************

Freestyle is an edge/line-based non-photorealistic (NPR) rendering engine.
It relies on mesh data and Z-depth information to draw lines on selected edge types.
Various line styles can be added to produce artistic ("hand drawn", "painted", etc.)
or technical (hard line) looks.

Freestyle can generate a powerful diversity of line styles and results.
There are currently, two ways to define the way lines look;
the first uses a series of parameter to create a :doc:`Line Style </render/freestyle/view_layer/line_style/index>`.
This mode allows intuitive editing of features such as dotted lines
and easy setup of multiple line types and edge definitions.
On top of all of that, with line style modifiers, the sky is the limit!

The second method of generating lines is by using :doc:`Python Scripting </render/freestyle/python>`.
This method is much more advanced but Blender includes many pre-scripted styles
such as Japanese big brush, cartoon, blueprint, and thickness-with-depth.

.. list-table::

   * - .. figure:: /images/render_freestyle_introduction_example-1.png

          ATV buggy by Rylan Wright (RONIN). CC BY.
          (`File:AtvBuggy.zip <https://archive.blender.org/wiki/2015/index.php/File:AtvBuggy.zip>`__)

     - .. figure:: /images/render_freestyle_introduction_example-2.png

          By mato.sus304. CC BY-SA.
          (`File:Mato_sus304_cut02.zip
          <https://archive.blender.org/wiki/2015/index.php/File:Mato_sus304_cut02.zip>`__)

   * - .. figure:: /images/render_freestyle_introduction_example-3.png

          A cartoon scene from `OHA Studio <https://oha-studios.com/>`__
          © Mechanimotion Entertainment.
          (`blend-file <https://download.blender.org/demo/test/freestyle_demo_file.blend.zip>`__)

     - .. figure:: /images/render_freestyle_introduction_example-4.png

          Blueprint render of Martin M-130 from 1935 by LightBWK. CC0. Warning:
          heavy file! designed for stress test Blender to the limits and may crash Blender.
          (`File:M-130Blueprint.zip <https://archive.blender.org/wiki/2015/index.php/File:M-130Blueprint.zip>`__)


The Big Picture
===============

- Activate Freestyle by the :menuselection:`Properties --> Render --> Freestyle` checkbox.
- Freestyle settings are located in the :doc:`View Layer </scene_layout/view_layers/index>` properties.
- One view layer can only have one view map. A view map holds the edge detection settings
  (Crease Angle, Culling toggle, Face Smoothness toggle, Material Boundaries toggle,
  Sphere Radius, and Kr Derivative Epsilon advanced options).
- A view map can have multiple Line Sets.
- A line set controls which line types and selections will be rendered, from lines based on your scene.
- Each line set uses one line style (which can be shared between multiple Line Sets).
- A line style tells Freestyle how to render the linked Line Sets in terms of color, alpha,
  thickness and other aspects.

.. figure:: /images/render_freestyle_introduction_view-map-processes.png

   Block diagram of Freestyle view map and processes.


Known Limitations
=================

- Highly memory demanding: All mesh objects in a view layer are loaded at once.
- Only faced mesh objects are supported.
- No edges at face intersections are detected yet.
- Freestyle rendering results do not have any Z depth information.
- Panoramic cameras are not supported.


## Material


*******************
Material Properties
*******************

.. reference::

   :Panel:     :menuselection:`Properties --> Material --> Freestyle Line`

.. _bpy.types.Material.line_color:

Line Color
   Specifies the line colors on a per-material basis.

.. _bpy.types.Material.line_priority:

Priority
   Specify the ordering of competing line colors at material boundaries.

.. seealso::

   A use case of the line color priority is detailed in a Freestyle development
   `blog article <https://freestyleintegration.wordpress.com/2014/07/07/line-color-priority/>`__.


## Python

.. _bpy.types.FreestyleModuleSettings:

****************
Python Scripting
****************

The Python Scripting mode offers full programmable line stylizes.
In this control mode, all styling operations are written as Python scripts referred to as
style modules in the Freestyle terminology. The input to a style module is a view map
(i.e. a set of detected feature edges), and the output is a set of stylized strokes.

A style module is composed of successive calls of five basic operators: selection, chaining,
splitting, sorting and stroke creation. The selection operator identifies a subset of input
feature edges based on one or more user-defined selection conditions (predicates).
The selected edges are processed with the chaining,
splitting and sorting operators to build chains of feature edges. These operators are also
controlled by user-supplied predicates and functions in order to determine how to transform
the feature edges into chains. Finally, the chains are transformed into stylized strokes
by the stroke creation operator, which takes a list of user-defined stroke shaders.

Python style modules are stored within blend-files as text data-blocks.
External style module files first need to be loaded in the Text Editor.
Then the select menu within an entry of the style module stack
allows you to select a module from the list of loaded style modules.

.. figure:: /images/render_freestyle_python_scripting-mode.png

   A screen capture of a style module ``cartoon.py`` loaded in the Text Editor (left),
   as well as Freestyle options in the Python Scripting mode in the View Layers buttons (right).

Freestyle for Blender comes with a number of Python style modules that can serve as a starting
point of your own style module writing. See also the section of the Freestyle Python API in
the Blender Python API reference manual for the full detail of style module constructs.

.. list-table::

   * - .. figure:: /images/render_freestyle_python_scripting-mode-example-1.jpg

          By T.K. using the Python Scripting mode
          (`blend-file <https://archive.blender.org/wiki/2015/index.php/File:Turning_Pages.zip>`__, CC0).

     - .. figure:: /images/render_freestyle_python_scripting-mode-example-2.png

          By T.K. using the Python Scripting mode
          (`blend-file <https://archive.blender.org/wiki/2015/index.php/File:Lily_Broken_Topology.zip>`__, CC0).


Writing Style Modules
=====================

A style module is a piece of code responsible for the stylization of Freestyle line drawing.
The input of a style module is a set of feature edges called view map (ViewMap).
The output is a set of stylized lines also referred to as strokes. A style module is
structured as a pipeline of operations that allow for building strokes from the input edges
within the view map.

There are five kinds of operations (listed with corresponding operator functions):

- Selection ``Operators.select()``
- Chaining ``Operators.chain(), Operators.bidirectional_chain()``
- Splitting ``Operators.sequential_split(), Operators.recursive_split()``
- Sorting ``Operators.sort()``
- Stroke creation ``Operators.create()``

The input view map is populated with a set of ViewEdge objects. The selection operation is
used to pick up ViewEdges of interest to artists based on user-defined selection conditions
(predicates). Chaining operations take the subset of ViewEdges and build Chains by
concatenating ViewEdges according to user-defined predicates and functions.
The Chains can be further refined by splitting them into smaller pieces
(e.g. at points where edges make an acute turn) and selecting a fraction of them
(e.g. to keep only those longer than a length threshold).
The sorting operation is used to arrange the stacking order of chains to draw one line on top of another.
The chains are finally transformed into stylized strokes
by the stroke creation operation applying a series of stroke shaders to individual chains.

ViewEdges, Chains and Strokes are generically referred to as one-dimensional (1D) elements.
A 1D element is a polyline that is a series of connected straight lines.
Vertices of 1D elements are called 0D elements in general.

All the operators act on a set of active 1D elements.
The initial active set is the set of ViewEdges in the input view map.
The active set is updated by the operators.


Selection
---------

The selection operator goes through every element of the active set and keeps only the ones
satisfying a certain predicate. The ``Operators.select()`` method takes as the argument a unary
predicate that works on any ``Interface1D`` that represents a 1D element. For example::

   Operators.select(QuantitativeInvisibilityUP1D(0))

This selection operation uses the ``QuantitativeInvisibilityUP1D`` predicate to select only
the visible ``ViewEdge`` (more precisely, those whose quantitative invisibility is equal to 0).
The selection operator is intended to selectively apply the style to a fraction of the active 1D elements.

It is noted that ``QuantitativeInvisibilityUP1D`` is a class implementing the predicate that tests line visibility,
and the ``Operators.select()`` method takes an instance of the predicate class as argument.
The testing of the predicate for a given 1D element is actually done by calling the predicate instance,
that is, by invoking the ``__call__`` method of the predicate class. In other words, the ``Operators.select()``
method takes as argument a functor which in turn takes an ``Interface0D`` object as argument.
The Freestyle Python API employs functors extensively to implement predicates, as well as functions.


Chaining
--------

The chaining operators act on the set of active ``ViewEdge`` objects and determine the topology
of the future strokes.
The idea is to implement an iterator to traverse the ViewMap graph by marching along ViewEdges.
The iterator defines a chaining rule that determines the next ``ViewEdge``
to follow at a given vertex (see ``ViewEdgeIterator``). Several such iterators are provided
as part of the Freestyle Python API (see ``ChainPredicateIterator`` and ``ChainSilhouetteIterator``).
Custom iterators can be defined by inheriting the ``ViewEdgeIterator`` class.
The chaining operator also takes as argument a UnaryPredicate working on ``Interface1D`` as a stopping criteria.
The chaining stops when the iterator has reached a ``ViewEdge`` satisfying this
predicate during the march along the graph.

Chaining can be either unidirectional ``Operators.chain()`` or bidirectional ``Operators.bidirectional_chain()``.
In the latter case, the chaining will propagate in the two directions from the starting edge.

The following is a code example of bidirectional chaining::

   Operators.bidirectional_chain(
           ChainSilhouetteIterator(),
           NotUP1D(QuantitativeInvisibilityUP1D(0)),
           )

The chaining operator uses the ``ChainSilhouetteIterator`` as the chaining rule and stops chaining
as soon as the iterator has come to an invisible ``ViewEdge``.

The chaining operators process the set of active ``ViewEdge`` objects in order.
The active ViewEdges can be previously sorted using the ``Operators.sort()`` method (see below).
It starts a chain with the first ``ViewEdge`` of the active set.
All ViewEdges that have already been involved in the chaining process are marked
(in the case of the example above, the time stamp of each ``ViewEdge`` is modified by default),
in order not to process the same ``ViewEdge`` twice.
Once the chaining reaches a ``ViewEdge`` that satisfies the stopping predicate, the chain is terminated.
Then a new chain is started from the first unmarked ``ViewEdge`` in the active set.
This operation is repeated until the last unmarked ``ViewEdge`` of the active set was processed.
At the end of the chaining operation,
the active set is set to the Chains that have just been constructed.


Splitting
---------

The splitting operation is used to refine the topology of each Chain.
Splitting is performed either sequentially or recursively. Sequential splitting
``Operators.sequentialSplit()`` in its basic form,
parses the Chain at a given arbitrary resolution and evaluates a unary predicate
(working on 0D elements) at each point along the Chain.
Every time the predicate is satisfied, the chain is split into two chains.
At the end of the sequential split operation,
the active set of chains is set to the new chains. ::

   Operators.sequentialSplit(TrueUP0D(), 2)

In this example, the chain is split every 2 units.
A more elaborated version uses two predicates instead of one: One to determine the starting
point of the new chain and the other to determine its ending point. This second version can
lead to a set of Chains that are disjoint or that overlap if the two predicates are different
(see ``Operators.sequentialSplit()`` for more details).

Recursive splitting ``Operators.recursiveSplit()`` evaluates a function
on the 0D elements along the Chain at a given resolution and
find the point that gives the maximum value for the function.
The Chain is then split into two at that point.
This process is recursively repeated on each of the two new Chains,
until the input Chain satisfies a user-specified stopping condition. ::

   func = Curvature2DAngleF0D()
   Operators.recursive_split(func, NotUP1D(HigherLengthUP1D(5)), 5)

In the code example above,
the Chains are recursively split at points of the highest 2D curvature.
The curvature is evaluated at points along the Chain at a resolution of 5 units.
Chains shorter than 5 units will not be split anymore.


Sorting
-------

The sorting operator ``Operators.sort()`` arranges the stacking order of active 1D elements.
It takes as argument a binary predicate used as a "smaller than" operator to order two 1D elements. ::

   Operators.sort(Length2DBP1D())

In this code example, the sorting uses the ``Length2DBP1D`` binary predicate to sort
the ``Interface1D`` objects in the ascending order in terms of 2D length.

The sorting is particularly useful when combined with causal density. Indeed,
the causal density evaluates the density of the resulting image as it is modified. If we wish
to use such a tool to decide to remove strokes whenever the local density is too high,
it is important to control the order in which the strokes are drawn. In this case,
we would use the sorting operator to ensure that the most "important" lines are drawn first.


Stroke Creation
---------------

Finally, the stroke creation operator ``Operators.create()``
takes the active set of Chains as input and build Strokes. The operator takes two arguments.
The first is a unary predicate that works on ``Interface1D`` that is designed to make a last
selection on the set of chains.
A Chain that does not satisfy the condition will not lead to a Stroke.
The second input is a list of shaders that will be responsible for the shading of each built stroke. ::

   shaders_list = [
       SamplingShader(5.0),
       ConstantThicknessShader(2),
       ConstantColorShader(0.2,0.2,0.2,1),
       ]
   Operators.create(DensityUP1D(8,0.1, IntegrationType.MEAN), shaders_list)

In this example,
the ``DensityUP1D`` predicate is used to remove all Chains whose mean density is higher than 0.1.
Each chain is transformed into a stroke by resampling it so as to have a point every 5 units
and assigning to it a constant thickness of 2 units and a dark gray constant color.


User Control on the Pipeline Definition
---------------------------------------

Style module writing offers different types of user control,
even though individual style modules have a fixed pipeline structure.
One is the sequencing of different pipeline control structures, and another is through
the definition of functor objects that are passed as argument all along the pipeline.

Different pipeline control structures can be defined by sequencing the selection,
chaining, splitting, and sorting operations.
The stroke creation is always the last operation that concludes a style module.

Predicates, functions, chaining iterators, and stroke shaders can be defined by inheriting
base classes and overriding appropriate methods. See the reference manual entries of
the following base classes for more information on the user-scriptable constructs.

.. seealso::

   Predicates, functions, chaining iterators, and stroke shaders can be defined by
   inheriting base classes and overriding appropriate methods.
   See :mod:`Freestyle python module <blender_api:freestyle>` for more information
   on the user-scriptable constructs.


## Render


*****************
Render Properties
*****************

.. reference::

   :Panel:     :menuselection:`Properties --> Render --> Freestyle`

Freestyle can be activated with the checkbox in the header of the Freestyle panel in the *Render* tab.

.. figure:: /images/render_freestyle_render_freestyle-panel.png
   :align: center
   :width: 50%

   Freestyle Render Properties.

.. _bpy.types.RenderSettings.line_thickness_mode:

Line Thickness Mode
   There are two different modes for defining the base line thickness:

   :Absolute:
      The line thickness is given by a user-specified number of pixels.
   :Relative:
      The unit line thickness is scaled by the proportion of the present vertical image resolution to 480 pixels.
      For instance, the "unit line thickness" is 1.0 when the image height set to 480px, 1.5 with 720px
      and 2.0 with 960px.

.. _bpy.types.RenderSettings.line_thickness:

Line Thickness
   Line thickness to use for rendering (only for *Absolute* line thickness).


## Freestyle

.. _bpy.types.ViewLayer.use_freestyle:
.. _bpy.types.FreestyleSettings:

*********
Freestyle
*********

.. reference::

   :Panel:     :menuselection:`Properties --> View Layer --> Freestyle`

There is only one view map per view layer. It controls the edge detection parameters.
Freestyle can be enabled/disabled per View Layer by toggling the checkbox in the panel header.

.. figure:: /images/render_freestyle_view-layer_freestyle-panel.png
   :align: center

   View Layer: Freestyle panel.

.. _bpy.types.FreestyleSettings.mode:

Control Mode
   Which detected edges are actually rendered, and how, can be controlled either through:

   :Parameter Editor Mode:
      Lines are rendered via parameters defined in a user-friendly interface
      to define and control Line Sets and line styles.

      A view map (hence a view layer) can have multiple Line Sets,
      and each line set is linked to one line style.
   :Python Scripting Mode:
      Lines are rendered via :doc:`Python scripting </render/freestyle/python>`, powerful but complex.

.. _bpy.types.FreestyleSettings.use_view_map_cache:

View Map Cache
   An option to reuse a previously computed view map for subsequent rendering.
   The cache is automatically updated when the mesh geometry of the input 3D scene has been changed.

   This functionality offers a major performance boost for Freestyle animation rendering
   when the camera-space mesh geometry is static, as well as for repeated still renders
   with updates of line stylization options.

   Although the *View Map Cache* checkbox is a view layer option,
   the cache memory is shared by all view layers and scenes.
   This means that if Freestyle is used for two or more view layers
   (possibly in different scenes through the Compositor),
   then the cached view map for one view layer is replaced by a new view map
   for another view layer and hence no performance gain is expected.

.. _bpy.types.FreestyleSettings.as_render_pass:

As Render Pass
   Freestyle lines will not immediately be visible on top of the render image.
   Instead, Freestyle lines are rendered as a :doc:`Render Pass </render/layers/passes>`
   which can be composited with the rendered image with an Alpha Over node.


Edge Detection
==============

.. _bpy.types.FreestyleSettings.crease_angle:

Crease Angle
   If two adjacent faces form an angle less than the defined *Crease Angle*,
   the edge between them will be rendered when using *Crease* edge type selection in a line set.
   The value also affects *Silhouette* edge type selection.

.. _bpy.types.FreestyleSettings.use_culling:

Culling
   Ignore the edges that are out of view.
   (Saves some processing time and memory, but may reduce the quality of the result in some cases.)

.. _bpy.types.FreestyleSettings.use_smoothness:

Face Smoothness
   Takes *Smooth Shading* into account for edges calculation.

.. _bpy.types.FreestyleSettings.sphere_radius:

Sphere Radius
   Affects the calculation of curvatures for *Ridge*, *Valley*
   and *Suggestive Contour* edge type selection in a line set.
   The curvature at each vertex is computed by averaging the shape
   of the surface within the specified radius.
   Increasing the value reduces noise and detail.

.. _bpy.types.FreestyleSettings.kr_derivative_epsilon:

Kr Derivative Epsilon
   Controls the threshold on the minimum rate of change of curvature used to filter the output
   of the *Suggestive Contour* edge type selection. Increasing the value reduces the amount of
   rendered lines, starting from smoother areas of the object (further information in
   `this pdf <https://archive.blender.org/wiki/2015/index.php/File:Manual-2.6-Render-Freestyle-
   PrincetownLinestyle.pdf>`__).


## Index


#########################
  View Layer Properties
#########################

.. toctree::
   :maxdepth: 2

   freestyle.rst
   line_set.rst
   line_style/index.rst


## Line Set

.. _bpy.types.Linesets:
.. _bpy.types.FreestyleLineSet:

********
Line Set
********

.. reference::

   :Panel:     :menuselection:`Properties --> View Layer --> Freestyle Line Set`

A line set selects, among the lines (edges) detected by Freestyle,
which ones will be rendered using its attached
:doc:`line style </render/freestyle/view_layer/line_style/introduction>`, through various methods.

.. figure:: /images/render_freestyle_parameter-editor_line-set_panel.png

   Freestyle Line Set panel.

.. _bpy.types.FreestyleLineSet.select_by_image_border:

Select By
   Image Border
      Causes Freestyle to only take geometry within the image border into consideration for line calculation.
      This reduces render times but increases continuity problems when geometry is moved out of and
      into camera view.


Visibility
==========

.. _bpy.types.FreestyleLineSet.visibility:

Type
   Determine how to use visibility for feature edge selection.

   :Visible:
      Only lines occluded by no surfaces are rendered.
   :Hidden:
      Lines occluded by at least one surface are rendered.

      .. figure:: /images/render_freestyle_parameter-editor_line-set_visibility-hidden-edges.png
         :align: center
         :width: 60%

         Proof of concept of visible and hidden edges by LightBWK
         (`blend-file <https://archive.blender.org/wiki/2015/index.php/File:HiddenCreaseEdgeMark.zip>`__).

   :Quantitative Invisibility:
      Lines occluded by a number of surfaces in the given range are rendered.

      .. _bpy.types.FreestyleLineSet.qi_start:
      .. _bpy.types.FreestyleLineSet.qi_end:

      Start, End
         Min/max number of occluding surfaces for a line to be rendered.

      .. figure:: /images/render_freestyle_parameter-editor_line-set_visibility-qi-range.png
         :align: center
         :width: 60%

         QI Range proof of concept demo, Start: 3, End: 7, by LightBWK
         (`blend-file <https://archive.blender.org/wiki/2015/index.php/File:QI-Range.zip>`__).


.. _bpy.types.FreestyleLineSet.exclude:
.. _bpy.types.FreestyleLineSet.select_by_edge_types:

Edge Types
==========

Edge types are basic algorithms for the selection of lines from geometry.
When using the parameter editor you have to choose at least one edge type in order to get a render output,
but several edge types can be combined in one line set.
Edge types can also be excluded from calculation by pressing the *X* next to them.

.. figure:: /images/render_freestyle_parameter-editor_line-set_edge-types-basic.png
   :align: center
   :width: 60%

   Examples of some basic edge types:
   Silhouette (green), Crease (black), Border (blue) and Edge Marks (red)
   (`blend-file <https://archive.blender.org/wiki/2015/index.php/File:EdgeType.zip>`__ by LightBWK).

.. _bpy.types.FreestyleLineSet.select_silhouette:

Type
   Silhouette
      Draws silhouettes around your closed objects by rendering lines where the surface normal transitions between
      pointing toward and away from the camera. It is often good for organic objects (like Suzanne & Sphere),
      and bad for sharp edges, like a box. It cannot render open mesh objects like open cylinders and flat planes.

   .. _bpy.types.FreestyleLineSet.select_crease:

   Crease
      Shows only edges whose adjacent faces form an angle sharper than the defined view map's *Crease Angle*.

      .. figure:: /images/render_freestyle_parameter-editor_line-set_edge-types-crease.png
         :align: center
         :width: 60%

         Crease Angle proof of concept for 121° by LightBWK
         (`blend-file <https://archive.blender.org/wiki/2015/index.php/File:CreaseAngle.zip>`__).

   .. _bpy.types.FreestyleLineSet.select_border:

   Border
      Border shows open mesh edges, i.e. edges that belong to only one face. An open cylinder has open edges at
      the top and bottom, and a plane is open all around. Suzanne's eye socket is an open edge.

   .. _bpy.types.FreestyleLineSet.select_edge_mark:

   Edge Mark
      Renders marked edges. See `Edge Marks`_ for details.

   .. _bpy.types.FreestyleLineSet.select_contour:

   Contour
      Draws lines around each object, separating it from other objects behind it, or the scene background.

   .. _bpy.types.FreestyleLineSet.select_external_contour:

   External Contour
      Draws lines around all objects, separating them from the scene background, but not each other.

   .. figure:: /images/render_freestyle_parameter-editor_line-set_edge-types-contour.png
      :align: center
      :width: 60%

      Left pair: Contour; Right pair: External Contour.

   .. _bpy.types.FreestyleLineSet.select_material_boundary:

   Material Boundary
      Draws lines where two materials meet on the same object.

   .. _bpy.types.FreestyleLineSet.select_suggestive_contour:

   Suggestive Contour
      Draws some lines which would form the *Silhouette* of the mesh if the view point was shifted.
      Depends on your view map settings for *Kr Derivative Epsilon* and *Sphere Radius*
      (further information: `File:Manual-2.6-Render-Freestyle-PrincetownLinestyle.pdf
      <https://archive.blender.org/wiki/2015/index.php/File:Manual-2.6-Render-Freestyle-PrincetownLinestyle.pdf>`__).

   .. _bpy.types.FreestyleLineSet.select_ridge_valley:

   Ridge & Valley
      Draws lines marking crests of ridges and valleys, i.e. places where the surface curvature is at
      its minimum or maximum. Depends on your *Sphere Radius* view map settings.


Edge Marks
----------

In Edit Mode you can mark "Freestyle Edges" in the same manner
you can mark "Seams" for UV unwrapping or "Sharp" for edge split.
These marked edges are available to render when you select *Edge Mark*.

This is done as follows:

#. Select the mesh object and enter *Edit Mode*.
#. Select the edges you want to be marked.
#. Press :kbd:`Ctrl-E` and select *Mark Freestyle Edge*.

Edge marks are useful when you want to draw lines along particular mesh edges.
The examples below explain the use of edge marks.

.. figure:: /images/render_freestyle_parameter-editor_line-set_edge-marks-mark-freestyle-edge.png
   :align: center
   :width: 60%

   Marking Freestyle Edges in Edit Mode; the edge marks are highlighted in green.

With *Edge Marks* enabled, the previously-marked lines are always rendered.
You can see the black contour lines and the blue lines that are made with edge marks.

.. list-table::

   * - .. figure:: /images/render_freestyle_parameter-editor_line-set_edge-marks-example-1.png

          Render without Edge Marks.

     - .. figure:: /images/render_freestyle_parameter-editor_line-set_edge-marks-example-2.png

          Render with Edge Marks enabled.

What are edge marks good for?

- When you need to render marks on an almost-flat plane, when other edge types cannot detect any line.
- When you want full control of edge rendering. Often used for edges of squarish shapes.
- Mark the whole base mesh to be rendered for base mesh preview.

What are edge marks not good for?

- Round outer edges (use instead *Contour/External Contour/Silhouette*).


.. _bpy.types.FreestyleLineSet.select_by_face_marks:

Face Marks
==========

Face marks are useful for removing lines from certain areas of a mesh.

To set a face mark:

#. Select a mesh object and enter *Edit Mode*.
#. Select the faces you want to be marked.
#. Press :kbd:`Ctrl-F` and select :menuselection:`Face Data --> Mark Freestyle Face`.

In this example, two faces of the default cube are marked like the image on the left.
On the right is a render without face marks activated.

.. list-table::

   * - .. figure:: /images/render_freestyle_parameter-editor_line-set_face-marks-example-1.png

          Marked faces (Edit Mode).

     - .. figure:: /images/render_freestyle_parameter-editor_line-set_face-marks-example-2.png

          Render output.

The line selection can be controlled via inclusion and faces options:

.. _bpy.types.FreestyleLineSet.face_mark_negation:

Negation
   Whether to include or exclude edges matching defined face mark conditions from the line set.

.. _bpy.types.FreestyleLineSet.face_mark_condition:

Condition
   :One Face:
      (De)select all edges which have one or both neighbor faces marked.
   :Both Faces:
      (De)select all edges which have both of their neighbor faces marked.

.. list-table::

   * - .. figure:: /images/render_freestyle_parameter-editor_line-set_face-marks-example-3.png

          Inclusive, One Face.

     - .. figure:: /images/render_freestyle_parameter-editor_line-set_face-marks-example-4.png

          Inclusive, Both Faces.

   * - .. figure:: /images/render_freestyle_parameter-editor_line-set_face-marks-example-5.png

          Exclusive, One Face.

     - .. figure:: /images/render_freestyle_parameter-editor_line-set_face-marks-example-6.png

          Exclusive, Both Faces.


.. _bpy.types.FreestyleLineSet.select_by_collection:

Collection
==========

Include or exclude objects for line calculation,
based on their belonging to a :doc:`Collection </scene_layout/collections/index>`.

.. _bpy.types.FreestyleLineSet.collection:

Line Set Collection
   The name of the object collection to use.

.. _bpy.types.FreestyleLineSet.collection_negation:

Negation
   Whether to include or exclude lines from those objects in this line set.


## Alpha


*****
Alpha
*****

In this tab you control the alpha (transparency) of your strokes.

.. figure:: /images/render_freestyle_parameter-editor_line-style_alpha_tab.png
   :align: center
   :width: 50%

   Line Style: Alpha.

.. _bpy.types.FreestyleLineStyle.alpha:

Base Transparency
   The base alpha for this line style.


.. _bpy.ops.scene.freestyle_alpha_modifier_add:

Modifiers
=========

Common Options
--------------

Mix
   The modifier output can be mixed with the base property using the usual methods
   (see for example the :doc:`Mix compositing node </compositing/types/color/mix/mix_color>`).
Influence
   How much the result of this modifier affects the current property.
Mapping
   Either a linear progression (from 0.0 to 1.0),
   or a custom mapping :ref:`curve <ui-curve-widget>`.

   .. note::

      Note the linear non-inverted option is equivalent to "do nothing",
      as original values from materials are already in the (0.0 to 1.0) range.
      That is the case for: Crease Angle, Curvature 3D, Material, Noise, Tangent.

Invert
   Inverts the *Mapping*.


Types
-----

- :doc:`/render/freestyle/view_layer/line_style/modifiers/alpha/along_stroke`
- :doc:`/render/freestyle/view_layer/line_style/modifiers/alpha/crease_angle`
- :doc:`/render/freestyle/view_layer/line_style/modifiers/alpha/curvature_3d`
- :doc:`/render/freestyle/view_layer/line_style/modifiers/alpha/distance_from_camera`
- :doc:`/render/freestyle/view_layer/line_style/modifiers/alpha/distance_from_object`
- :doc:`/render/freestyle/view_layer/line_style/modifiers/alpha/material`
- :doc:`/render/freestyle/view_layer/line_style/modifiers/alpha/noise`
- :doc:`/render/freestyle/view_layer/line_style/modifiers/alpha/tangent`


## Color


*****
Color
*****

In this tab you control the color of your strokes.

.. figure:: /images/render_freestyle_parameter-editor_line-style_color_tab.png
   :align: center
   :width: 50%

   Line Style: Color.

.. _bpy.types.FreestyleLineStyle.color:

Base Color
   The base color for this line style.


.. _bpy.ops.scene.freestyle_color_modifier_add:

Modifiers
=========

Common Options
--------------

Mix
   The modifier output can be mixed with the base property using the usual methods
   (see for example the :doc:`Mix compositing node </compositing/types/color/mix/mix_color>`).
Influence
   How much the result of this modifier affects the current property.
Color Ramp
   Each modifier has :ref:`color ramp <ui-color-ramp-widget>` that maps the property to a stroke color.


Types
-----

- :doc:`/render/freestyle/view_layer/line_style/modifiers/color/along_stroke`
- :doc:`/render/freestyle/view_layer/line_style/modifiers/color/crease_angle`
- :doc:`/render/freestyle/view_layer/line_style/modifiers/color/curvature_3d`
- :doc:`/render/freestyle/view_layer/line_style/modifiers/color/distance_from_camera`
- :doc:`/render/freestyle/view_layer/line_style/modifiers/color/distance_from_object`
- :doc:`/render/freestyle/view_layer/line_style/modifiers/color/material`
- :doc:`/render/freestyle/view_layer/line_style/modifiers/color/noise`
- :doc:`/render/freestyle/view_layer/line_style/modifiers/color/tangent`


## Geometry


********
Geometry
********

In this tab you control the geometry of your strokes.
It contains only the option to add modifiers.

As they always completely apply to the strokes' geometry (like object modifiers do).
They take the resulting two-dimensional strokes from the Freestyle line set and
displace or deform them in various ways.

As with other modifier stacks in Blender, they are applied from top to bottom.

.. figure:: /images/render_freestyle_parameter-editor_line-style_geometry_tab.png
   :align: center
   :width: 50%

   Line Style: Geometry.


.. _bpy.ops.scene.freestyle_geometry_modifier_add:

Modifiers
=========

Types
-----

- :doc:`/render/freestyle/view_layer/line_style/modifiers/geometry/2d_offset`
- :doc:`/render/freestyle/view_layer/line_style/modifiers/geometry/2d_transform`
- :doc:`/render/freestyle/view_layer/line_style/modifiers/geometry/backbone_stretcher`
- :doc:`/render/freestyle/view_layer/line_style/modifiers/geometry/bezier_curve`
- :doc:`/render/freestyle/view_layer/line_style/modifiers/geometry/blueprint`
- :doc:`/render/freestyle/view_layer/line_style/modifiers/geometry/guiding_lines`
- :doc:`/render/freestyle/view_layer/line_style/modifiers/geometry/perlin_noise_1d`
- :doc:`/render/freestyle/view_layer/line_style/modifiers/geometry/perlin_noise_2d`
- :doc:`/render/freestyle/view_layer/line_style/modifiers/geometry/polygonization`
- :doc:`/render/freestyle/view_layer/line_style/modifiers/geometry/sampling`
- :doc:`/render/freestyle/view_layer/line_style/modifiers/geometry/simplification`
- :doc:`/render/freestyle/view_layer/line_style/modifiers/geometry/sinus_displacement`
- :doc:`/render/freestyle/view_layer/line_style/modifiers/geometry/spatial_noise`
- :doc:`/render/freestyle/view_layer/line_style/modifiers/geometry/tip_remover`


## Index

.. _bpy.types.FreestyleLineStyle:

##############
  Line Style
##############

.. toctree::
   :maxdepth: 2

   introduction.rst


Properties
==========

.. toctree::
   :maxdepth: 1

   strokes.rst
   color.rst
   alpha.rst
   thickness.rst
   geometry.rst
   texture.rst


Modifiers
=========

.. toctree::
   :maxdepth: 1

   Color <modifiers/color/index.rst>
   Alpha <modifiers/alpha/index.rst>
   Thickness <modifiers/thickness/index.rst>
   Geometry <modifiers/geometry/index.rst>


## Introduction


************
Introduction
************

In Freestyle, the line style settings define the appearance of a line set using five main aspects:
Stroke, Color, Alpha, Thickness, Geometry, and Texture with each on a separate tab.
These allow you to get many different styles of renders
(technical draw, rough sketch, cartoon, calligraphy, etc.).

You can create as many line styles as you wish, and reuse a given line style for several line
sets by selecting it from the select menu next to its name.

.. note::

   Unless otherwise specified, all lengths in line style settings are in pixels
   (either relative or absolute, as specified in the :doc:`core options </render/freestyle/render>`).

.. figure:: /images/render_freestyle_parameter-editor_line-style_introduction_line-style-example.png

   Line Style Example (`blend-file <https://archive.blender.org/wiki/2015/index.php/File:LineStyles.zip>`__).


## Strokes


*******
Strokes
*******

Strokes are the final rendered lines. Yet you can tweak them, for example,
by removing the ones longer/shorter than some threshold,
chaining lines into a single stroke or breaking a stroke into several ones based on angles,
dashed pattern, etc.

.. figure:: /images/render_freestyle_parameter-editor_line-style_strokes_tab.png
   :align: center
   :width: 50%

   Line Style: Strokes.

.. _bpy.types.FreestyleLineStyle.caps:

Caps
   You can choose between three types of line caps:

   :Butt:
      Flat cap, exactly at the point the line ends.
   :Round:
      A half circle centered on the end point of the line.
   :Square:
      A square centered on the end point of the line
      (hence, like the circle, the drawn end of the line is slightly extended compared to its computed value).

   .. figure:: /images/render_freestyle_parameter-editor_line-style_strokes_caps-example.png
      :align: center

      Line caps example.


.. _bpy.types.FreestyleLineStyle.use_chaining:

Chaining
========

By default all retrieved lines from the line set are chained together.
There are two basic chaining methods:

.. _bpy.types.FreestyleLineStyle.chaining:

Method
   :Plain:
      The default chaining method; it creates simple chains.
   :Sketchy:
      This chaining option allows for generating chains of feature edges with sketchy multiple strokes.
      Basically, it generates *Round* strokes instead of a single one.
      It is only really useful if you use some random-driven modifiers in the line style!

      .. _bpy.types.FreestyleLineStyle.rounds:

      Rounds
         It specifies the number of rounds in sketchy strokes.

.. _bpy.types.FreestyleLineStyle.use_same_object:

Same Object
   If true, only feature edges of the same object are joined.

Chaining can also be turned off to render each line separately,
which can be useful for line styles which depend on accurate representation of the line set.


Splitting
=========

You can split up chains of Freestyle lines by enabling one of the following:

.. _bpy.types.FreestyleLineStyle.use_angle_min:
.. _bpy.types.FreestyleLineStyle.use_angle_max:
.. _bpy.types.FreestyleLineStyle.angle_min:
.. _bpy.types.FreestyleLineStyle.angle_max:

Min/Max 2D Angle
   Splits chains of feature edges when they make a 2D angle above (or below) a minimum (or maximum) threshold.

.. _bpy.types.FreestyleLineStyle.use_split_length:
.. _bpy.types.FreestyleLineStyle.split_length:

2D Length
   Splits chains when they are longer than the given value.

.. _bpy.types.FreestyleLineStyle.material_boundary:

Material Boundary
   Splits chains of feature edges if they cross from one material to another.


.. _bpy.types.FreestyleLineStyle.use_split_pattern:

Split Pattern
-------------

Splits the chains using the given dashed pattern (see also `Dashed Line`_).

.. _bpy.types.FreestyleLineStyle.split_dash:

Dash 1, 2, 3
   Length of the specified dash for splitting.

.. _bpy.types.FreestyleLineStyle.split_gap:

Gap 1, 2, 3
   Length of the specified gap for splitting.


.. _bpy.types.FreestyleLineStyle.use_sorting:

Sorting
=======

You can sort the order of your strokes, allowing the lines to stack in the order given.

.. _bpy.types.FreestyleLineStyle.sort_key:

Sort Key
   A sort key is used to determine the stacking order of lines.

   :Distance from Camera:
      Lines closer to the camera lie on top of further lines.
   :2D Length:
      Longer lines lie on top of shorter lines.
   :Projected X/Y:
      Sort by the projected X or Y value in the image coordinate system.

.. _bpy.types.FreestyleLineStyle.integration_type:

Integration Type
   Use in tandem with the Sort Key to determine the range for sorting.
   Since the distance of a line from the camera may vary over vertices,
   this option computes the sort key for a line from the values computed at
   individual vertices. The value computed for the line is:

   :Mean: The mean of the values obtained for the vertices.
   :Min: The minimum of the values obtained for the vertices.
   :Max: The maximum of the values obtained for the vertices.
   :First: The value obtained for the first vertex.
   :Last: The value obtained for the last vertex.

.. _bpy.types.FreestyleLineStyle.sort_order:

Sort Order
   With the given result you can choose to "Reverse" the sort order.


Selection
=========

You can also choose to only render selected chains.

.. _bpy.types.FreestyleLineStyle.use_length_min:
.. _bpy.types.FreestyleLineStyle.use_length_max:
.. _bpy.types.FreestyleLineStyle.length_min:
.. _bpy.types.FreestyleLineStyle.length_max:

Min/Max 2D Length
   Chains longer and/or shorter than *2D Length*.

.. _bpy.types.FreestyleLineStyle.use_chain_count:
.. _bpy.types.FreestyleLineStyle.chain_count:

Chain Count
   Allows the selection of first N chains.


.. _bpy.types.FreestyleLineStyle.use_dashed_line:

Dashed Line
===========

By enabling the *Dashed Line* checkbox,
you can specify three pairs of dash and gap lengths.
Dash values define the lengths of dash strokes,
while gap values specify intervals between two dashes.

If a zero gap is specified,
then the corresponding dash is ignored even if it has a nonzero value.

Dashes are treated as separate strokes, meaning that you can apply line caps,
as well as color, alpha and thickness modifiers.

.. _bpy.types.FreestyleLineStyle.dash:

Dash 1, 2, 3
   Length of the specified dash for dashed lines.

.. _bpy.types.FreestyleLineStyle.gap:

Gap 1, 2, 3
   Length of the specified gap for dashed lines.


## Texture


*******
Texture
*******

Assigns a texture to the Freestyle stroke.

.. figure:: /images/render_freestyle_parameter-editor_line-style_texture_tab.png
   :align: center
   :width: 50%

   Line Style: Texture.

.. _bpy.types.FreestyleLineStyle.use_nodes:

Use Nodes
   In Cycles textures are defined by means of shader `Nodes`_.

.. _bpy.types.FreestyleLineStyle.texture_spacing:

Spacing Along Stroke
   Allows to set the "pace" of textures mapped along the length of strokes.

Go to Linestyle Textures
   A shortcut to the line style texture properties in the other *Textures* tab of the Properties.
   Make sure to first enable *Use Nodes*.


Nodes
=====

UV Along Stroke Node
--------------------

.. figure:: /images/node-types_ShaderNodeUVAlongStroke.webp
   :align: right

   UV Along Stroke Node.

The *UV Along Stroke* input node is maps textures along the stroke length,
making it possible to mimic pencil, paintbrush, and other art medium marks.

.. note::

   These UV maps become available only during the Freestyle rendering process.
   Hence, the UV Along Stroke node cannot be replaced by the conventional UV Map input node
   which takes an existing UV map already defined as part of mesh data.


Inputs
^^^^^^

This node has no inputs.


Properties
^^^^^^^^^^

Use Tips
   Allows to use lower quarters of a texture image for the head and tail tips of a stroke,
   while the upper half for the stroke body.


Outputs
^^^^^^^

UV
   UV maps defined along strokes.


Line Style Output Node
----------------------

.. figure:: /images/node-types_ShaderNodeOutputLineStyle.webp
   :align: right

   Line Style Output Node.

The *Line Style Output* node specifies how to mix the texture information
into the base color of line styles.


Inputs
^^^^^^

Color
   Color input for the texture.
Color Factor
   Standard mix factor of the *Color* value.
Alpha
   Alpha input for the texture.
Alpha Factor
   Standard mix factor of the *Alpha* value.


Properties
^^^^^^^^^^

Mix
   The Blend mode can be selected in the select menu.
   See :term:`Color Blend Modes` for details on each blending mode.
Clamp
   Limit the highest color value to not exceed 1.0.


Outputs
^^^^^^^

This node has no outputs.


Example
=======

The image below shows a typical shader node tree that maps a floral texture image along strokes.
The UV Along Stroke input node retrieves UV maps defined by Freestyle along generated strokes, and
passes them to the Vector input channel of the Image Texture node.
A texture image is selected in the Image Texture node,
and its color is inputted to the Alpha channel of the Line Style Output node.
Since the Alpha Factor is set to one, the texture image replaces the base alpha transparency of the active line style
(shown in the Freestyle Line Style panel).
On the other hand, the Mix blend mode is selected in the Line Style Output node with the Color Factor set to zero,
so that the gradient line color specified in the active line style is applied along strokes.

.. figure:: /images/render_freestyle_parameter-editor_line-style_texture_uv-along-stroke-example.png

   Example of Line Style Nodes
   (`blend-file
   <https://archive.blender.org/wiki/2015/index.php/File:Blender_272_textured_strokes_in_cycles.blend>`__).

It is noted that the texture image ``FS_floral_brush.png``
shown in the screen capture is an example of Freestyle brush images with tips.
Specifically, the upper half of the image is used as a seamless horizontal tile of the stroke body.
Whereas the parts in the lower half are tips (stroke caps) at both ends of the stroke.


## Thickness


*********
Thickness
*********

Controls the thickness of the Freestyle strokes.

.. figure:: /images/render_freestyle_parameter-editor_line-style_thickness_tab.png
   :align: center
   :width: 50%

   Line Style: Thickness.

Base Thickness
   The base thickness for this line style.

Thickness Position
   Control the position of stroke thickness from the original (backbone) stroke geometry. There are four choices:

   :Center:
      The thickness is evenly split to the left and right side of the stroke geometry.
   :Inside:
      The strokes are drawn within object boundary.
   :Outside:
      The strokes are drawn outside the object boundary.
   :Relative:
      Specifies the relative position by a number between 0.0 (inside) and 1.0 (outside),
      in the *Thickness Ratio* number field just below.

   .. note::

      The thickness position options are applied only to strokes of edge types
      *Silhouette* and *Border*,
      since these are the only edge types defined in terms of the object boundary.
      Strokes of other edge types are always drawn using the *Center* option.


Modifiers
=========

Common Options
--------------

Mix
   The modifier output can be mixed with the base property using the usual methods
   (see for example the :doc:`Mix compositing node </compositing/types/color/mix/mix_color>`).
Influence
   How much the result of this modifier affects the current property.


Types
-----

- :doc:`/render/freestyle/view_layer/line_style/modifiers/thickness/along_stroke`
- :doc:`/render/freestyle/view_layer/line_style/modifiers/thickness/calligraphy`
- :doc:`/render/freestyle/view_layer/line_style/modifiers/thickness/crease_angle`
- :doc:`/render/freestyle/view_layer/line_style/modifiers/thickness/curvature_3d`
- :doc:`/render/freestyle/view_layer/line_style/modifiers/thickness/distance_from_camera`
- :doc:`/render/freestyle/view_layer/line_style/modifiers/thickness/distance_from_object`
- :doc:`/render/freestyle/view_layer/line_style/modifiers/thickness/material`
- :doc:`/render/freestyle/view_layer/line_style/modifiers/thickness/noise`
- :doc:`/render/freestyle/view_layer/line_style/modifiers/thickness/tangent`


## Index

:orphan:

########################
  Line Style Modifiers
########################

Color
=====

.. toctree::
   :glob:

   color/*


Alpha
=====

.. toctree::
   :glob:

   alpha/*


Thickness
=========

.. toctree::
   :glob:

   thickness/*


Geometry
========

.. toctree::
   :glob:

   geometry/*


## Along Stroke

.. DO NOT EDIT FILE. This is simply a stub which copies everything from the link below.
.. include:: /render/freestyle/view_layer/line_style/modifiers/color/along_stroke.rst
   :start-after: .. --- copy below this line ---


## Crease Angle

.. DO NOT EDIT FILE. This is simply a stub which copies everything from the link below.
.. include:: /render/freestyle/view_layer/line_style/modifiers/color/crease_angle.rst
   :start-after: .. --- copy below this line ---


## Curvature 3D

.. DO NOT EDIT FILE. This is simply a stub which copies everything from the link below.
.. include:: /render/freestyle/view_layer/line_style/modifiers/color/curvature_3d.rst
   :start-after: .. --- copy below this line ---


## Distance From Camera

.. DO NOT EDIT FILE. This is simply a stub which copies everything from the link below.
.. include:: /render/freestyle/view_layer/line_style/modifiers/color/distance_from_camera.rst
   :start-after: .. --- copy below this line ---


## Distance From Object

.. DO NOT EDIT FILE. This is simply a stub which copies everything from the link below.
.. include:: /render/freestyle/view_layer/line_style/modifiers/color/distance_from_object.rst
   :start-after: .. --- copy below this line ---


## Index


###################
  Alpha Modifiers
###################

.. toctree::
   :glob:

   *


## Material

.. DO NOT EDIT FILE. This is simply a stub which copies everything from the link below.
.. include:: /render/freestyle/view_layer/line_style/modifiers/color/material.rst
   :start-after: .. --- copy below this line ---


## Noise

.. DO NOT EDIT FILE. This is simply a stub which copies everything from the link below.
.. include:: /render/freestyle/view_layer/line_style/modifiers/color/noise.rst
   :start-after: .. --- copy below this line ---


## Tangent

.. DO NOT EDIT FILE. This is simply a stub which copies everything from the link below.
.. include:: /render/freestyle/view_layer/line_style/modifiers/color/tangent.rst
   :start-after: .. --- copy below this line ---


## Along Stroke

.. _bpy.types.LineStyle*Modifier_AlongStroke:
.. Editor's Note: This page gets copied into:
   :doc:`</render/freestyle/view_layer/line_style/modifiers/alpha/along_stroke>`
   :doc:`</render/freestyle/view_layer/line_style/modifiers/thickness/along_stroke>`
.. --- copy below this line ---

************
Along Stroke
************

The *Along Stroke* modifier alters the base property with a new one from
a given range mapped along each stroke's length. In other words,
it applies a gradient along each stroke.


## Crease Angle

.. _bpy.types.LineStyle*Modifier_CreaseAngle:
.. Editor's Note: This page gets copied into:
   :doc:`</render/freestyle/view_layer/line_style/modifiers/alpha/crease_angle>`
   :doc:`</render/freestyle/view_layer/line_style/modifiers/thickness/crease_angle>`
.. --- copy below this line ---

************
Crease Angle
************

A modifier based on the Crease Angle (angle between two adjacent faces).
If a stroke segment does not lie on a crease (i.e. the edge does not have the *Crease Angle nature*),
its properties are not touched by the modifier.

Angle Min, Max
   The range of input values to the mapping.
   Out-of-range crease angle values will be clamped by
   the Min and Max angles and their corresponding property values.

.. figure:: /images/render_freestyle_parameter-editor_line-style_modifiers_color_crease-angle_example.png
   :align: center
   :width: 50%

   Crease Angle modifier example by T.K.
   (`blend-file
   <https://archive.blender.org/wiki/2015/index.php/File:Render_freestyle_modifier_crease_angle.blend>`__).


## Curvature 3D

.. _bpy.types.LineStyle*Modifier_Curvature_3D:
.. Editor's Note: This page gets copied into:
   :doc:`</render/freestyle/view_layer/line_style/modifiers/alpha/curvature_3d>`
   :doc:`</render/freestyle/view_layer/line_style/modifiers/thickness/curvature_3d>`
.. --- copy below this line ---

************
Curvature 3D
************

A modifier based on radial curvatures of the underlying 3D surface.
The `curvature <https://en.wikipedia.org/wiki/Curvature>`__ of a 2D curve
at a point is a measure of how quickly the curve turns at the point.
The quicker the turn is, the larger the curvature is at the point.
The curvature is zero if the curve is a straight line.
Radial curvatures are those computed for a 2D curve that appears at the cross section
between the 3D surface and a plane defined by the view point (camera location)
and the normal direction of the surface at the point.

For radial curvatures to be calculated (and therefore for this modifier to have any effect),
the *Face Smoothness* option has to be turned on and the object needs to have *Smooth Shading*.

Curvature Min, Max
   The limits of the mapping.
   If the current point of the stroke is at *Min Curvature* or less from the target,
   it will take the start point of the mapping. And conversely,
   if it is at *Max Curvature* or more from the target, it will take the end-point value of the mapping.

.. figure:: /images/render_freestyle_parameter-editor_line-style_modifiers_color_curvature-3d_example.png
   :align: center
   :width: 50%

   Curvature 3D modifier demo by T.K.
   (`blend-file
   <https://archive.blender.org/wiki/2015/index.php/File:Render_freestyle_modifier_curvature_3d.blend>`__).


## Distance From Camera

.. _bpy.types.LineStyle*Modifier_DistanceFromCamera:
.. Editor's Note: This page gets copied into:
   :doc:`</render/freestyle/view_layer/line_style/modifiers/alpha/distance_from_camera>`
   :doc:`</render/freestyle/view_layer/line_style/modifiers/thickness/distance_from_camera>`
.. --- copy below this line ---

********************
Distance from Camera
********************

The *Distance from Camera* modifier alters the base property with a new one
from a given range using the distance to the active *camera*.

Range Min, Max
   The limits of the mapping from "distance to camera" to "property in mapping".
   If the current point of the stroke is at *Range Min* or less from the active camera or the object,
   it will take the start value. And conversely,
   if it is at *Range Max* or more from the camera/object, it will take the end value.
   These values are in the current scene's units, not in pixels!

Fill Range by Selection
   Set the min/max range values from the distances between the current selected mesh vertices and
   the camera or the target.


## Distance From Object

.. _bpy.types.LineStyle*Modifier_DistanceFromObject:
.. Editor's Note: This page gets copied into:
   :doc:`</render/freestyle/view_layer/line_style/modifiers/alpha/distance_from_object>`
   :doc:`</render/freestyle/view_layer/line_style/modifiers/thickness/distance_from_object>`
.. --- copy below this line ---

********************
Distance from Object
********************

The *Distance from Object* modifier alters the base property with a new one
from a given range using the distance to the active *camera* or to a given *object* as the parameter.

Target
   The object to measure distance from.

Range Min, Max
   The limits of the mapping from "distance to camera" to "property in mapping".
   If the current point of the stroke is at *Range Min* or less from the active camera or the object,
   it will take the start value. And conversely,
   if it is at *Max* or more from the camera/object, it will take the end value.
   These values are in the current scene's units, not in pixels!

Fill Range by Selection
   Set the min/max range values from the distances between the current selected mesh vertices and
   the camera or the target.


## Index


###################
  Color Modifiers
###################

.. toctree::
   :glob:

   *


## Material

.. _bpy.types.LineStyle*Modifier_Material:
.. Editor's Note: This page gets copied into:
   :doc:`</render/freestyle/view_layer/line_style/modifiers/alpha/material>`
   :doc:`</render/freestyle/view_layer/line_style/modifiers/thickness/material>`
.. --- copy below this line ---

********
Material
********

The *Material* modifier alters the base property with a new one taken from a given range mapped on
the current material under the stroke.

You can use various properties of the materials, among which many are mono-component
(i.e. give black-and-white results). In this case for the color modifier, an optional color ramp can be used to
map these gray-scale values to colored ones.
In the reverse case properties of the materials, which are multi-components
(i.e. give RGB results) the mean value will be used for Alpha and Thickness modifiers.

If used with the *Split by Material* option in the *Stroke* tab,
the result will not be blurred between materials along the strokes.

.. figure:: /images/render_freestyle_parameter-editor_line-style_modifiers_color_material_example.png
   :width: 50%
   :align: center

   Material modifiers demo by T.K.
   (`blend-file <https://archive.blender.org/wiki/2015/index.php/File:Lilies_Color_Material.zip>`__).


## Noise

.. _bpy.types.LineStyle*Modifier_Noise:
.. Editor's Note: This page gets copied into:
   :doc:`</render/freestyle/view_layer/line_style/modifiers/alpha/noise>`
   :doc:`</render/freestyle/view_layer/line_style/modifiers/thickness/noise>`
.. --- copy below this line ---

*****
Noise
*****

The *Noise* modifier uses a pseudo-random number generator to variably distribute the property along the stroke.

Amplitude
   The maximum value of the noise. A higher amplitude means a less transparent (more solid) stroke.
Period
   The period of the noise. This means how quickly the property value can change.
   A higher value means a more smoothly changing color along the stroke.
Seed
   Seed used by the pseudo-random number generator.

Asymmetric :guilabel:`Thickness only`
   Allows the thickness to be distributed unevenly at every point.
   Internally, the stroke is represented as a backbone with a thickness to the right and left side.
   All other thickness shaders make sure that the left and right thickness values are equal.
   For the Noise shader however, a meaningful (and good-looking) result
   can be created by assigning different values to either side of the backbone.

.. figure:: /images/render_freestyle_parameter-editor_line-style_modifiers_color_noise_thickness-noise-example.png
   :align: center
   :width: 50%

   Effect generated with a noise thickness modifier using asymmetric thickness.


## Tangent

.. _bpy.types.LineStyleColorModifier_Tangent:
.. Editor's Note: This page gets copied into:
   :doc:`</render/freestyle/view_layer/line_style/modifiers/alpha/tangent>`
   :doc:`</render/freestyle/view_layer/line_style/modifiers/thickness/tangent>`
.. --- copy below this line ---

*******
Tangent
*******

This modifier bases its effect on the traveling direction of the stroke evaluated at the stroke's vertices.


## 2D Offset

.. _bpy.types.LineStyleGeometryModifier_2DOffset:

*********
2D Offset
*********

The *2D Offset* modifier adds some two-dimensional offsets to the stroke backbone geometry.
It has two sets of independent options/effects:

Start, End
   These two options add the given amount of offset to the start (or end) point of the stroke,
   along the (2D) normal at those points. The effect is blended over the whole stroke, if you for example,
   set only *Start* to 50, the start of the stroke is offset 50 pixels along its normal,
   the middle of the stroke, 25 pixels along its own normal, and the end point is not moved.
X, Y
   These two options simply add a constant horizontal and/or vertical offset to the whole stroke.


## 2D Transform

.. _bpy.types.LineStyleGeometryModifier_2DTransform:

************
2D Transform
************

The *2D Transform* modifier applies two-dimensional scaling and/or rotation to
the stroke backbone geometry. Scale is applied before rotation.

Pivot
   The center (pivot point) of these 2D transformations can be:

   :Stroke Center: The median point of the stroke.
   :Stroke Start: The beginning point of the stroke.
   :Stroke End: The end point of the stroke.
   :Stroke Point Parameter:
      The *Stroke Point Parameter* factor controls where along the stroke the pivot point is
      (start point if set to 0.0; end point if set to 1.0).
   :Absolute 2D Point:
      The *Pivot X* and *Y* values define the position of the pivot point in the final render
      (from the bottom left corner).

      .. important::

         Currently, you have to take into account the *real* render size,
         i.e. resolution **and** resolution percentage.

Scale X, Y
   The scaling factors, in their respective axes.

Rotation Angle
   The rotation angle.

.. figure:: /images/render_freestyle_parameter-editor_line-style_modifiers_geometry_2d-transform_example.png
   :width: 50%
   :align: center

   2D Transform modifier
   (`blend-file <https://archive.blender.org/wiki/2015/index.php/File:Toycar_Three_Contours.zip>`__).


## Backbone Stretcher

.. _bpy.types.LineStyleGeometryModifier_BackboneStretcher:

******************
Backbone Stretcher
******************

The *Backbone Stretcher* modifier stretches (adds some length to)
the beginning and end of the stroke.

Backbone Length
   Length to add to the strokes' ends.


## Bezier Curve

.. _bpy.types.LineStyleGeometryModifier_BezierCurve:

************
Bézier Curve
************

The *Bézier Curve* modifier replaces the stroke by a Bézier approximation of it.

Error
   The maximum distance allowed between the new Bézier curve and the original stroke.

.. figure:: /images/render_freestyle_parameter-editor_line-style_modifiers_geometry_bezier-curve_example.png
   :width: 50%
   :align: center

   Bézier Curve modifier demo by T.K.
   (`blend-file <https://archive.blender.org/wiki/2015/index.php/File:Toycar_bezier.zip>`__).


## Blueprint

.. _bpy.types.LineStyleGeometryModifier_Blueprint:

*********
Blueprint
*********

The *Blueprint* modifier produces blueprint-like strokes using either circular,
elliptical, or square contours. A blueprint here refers to those lines drawn at the beginning
of free-hand drawing to capture the silhouette of objects with a simple shape such as circles,
ellipses and squares.

Shape
   Which base shapes to use for this blueprint: *Circles*, *Ellipses* or *Squares*.

Rounds
   How many rounds are generated, as if the pen draws the same stroke several times
   (i.e. how many times the process is repeated).

Random Radius, Center
   For the *Circles* and *Ellipses* shapes.
   Adds some randomness to each round in the relevant aspect.
   Using more than one round with no randomness would be meaningless, as they would draw over each other exactly.

Backbone Length, Backbone
   For the *Squares* shapes.
   The first adds some extra length to each edge of the generated squares (also affected by the second parameter).
   The second adds some randomness to the squares.

Note that the *Min 2D Length* feature from the *Strokes* settings is quite
handy here, to avoid the noise generated by small strokes...


## Guiding Lines

.. _bpy.types.LineStyleGeometryModifier_GuidingLines:

*************
Guiding Lines
*************

The *Guiding Lines* modifier replaces a stroke by a straight line connecting both of its ends.

Offset
   Offset the start and end points along the original stroke, before generating the new straight one.

This modifier will produce reasonable results when strokes are short enough,
because shorter strokes are more likely to be well approximated by straight lines.
Therefore, it is recommended to use this modifier together with one of the splitting options
(by 2D angle or by 2D length) from the *Strokes* panel.

.. figure:: /images/render_freestyle_parameter-editor_line-style_modifiers_geometry_guiding-lines_example.png
   :width: 50%
   :align: center

   Guiding Lines modifier Demo by T.K.
   (`blend-file <https://archive.blender.org/wiki/2015/index.php/File:Toycar_Guiding_Line.zip>`__).


## Index


######################
  Geometry Modifiers
######################

.. toctree::
   :glob:

   *


## Perlin Noise 1D

.. _bpy.types.LineStyleGeometryModifier_PerlinNoise1D:

***************
Perlin Noise 1D
***************

The *Perlin Noise 1D* modifier adds one-dimensional Perlin noise to the stroke.
The curvilinear abscissa (value between 0 and 1 determined by a point's position
relative to the first and last point of a stroke) is used as the input to
the noise function to generate noisy displacements.

This means that this modifier will give an identical result for two strokes
with the same length and sampling interval.

Frequency
   How dense the noise is (kind of a scale factor along the stroke).
Amplitude
   How much the noise distorts the stroke in the *Angle* direction.
Seed
   The seed of the random generator (the same seed over a stroke will always give the same result).
Octaves
   The "level of detail" of the noise.
Angle
   In which direction the noise is applied (0.0 is fully horizontal).


## Perlin Noise 2D

.. _bpy.types.LineStyleGeometryModifier_PerlinNoise2D:

***************
Perlin Noise 2D
***************

The *Perlin Noise 2D* modifier adds one-dimensional Perlin noise to the stroke. The modifier generates
noisy displacements using 2D coordinates of stroke vertices as the input of the noise generator.

Frequency
   How dense the noise is (kind of a scale factor along the stroke).
Amplitude
   How much the noise distorts the stroke in the *Angle* direction.
Seed
   The seed of the random generator (the same seed over a stroke will always give the same result).
Octaves
   The "level of detail" of the noise.
Angle
   In which direction the noise is applied (0.0 is fully horizontal).


## Polygonization

.. _bpy.types.LineStyleGeometryModifier_Polygonalization:

**************
Polygonization
**************

The *Polygonization* modifier simplifies strokes as much as possible
(in other words, it transforms smooth strokes into jagged polylines).

Error
   The maximum distance allowed between the new simplified stroke and the original one
   (the larger this value is, the more jagged/approximated the resulting polylines are).


## Sampling

.. _bpy.types.LineStyleGeometryModifier_Sampling:

********
Sampling
********

The *Sampling* modifier changes the definition, precision of the stroke,
for the following modifiers.

Sampling
   The smaller this value, the more precise are the strokes.
   Be careful; too small values will require a huge amount of time and memory during render!


## Simplification

.. _bpy.types.LineStyleGeometryModifier_Simplification:

**************
Simplification
**************

The *Simplification* modifier merges stroke vertices that lie close to one another,
like the *Decimate* modifier for meshes.

Tolerance
   Measure for how close points have to be to each other to be merged.
   A higher tolerance means more vertices are merged.

.. figure:: /images/render_freestyle_parameter-editor_line-style_modifiers_geometry_simplification_example.png
   :width: 50%
   :align: center


## Sinus Displacement

.. _bpy.types.LineStyleGeometryModifier_SinusDisplacement:

******************
Sinus Displacement
******************

The *Sinus Displacement* modifier adds a sinusoidal displacement to the stroke.

Wavelength
   How wide the undulations are along the stroke.
Amplitude
   How high the undulations are across the stroke.
Phase
   Allows "offsetting" ("moving") the undulations along the stroke.

   .. tip::

      The undulations this modifier produces look exactly the same at a Phase of ``0``
      and any positive or negative multiple of the ``Wavelength`` set on the modifier.
      This can be used for rendering short video sequences with wavy lines
      that can then be seamlessly looped without any visual jumps in the undulations along the line.

.. figure:: /images/render_freestyle_parameter-editor_line-style_modifiers_geometry_sinus-displacement_example.png
   :width: 50%
   :align: center

   Sinus Displacement modifier demo by T.K.
   (`blend-file <https://archive.blender.org/wiki/2015/index.php/File:Toycar_Sinus.zip>`__).


## Spatial Noise

.. _bpy.types.LineStyleGeometryModifier_SpatialNoise:

*************
Spatial Noise
*************

The *Spatial Noise* modifier adds some spatial noise to the stroke.
Spatial noise displacements are added in the normal direction
(i.e. the direction perpendicular to the tangent line) evaluated at each stroke vertex.

Amplitude
   How much the noise distorts the stroke.
Scale
   How wide the noise is along the stroke.
Octaves
   The level of detail of the noise.
Smooth
   When enabled, apply some smoothing over the generated noise.
Pure Random
   When disabled, the next generated random value depends on the previous one;
   otherwise they are completely independent. Disabling this setting gives a more "consistent" noise along a stroke.


## Tip Remover

.. _bpy.types.LineStyleGeometryModifier_TipRemover:

***********
Tip Remover
***********

The *Tip Remover* modifier removes a piece of the stroke at its beginning and end.

Tip Length
   Length of stroke to remove at both of its tips.


## Along Stroke

.. DO NOT EDIT FILE. This is simply a stub which copies everything from the link below.
.. include:: /render/freestyle/view_layer/line_style/modifiers/color/along_stroke.rst
   :start-after: .. --- copy below this line ---


## Calligraphy

.. _bpy.types.LineStyleThicknessModifier_Calligraphy:

***********
Calligraphy
***********

The *Calligraphy* modifier mimics some broad and flat pens for calligraphy.
It generates different thickness based on the orientation of the stroke.

Orientation
   The angle (orientation) of the virtual drawing tool, from the vertical axis of the picture.
   For example, an angle of 0.0 mimics a pen aligned with the vertical axis.
   Hence, the thickest strokes will be the vertical ones i.e. stroke's direction is aligned with the angle, and
   the thinnest will be the horizontal ones i.e. stroke's direction is perpendicular to the angle.

.. figure:: /images/render_freestyle_parameter-editor_line-style_modifiers_thickness_calligraphy_example.png
   :width: 50%
   :align: center

   Calligraphy modifier demo by T.K.
   (`blend-file <https://archive.blender.org/wiki/2015/index.php/File:Toycar_Calligraphy.zip>`__).


## Crease Angle

.. DO NOT EDIT FILE. This is simply a stub which copies everything from the link below.
.. include:: /render/freestyle/view_layer/line_style/modifiers/color/crease_angle.rst
   :start-after: .. --- copy below this line ---


## Curvature 3D

.. DO NOT EDIT FILE. This is simply a stub which copies everything from the link below.
.. include:: /render/freestyle/view_layer/line_style/modifiers/color/curvature_3d.rst
   :start-after: .. --- copy below this line ---


## Distance From Camera

.. DO NOT EDIT FILE. This is simply a stub which copies everything from the link below.
.. include:: /render/freestyle/view_layer/line_style/modifiers/color/distance_from_camera.rst
   :start-after: .. --- copy below this line ---


## Distance From Object

.. DO NOT EDIT FILE. This is simply a stub which copies everything from the link below.
.. include:: /render/freestyle/view_layer/line_style/modifiers/color/distance_from_object.rst
   :start-after: .. --- copy below this line ---


## Index


#######################
  Thickness Modifiers
#######################

.. toctree::
   :glob:

   *


## Material

.. DO NOT EDIT FILE. This is simply a stub which copies everything from the link below.
.. include:: /render/freestyle/view_layer/line_style/modifiers/color/material.rst
   :start-after: .. --- copy below this line ---


## Noise

.. DO NOT EDIT FILE. This is simply a stub which copies everything from the link below.
.. include:: /render/freestyle/view_layer/line_style/modifiers/color/noise.rst
   :start-after: .. --- copy below this line ---


## Tangent

.. DO NOT EDIT FILE. This is simply a stub which copies everything from the link below.
.. include:: /render/freestyle/view_layer/line_style/modifiers/color/tangent.rst
   :start-after: .. --- copy below this line ---


## Index


###################
  Layers & Passes
###################

.. toctree::
   :maxdepth: 2

   introduction.rst
   view_layer.rst
   passes.rst


## Introduction

.. _bpy.ops.scene.view_layer:
.. _bpy.types.ViewLayer:
.. _render-layers:

************
Introduction
************

Renders can be separated into layers, to composite them back together afterwards.

Some example usages are applying compositing effects to characters separately,
blurring the background and foreground layers separately for depth of field,
or rendering different lighting variations of the same scene.

Using View Layers can also save you from having to re-render your entire image after each change,
allowing you to instead re-render only the layer(s) that you have altered.


View Layers
===========

.. figure:: /images/render_layers_introduction_list.png

   View Layers.

In the top of the screen there is a list of all the View Layers in the active scene.

.. _bpy.types.ViewLayer.name:

Name
   The name of the active view layer, click to edit the name.

.. _bpy.ops.scene.view_layer_add:

Add View Layer
   Will add a new view layer to the active scene.

   New
      Adds a new view layer.
   Copy Settings
      Adds a new view layer with all the settings of current view layer.
   Blank
      Adds a new view layer with all collections disabled.

.. _bpy.ops.scene.view_layer_remove:

Remove View Layer
   Will remove the selected view layer from the active scene.

   .. note::

      A scene must have at least one view layer.


Usage
=====

Each :doc:`Scene </scene_layout/scene/introduction>` has an associated set of
:doc:`Collections </scene_layout/collections/collections>`.
The visibility settings of each collection can be changed per View Layer to separate the
rendering of different objects and lights into layers.


Collections
===========

Per collection you can adjust the way how the render engine needs to render the objects inside.
Based on the render engine different options can be set.

.. figure:: /images/render_layers_introduction_viewlayer-collection.png

   Collection/View layer settings.

Disable from View Layer
   Remove this collection from the active view layer. Objects that are only in
   this collection will not be rendered for the active view layer.
   This is useful to sometimes leave out some object influence for a particular view layer.

Enable in View Layer
   Add this collection to the active view layer. Objects inside the collection
   will be rendered with the active view layer.

.. _bpy.ops.outliner.collection_holdout_set:

Set Holdout
   Objects inside this collection will generate a holdout/mask in the active view layer.

.. _bpy.ops.outliner.collection_holdout_clear:

Clear Holdout
   Clear the Set Holdout flag.

.. _bpy.ops.outliner.collection_indirect_only_set:

Set Indirect Only :guilabel:`Cycles Only`
   Objects inside this collection will only contribute to the final image
   indirectly through shadows and reflections.

.. _bpy.ops.outliner.collection_indirect_only_clear:

Clear Indirect Only :guilabel:`Cycles Only`
   Clear the *Indirect Only* flag. Objects inside this collection will contribute normally to the final image.


Cycles
======

.. reference::

   :Panel:     :menuselection:`View Layers --> Layer`

This section covers only the Render Layer settings appropriate for the Cycles renderer.
For the engine-independent settings, see :ref:`this section <render-layers>`.


Filter
------

.. _bpy.types.ViewLayer.use_sky:

Include
   Environment
      Disables rendering the *Environment* render pass in the final render.

   .. _bpy.types.ViewLayer.use_solid:

   Surfaces
      Disables rendering object materials in the final render.

   .. _bpy.types.ViewLayer.use_strand:

   Curves
      Disables rendering curve strands in the final render.

   .. _bpy.types.ViewLayer.use_volumes:

   Volume
      Disables rendering :doc:`Volumes </modeling/volumes/index>` in the final render.

.. _bpy.types.ViewLayer.use_motion_blur:

Use
   Motion Blur
      Render motion blur for this Layer,
      if enabled in the :ref:`Render Settings <bpy.types.RenderSettings.use_motion_blur>`.


Override
--------

.. _bpy.types.ViewLayer.material_override:

Material Override
   Overrides all materials in the render layer.

.. _bpy.types.ViewLayer.world_override:

World Override
   Overrides world background in the render layer.

.. _bpy.types.ViewLayer.samples:

Samples
   View layer samples to override the scene samples.
   Controlled by the :ref:`layer samples <bpy.types.CyclesRenderSettings.use_layer_samples>` in the Sampling panel.


## Passes

.. _bpy.types.RenderLayer:

******
Passes
******

.. reference::

   :Panel:     :menuselection:`Scene --> View Layers --> Passes`

Passes can be used to split rendered images into colors, direct and indirect light to edit them individually,
and also to extract data such as depth or normals.


.. _render_layers_passes_data:

Data
====

Cycles
------

Include
   Combined
      The final combination of render passes with everything included.
   Z
      Distance to any visible surfaces.

      .. note::

         The Z pass only uses one sample.
         When depth values need to be blended in case of motion blur or :term:`Depth of Field`, use the mist pass.
   Mist
      Distance to visible surfaces, mapped to the 0.0 - 1.0 range.
      When enabled, settings are in :ref:`World tab <bpy.types.WorldMistSettings>`.
      This pass can be used in compositing to fade out objects that are farther away.
   Position
      World position of objects in the scene.
   Normal
      Surface normal used for shading.
   Vector
      Motion vectors for the Vector Blur node. The four components consist of 2D vectors
      giving the motion towards the next and previous frame position in pixel space.
   UV
      Mapped UV coordinates, used to represent where on a mesh a texture gets mapped too.
      This is represented through the red and green channels of the image.
      The blue channel is encoded with a constant value of 1 but does not hold any information.
   Denoising Data
      Includes *Denoising Albedo*, *Denoising Normal*, and a render pass of the original combined
      pass before denoising.

   .. note:: The Z, Position, Object Index, and Material Index passes are not anti-aliased.

Indexes
   Object Index
      Creates a mask of the object that can be later read by
      the :doc:`ID Mask Node </compositing/types/mask/id_mask>` in the Compositor.
   Material Index
      Creates a mask of the material that can be later read by
      the :doc:`ID Mask Node </compositing/types/mask/id_mask>` in the Compositor.

Debug
   Sample Count
      Number of samples/camera rays per pixel, to analyze adaptive sampling.

Alpha Threshold
   Z, Index, normal, UV and vector passes are
   only affected by surfaces with alpha transparency equal to or higher than this threshold.
   With value 0.0 the first surface hit will always write to these passes, regardless of transparency.
   With higher values surfaces that are mostly transparent can be skipped until an opaque surface is encountered.


EEVEE
-----

Include
   Combined
      The final combination of render passes with everything included.
   Z
      Distance to any visible surfaces.
   Mist
      Distance to visible surfaces, mapped to the 0.0 - 1.0 range.
   Normal
      Surface normal used for shading.


Light
=====

Cycles
------

Diffuse
   Direct
      Direct lighting from diffuse and subsurface BSDFs.
      We define direct lighting as coming from lights, emitting surfaces,
      the background, or ambient occlusion after a single reflection or transmission off a surface.
      BSDF color is not included in this pass.
   Indirect
      Indirect lighting from diffuse and subsurface BSDFs. We define indirect lighting as coming from lights,
      emitting surfaces or the background after more than one reflection or transmission off a surface.
      BSDF color is not included in this pass.
   Color
      Color weights of diffuse and subsurface BSDFs.
      These weights are the color input socket for BSDF nodes, modified by any Mix and Add Shader nodes.

Glossy
   Direct, Indirect, Color
      Same as above, but for glossy BSDFs.

Transmission
   Direct, Indirect, Color
      Same as above, but for transmission BSDFs.

Volume
   Direct, Indirect
      Same as above, but for volumetric BSDFs.

Other
   Emission
      Emission from directly visible surfaces.
   Environment
      Emission from the directly visible background. When the film is set to transparent,
      this can be used to get the environment color and composite it back in.
   Ambient Occlusion
      Ambient occlusion from directly visible surfaces. BSDF color or AO factor is not included; i.e.
      it gives a 'normalized' value between 0 and 1.
   Shadow Catcher
      Extra indirect light information collected by objects with
      the :ref:`Shadow Catcher <render-cycles-object-settings-visibility>` option enabled.
      Multiply this pass with existing footage using the :doc:`/compositing/types/color/mix/mix_color`
      in the :doc:`Compositor </editors/compositor>` to add the indirect lighting
      information to the footage.

.. note::

   :doc:`Transparent BSDFs are given special treatment </render/cycles/render_settings/light_paths>`.
   A fully transparent surface is treated as if there is no surface there at all;
   a partially transparent surface is treated as if only part of the light rays can pass through.
   This means it is not included in the Transmission passes;
   for that a glass BSDF with index of refraction 1.0 can be used.


EEVEE
-----

Diffuse
   Light
      Direct lighting from diffuse BSDFs. We define lighting as coming from lights,
      the background, or ambient occlusion off a surface.
      BSDF color is not included in this pass.
   Color
      Color weights of diffuse BSDFs. These weights are the color input socket for BSDF nodes,
      modified by any Mix and Add Shader nodes.

Specular
   Light, Color
      Same as above, but for specular BSDFs.

Volume
   Light
      The scattering pass from volume objects or world.

Other
   Emission
      Emission from directly visible surfaces.
   Environment
      Emission from the directly visible background. When the film is set to transparent,
      this can be used to get the environment color and composite it back in.
   Shadow
      Shadows from light objects. Mostly useful for compositing objects with shadow into existing footage.
   Ambient Occlusion
      Ambient occlusion from directly visible surfaces. BSDF color or AO factor is not included; i.e.
      it gives a 'normalized' value between 0 and 1.


Effects
=======

:guilabel:`EEVEE only`

.. _bpy.types.ViewLayerEEVEE.use_pass_bloom:

Bloom
   The influence of the Bloom effect.

.. _bpy.types.ViewLayerEEVEE.use_pass_transparent:

Transparency
   Contain :ref:`Blended <bpy.types.Material.render_method>` surfaces,
   so they can be adjusted in the compositor and later mixed with opaque passes.

   This pass only supports monochromatic opacity.
   Colored opacity will show differently than in combined pass.


Cryptomatte
===========

Cryptomatte is a standard to efficiently create mattes for compositing.
Cycles outputs the required render passes, which can then be used in the Blender Compositor
or another compositor with Cryptomatte support to create masks for specified objects.

Unlike the Material and Object Index passes, the objects to isolate are selected in compositing.
The mattes will be anti-aliased and take into account effects like motion blur and transparency.

.. _bpy.types.ViewLayer.use_pass_cryptomatte_object:

Object
   Render cryptomatte object pass, for isolating objects in compositing.

.. _bpy.types.ViewLayer.use_pass_cryptomatte_material:

Material
   Render cryptomatte material pass, for isolating materials in compositing.

.. _bpy.types.ViewLayer.use_pass_cryptomatte_asset:

Asset
   Render cryptomatte asset pass, for isolating groups of objects with
   the same :doc:`parent </scene_layout/object/editing/parent>` in compositing.

.. _bpy.types.ViewLayer.pass_cryptomatte_depth:

Levels
   Sets how many unique objects can be distinguished per pixel.


Typical Workflow
----------------

#. Enable Cryptomatte Object render pass in the Passes panel, and render.
#. In the compositing nodes, create a Cryptomatte node and
   link the Render Layer matching Image and Cryptomatte passes to it.
#. Attach a Viewer node to the Pick output of the Cryptomatte node.
#. Use the Cryptomatte Add/Remove button to sample objects in the Pick Viewer node.
#. Use the Matte output of the Cryptomatte node to get the alpha mask.

.. seealso::

   :doc:`Cryptomatte Node </compositing/types/mask/cryptomatte>`.


.. _bpy.ops.scene.view_layer_add_aov:
.. _bpy.ops.scene.view_layer_remove_aov:
.. _bpy.types.AOV:

Shader AOV
==========

Shader AOVs (Arbitrary Output Variables) provide custom render passes for any shader node components.
As an artist this can be a good way to fix or tweak fine details of a scene in post-processing.
To use Shader AOVs create the pass in the *Shader AOV* panel then reference this pass with
the :doc:`AOV Output </render/shader_nodes/output/aov>` shading node.
Shader AOVs can be added or removed in the *Shader AOV* panel.
In this panel is a list of all AOV passes; each AOV in the list consists of a *Name* and *Data Type*.

.. _bpy.types.ViewLayer.active_aov_index:

Active AOV Index
   The name of the render pass; this is the *Name* that is referenced in the *AOV Output* node.
   Any names can be used for these passes,
   as long as they do not conflict with built-in passes that are enabled.

.. _bpy.types.AOV.type:

Data Type
   Shader AOVs can either express a *Color* or a *Value* output.
   The *Color* type as the name suggest can be used for a color but also for normals.
   A *Value* type can be used for any single numerical value.


.. _bpy.ops.scene.view_layer_add_lightgroup:
.. _bpy.ops.scene.view_layer_remove_lightgroup:

Light Groups
============

:guilabel:`Cycles only`

Light Groups provide render passes that only contains information from the lights within that group.
Light Groups can be used to easily tweak the lighting color and intensity of specific lights without
having to re-render the scene.

.. _bpy.types.ViewLayer.active_lightgroup_index:

Active Light Group Index
   The name of the render pass. This is the name that is used when adding
   :ref:`World Shaders <bpy.types.World.lightgroup>`, :ref:`Lights and Objects <bpy.types.Object.lightgroup>`
   to *Light Groups*.


Lightgroup Sync
---------------

These operators are available in the menu to the right of the Light Group list.

.. _bpy.ops.scene.view_layer_add_used_lightgroups:

Add Used Lightgroups
   Adds all Light Groups  to the View Layer that have been created
   elsewhere and have lights assigned to them.

.. _bpy.ops.scene.view_layer_remove_unused_lightgroups:

Remove Unused Lightgroups
   Deletes all Light Groups that do not have any lights assigned to them.


Combining
=========

Cycles
------

All these lighting passes can be combined to produce the final image as follows:

.. figure:: /images/render_layers_passes_combine.svg


EEVEE
-----

The passes can be combined to produce the final image as follows:

.. figure:: /images/render_layers_passes_eevee-combine.svg


Known Limitations
=================

- Blended materials are not rendered in render passes except the combined pass.
  Use the *Dithered* as :ref:`Render Method <bpy.types.Material.render_method>`
  to render transparent materials in render passes.
- Depth of field is not rendered in render passes except the combined pass.
  It is possible to add the depth of field back in the Compositor using
  the :ref:`Defocus node <bpy.types.CompositorNodeDefocus>`.
- EEVEE render passes exclude parts of the BSDF equation.
  :doc:`Shader to RGB </render/shader_nodes/converter/shader_to_rgb>` is not supported as it needs
  the full BSDF equation.


## View Layer


**********
View Layer
**********

.. reference::

   :Panel:     :menuselection:`Properties --> Scene --> View Layer`

.. figure:: /images/render_layers_view-layer_panel.png

   View Layer panel (shown here for the EEVEE render engine).

The Layer Panel shows the settings of the active View Layer.

.. _bpy.types.ViewLayer.use:

Use for Rendering
   The active view layer will be used during rendering.

.. _bpy.types.RenderSettings.use_single_layer:

Render Single Layer
   Only render the active view layer.

   .. note::

      This option is ignored when rendering from the command line.

.. seealso::

   Additional options shown in this panel are different for each render engine.
   See :doc:`Render Passes </render/layers/passes>` for the options per render engine.


## Index

.. _render-lights-index:

##########
  Lights
##########

.. toctree::
   :maxdepth: 2

   light_object.rst
   world.rst


## Light Object


*************
Light Objects
*************

.. reference::

   :Panel:     :menuselection:`Properties --> Light` and :menuselection:`Shader Editor --> Sidebar --> Settings`


Common Settings
===============

.. _bpy.types.Light.type:

Type
   Defines the type of light.
Color
   Color tint of the emitted light.


Renderer Settings
=================

- :doc:`EEVEE specific settings </render/eevee/light_settings>`
- :doc:`Cycles specific settings </render/cycles/light_settings>`


.. _bpy.types.PointLight:

Point Light
===========

.. figure:: /images/render_lights_light-object_viewport.png
   :align: right
   :width: 260px

   Point light.

The point light is an omni-directional point of light,
that is, a point radiating the same amount of light in all directions.
It's visualized by a plain, circled dot.
Being a point light source, the direction of the light hitting an object's surface
is determined by the line joining the light and the point on the surface of the object itself.
It can be used as simple model of e.g. a light bulb.

Light intensity/energy decays based on (among other variables)
distance from the point light to the object. In other words,
surfaces that are further away will be rendered darker.

.. _bpy.types.PointLight.energy:

Power
   Power of the light in Watts. Higher values increase the intensity of the light.
   Negative values can be set, but should be avoided for predictable and physically based result.

.. _bpy.types.PointLight.use_soft_falloff:

Soft Falloff
   Apply falloff to avoid sharp edges when the light geometry intersects with other objects.

.. _bpy.types.PointLight.shadow_soft_size:

Radius
   When larger than zero, light will be emitted from a spherical surfaces with the specified radius.
   Lights with larger size have softer shadows and specular highlights, and they will also appear dimmer
   because their power is distributed over a larger area.


.. _bpy.types.SpotLight:

Spot Light
==========

A spot light emits a cone-shaped beam of light from the tip of the cone,
in a given direction.

.. _bpy.types.SpotLight.energy:

Power
   Power of the light in Watts. Higher values increase the intensity of the light.
   Negative values can be set, but should be avoided for predictable and physically based result.

.. _bpy.types.SpotLight.use_soft_falloff:

Soft Falloff
   Apply falloff to avoid sharp edges when the light geometry intersects with other objects.

.. _bpy.types.SpotLight.shadow_soft_size:

Radius
   When larger than zero, light will be emitted from a spherical surfaces with the specified radius.
   Lights with larger size have softer shadows and specular highlights.


Beam/Spot Shape
---------------

.. figure:: /images/render_lights_light-object_terms.png
   :width: 610px

   Changing the spot options also changes the appearance of the spotlight as displayed in the 3D Viewport.

.. _bpy.types.SpotLight.spot_size:

Size
   The size of the outer cone of a spot,
   which largely controls the circular area a spot light covers.
   This slider in fact controls the angle at the top of the lighting cone,
   and can be between (1.0 to 180.0).

   .. list-table:: Changing the spot *Size* option.

      * - .. figure:: /images/render_lights_light-object_size45.png
             :width: 320px

        - .. figure:: /images/render_lights_light-object_size60.png
             :width: 320px

.. _bpy.types.SpotLight.spot_blend:

Blend
   The *Blend* slider controls the inner cone of the spot.
   The *Blend* value can be between (0.0 to 1.0).
   The value is proportional and represents that amount of space that the inner cone should
   occupy inside the outer cone *Size*.

   The inner cone boundary line indicates the point at which light from the spot will start to blur/soften;
   before this point its light will mostly be full strength.
   The larger the value of *Blend* the more blurred/soft the edges of the spotlight will be,
   and the smaller the inner cone's circular area will be (as it starts to blur/soften earlier).

   To make the spot have a sharper falloff rate and therefore less blurred/soft edges,
   decrease the value of *Blend*.
   Setting *Blend* to 0.0 results in very sharp spotlight edges, without any transition between light and shadow.

   The falloff rate of the spot light is a ratio between the *Blend* and *Size* values;
   the larger the circular gap between the two, the more gradual the light fades between *Blend* and *Size*.

   *Blend* and *Size* only control the spot light cone's aperture and softness ("radial" falloff);
   they do not control the shadow's softness as shown below.

   .. figure:: /images/render_lights_light-object_shadow-spotlight.png
      :width: 400px

      Render showing the soft edge spotlighted area and the sharp/hard object shadow.

   Notice in the picture above that the object's shadow is sharp as a result of the ray tracing,
   whereas the spotlight edges are soft.
   If you want other items to cast soft shadows within the spot area, you will need to alter other shadow settings.

.. _bpy.types.SpotLight.show_cone:

Show Cone
   Displays a transparent cone in 3D Viewport to visualize which objects are contained in it.


.. _bpy.types.AreaLight:

Area Light
==========

The area light simulates light originating from a surface (or surface-like) emitter.
For example, a TV screen, office neon lights, a window,
or a cloudy sky are just a few types of area light. The area light produces shadows with
soft borders by sampling a light along a grid the size of which is defined by the user.
This is in direct contrast to point-like artificial lights which produce sharp borders.

.. _bpy.types.AreaLight.energy:

Power
   Power of the light in Watts. Higher values increase the intensity of the light.
   Negative values can be set, but should be avoided for predictable and physically based result.

.. _bpy.types.AreaLight.shape:

Shape
   Shape of the light.

   :Rectangle:
      The shape of the light can be represented as a rectangle and changed with the "X" and "Y" values.
   :Square:
      The shape of the light can be represented as a square and changed with the *Size* property.
   :Disk:
      The shape of the light can be represented as a disk and changed with the *Size* property.
   :Ellipse:
      The shape of the light can be represented as an ellipse and changed with the X and Y values.

   .. tip::

      Choosing the appropriate shape for your area light will enhance the believability of your scene.
      For example, you may have an indoor scene and would like to simulate light entering through a window.
      You could place a *Rectangular* area light in a window (vertical) or from neon lights (horizontal)
      with proper ratio for *Size X* and *Size Y*. For the simulation of the light emitted by
      a TV screen, a vertical *Square* area light would be better in most cases.

.. _bpy.types.AreaLight.size:

Size / Size X / Size Y
   Dimensions for the *Square* or *Rectangle*.


.. _bpy.types.SunLight:

Sun Light
=========

A sun light provides light of constant intensity emitted in a single direction from infinitely far away.
It can be very handy for a uniform clear daylight open-space illumination. In the 3D Viewport,
the sun light is represented by an encircled black dot with rays emitting from it,
plus a dashed line indicating the direction of the light.

.. note::

   This direction can be changed by rotating the sun light, like any other object,
   but because the light is emitted from a location considered infinitely far away,
   the location of a sun light does not affect the rendered result.

.. _bpy.types.SunLight.energy:

Strength
   Strength of the lights in Watts per square meter. Typical values are
   around 250 for an overcast day and 1000 or more for direct sunlight.
   See more details at `Power of Lights`_.

.. _bpy.types.SunLight.angle:

Angle
   The size of the sun light according to its
   `angular diameter <https://en.wikipedia.org/wiki/Angular_diameter#Use_in_astronomy>`__
   as seen from earth.


Power of Lights
===============

The power of sun lights is specified in Watts per square meter.
The power of point lights, spot lights, and area lights is specified in Watts.
But this is not the electrical Watts that consumer light bulbs are rated at.
It is `Radiant Flux or Radiant Power <https://en.wikipedia.org/wiki/Radiant_flux>`__ which is also measured in Watts.
It is the energy radiated from the light in the form of visible light.

If you want to set the power to real world values, you have to convert the wattage of consumer bulbs
or :abbr:`LED (Light-Emitting Diode)` lights to radiant flux, but it is not a straightforward process.
The wattage of bulbs means the electrical power required to power them. LED lights have
a "Watt equivalent" which is neither the electrical power they require nor the amount of light they put out.
Some consumer lights specify `lumens or luminous flux <https://en.wikipedia.org/wiki/Lumen_(unit)>`__
which is the radiant flux weighted with the wavelengths perceived by the human eye.

To save you from doing the conversion, here is a table of typical power values for point, spot, and area lights:

========================  =========   ======================
Real world light          Power       Suggested Light Type
========================  =========   ======================
Candle                    0.05 W      Point
800 lm LED bulb           2.1 W       Point
1000 lm light bulb        2.9 W       Point
1500 lm PAR38 floodlight  4 W         Area, Disk
2500 lm fluorescent tube  4.5 W       Area, Rectangle
5000 lm car headlight     22 W        Spot, size 125 degrees
========================  =========   ======================

And a table of typical Strength values for sun lights:

============  =====================
Sun type      Strength
============  =====================
Clear sky     1000 W/m\ :sup:`2`
Cloudy sky    500 W/m\ :sup:`2`
Overcast sky  200 W/m\ :sup:`2`
Moonlight     0.001 W/m\ :sup:`2`
============  =====================

These values will likely produce much brighter or dimmer lights than you would expect,
because our eyes adapt while a render engine does not. So to compensate,
adjust the *Exposure* in :menuselection:`Render --> Film`.

To get realistic results, remember to also set the light size and color to realistic values.
The color of your lights will also influence how bright they appear to the human visual system.
If you leave the power unchanged, a green light will seem the brightest, red darker and blue the darkest.
Thus you might want to manually compensate for these perceived differences.


## World

.. _bpy.types.World:

*****************
World Environment
*****************

.. figure:: /images/render_lights_world_environment-lighting.jpg
   :align: right

   Lighting with an HDR image.

The world defines the environment that the scene is in.
The surface shader sets the background and environment lighting,
either as a fixed color, sky model or HDRI texture.
With volume shaders the entire scene can be covered in mist or other volumetric effects.


Surface
=======

.. reference::

   :Panel:     :menuselection:`World --> Surface`

The surface shader defines the light emission from the environment into the scene.
The world surface is rendered as if it is very distant from the scene,
and as such there is no two-way interacting between objects in the scene and the environment,
only light coming in. The only shader accepted is the Background node with a color input and
strength factor for the intensity of the light.


Image Based Lighting
--------------------

For image based lighting,
use the Environment Texture node rather than the Image Texture node for correct mapping.
This supports *Equirectangular* (also known as latitude/longitude) for environment maps,
and *Mirror Ball* mapping for converting photos of mirror balls to environment maps.


Volume
======

.. reference::

   :Panel:     :menuselection:`World --> Volume`

A volume shader can be applied to the entirely world, filling the entire space.

Currently this is most useful for night time or other dark scenes,
as the world surface shader or sun lights will have no effect if a volume shader is used.
This is because the world background is assumed to be infinitely far away,
which is accurate enough for the sun for example.
However, for modeling effects such as fog or atmospheric scattering,
it is not a good assumption that the volume fills the entire space,
as most of the distance between the sun and the earth is empty space.
For such effects it is be better to create a volume object surrounding the scene.
The size of this object will determine how much light is scattered or absorbed.


Viewport Display
================

.. reference::

   :Panel:     :menuselection:`World --> Viewport Display`

.. _bpy.types.World.color:

Color
   The color to render the 3D Viewport background when choosing
   :ref:`World Background <bpy.types.View3DShading.background_type>`.


## Assignment


**********
Assignment
**********

.. reference::

   :Panel:     :menuselection:`Material --> Material Slots`

Materials are data-blocks that can be created and then assigned to one or more objects.
An object can also have multiple materials assigned in different material slots,
which correspond to different parts of an object. If a smooth transition between
materials is desired, then mixing shader nodes with a :doc:`Mix shader </render/shader_nodes/shader/mix>`
is a better solution.


.. _bpy.types.MaterialSlot:

Material Slots
==============

Material slots link materials to objects and meshes.
By default objects only have a single material slot, which assigns a material to the entire object.
If different parts of the mesh need different materials, multiple material slots can be created.

.. figure:: /images/render_materials_assignment_panel-object-mode.png

   Material slots panel.


Slot List
---------

The object's material slots and active material displayed in a :ref:`List View <ui-list-view>`.

.. _bpy.ops.material.new:

Add Material Slot
   Add a new material slot on the object.

Remove Material Slot
   Remove a material slot from the object.

.. _bpy.ops.material.copy:

Copy Material
   Copy material shader nodes and settings to clipboard.

.. _bpy.ops.material.paste:

Paste Material
   Paste material shader nodes and settings from clipboard.

Copy Material to Selected
   Copy the same material assignment from the active to other selected objects.

Remove Unused Slots
   Removes all material slots not assigned to the object.


Data-Block
----------

Material
   The Material :ref:`Data-Block Menu <ui-data-block>` for the selected material slot.
   Here new materials can be created, or existing materials can to the material slot.

.. _bpy.types.MaterialSlot.link:

Link
   Specifies whether the material is to be linked to the object or to the object data.

   The Link selector has two choices, Data and Object.
   These two menu choices determine whether the material is linked to the object or to the data
   (e.g. a mesh or curve). The Data menu item determines that this material will be linked to the mesh's
   data-block which is linked to the object's data-block.
   The Object menu item determines that the material will be linked to the object's data-block directly.

   This has consequences of course. For example, different objects may share the same mesh data-block.
   Since this data-block defines the shape of the object any change in Edit Mode
   will be reflected on all of those objects.
   Moreover, anything linked to that mesh data-block will be shared by every object that shares that mesh.
   So, if the material is linked to the mesh, every object will share it.

   On the other hand, if the material is linked directly to the object data-block, the objects can have
   different materials and still share the same mesh.

   Short explanation: If connected to the object, you can have several instances of the same Object Data using
   different materials. If linked to mesh data, you cannot.
   See :doc:`Data System </files/introduction>` for more information.


Edit Mode
---------

To assign materials to different parts of a mesh, enter Edit Mode on the mesh.
Additional buttons will then appear in the material slots panel.

.. figure:: /images/render_materials_assignment_panel-edit-mode.png

   Material slots panel in Edit Mode.

.. _bpy.ops.object.material_slot_assign:

Assign
   Assign active material slot and material to the selected faces in the mesh,
   strokes in a Grease Pencil, and similar for other object types.

.. _bpy.ops.object.material_slot_select:

Select
   Select faces assigned to the active material slot.

.. _bpy.ops.object.material_slot_deselect:

Deselect
   Deselect faces assigned to the active material slot.


Reusing Existing Materials
==========================

Blender is built to allow you to reuse *anything*, including material settings,
between many objects. Instead of creating duplicate materials,
you can simply reuse an existing material.
There are several ways to do this using the Material's data-block menu:

*Single Object* -- With the object selected, click the sphere located to the left of the Material name.
A pop-up appears showing all the materials available in the current blend-file.
To use one, just click on it.

.. tip:: Searching for Materials

   The search field at the bottom of the material list allows you to search the names in the list.
   For example, by entering "wood" all existent materials are filtered so that
   only materials containing "wood" are displayed in the list.

*Multiple Objects* -- In the 3D Viewport, with :kbd:`Ctrl-L`
you can quickly link all selected objects to the material (and other aspects)
of the :ref:`active object <object-active>`.
Very useful if you need to set a large number of objects to the same material;
just select all of them,
then the object that has the desired material, and :kbd:`Ctrl-L` links them to that "parent".


Deleting a Material
===================

To delete a material, select the material and click X in the Available Materials List entry.

Although the material will seem to disappear immediately,
the Delete action can depend on how the material is used elsewhere.

If the material is linked to the object and there are other objects which use this material,
then the material will be removed from that object (but remain on all its other objects).

If the "Fake User" button has been lit in the Available Materials list,
then the material will be retained when the file is saved, even if it has no users.

Only if it has 0 "real" users, and no "Fake" user, will the material be permanently deleted.
Note that it will still remain in the Materials list until the blend-file is saved,
but will have disappeared when the file is reloaded.


.. _bi-multiple-materials:

Multiple Materials
==================

Normally, different colors or patterns on an object are achieved by adding textures to your materials.
However, in some applications you can obtain multiple colors on an object by assigning
different materials to the individual faces of the object.

To apply several materials to different faces of the same object,
you use the Material Slots options in the Materials header panel.

The workflow for applying a second material to some faces of an object covered by
a base material is as follows:

#. In Object Mode, create a base material.
#. Go into Edit Mode and Face Select (a new list will appear below
   the Active Material list with *Assign*, *Select*, *Deselect* buttons).
#. Select the faces to be colored with the second material.
#. In the Object Material Slots list, click the ``+`` button to create a new slot or select an existing material.
#. Click the *Assign* button, and the material will appear on the selected object faces.


## Index

.. _bpy.types.Material:
.. _bpy.ops.material:

#############
  Materials
#############

.. toctree::
   :maxdepth: 2

   introduction.rst
   components/index.rst
   assignment.rst
   preview.rst
   settings.rst
   line_art.rst
   legacy_textures/index.rst


## Introduction


************
Introduction
************

Materials control the appearance of meshes, curves, volumes and other objects.
They define the substance that the object is made of, its color and texture,
and how light interacts with it.

Physically based materials can be created using
the :doc:`Principled BSDF </render/shader_nodes/shader/principled>`,
:doc:`Principled Hair </render/shader_nodes/shader/hair_principled>`,
and :doc:`Principled Volume </render/shader_nodes/shader/volume_principled>` shaders.
With these uber shaders, a wide variety of materials including
plastic, glass, metal, cloth, skin, hair, smoke and fire can be created.

A flexible :doc:`shading nodes </render/shader_nodes/introduction>` system is used
to set up textures and create entirely different types of materials like toon shading.


Setting up Materials
====================

Materials can be created in either the :doc:`Material properties </editors/properties_editor>`,
or in the :doc:`Shader Editor </editors/shader_editor>`.
These provide a different view of the same shader nodes and material settings.

The default Shading workspace has a Shader Editor and a 3D Viewport that can be set to
Material Preview or Rendered shading, to interactively preview how the material interacts
with objects and lights in the scene.

Materials are data-blocks that can be :doc:`assigned </render/materials/assignment>`
to one or more objects, and different materials can be assigned to different parts of meshes.

Image textures can be created from scratch in :doc:`Texture Paint Mode </sculpt_paint/texture_paint/index>`,
or by loading in existing images with the :doc:`Image Texture node </render/shader_nodes/textures/image>`.
A variety of :doc:`procedural texture nodes </render/shader_nodes/textures/index>` is also available.


Components
==========

Materials consist of three shaders, defining the appearance of the surface,
the volume inside the object, and the displacement of the surface.

.. figure:: /images/render_materials_introduction_shaders.svg
   :align: center


Surface Shader
--------------

The :doc:`surface shader </render/materials/components/surface>` controls the textures
and light interaction at the surface of the mesh.


Volume Shader
-------------

The :doc:`volume shader </render/materials/components/volume>` defines the interior of the mesh.
A material can have just a volume shader for cases like smoke and fire,
or it can be combined with a surface shader for materials like cloudy glass.


Displacement
------------

The shape of the surface and the volume inside it may be altered by
:doc:`displacement </render/materials/components/displacement>`.
This way, textures can then be used to make the mesh surface more detailed.

Depending on the settings, the displacement may be virtual,
only modifying the surface normals to give the impression of displacement,
which is known as bump mapping, or a combination of real and virtual displacement.


Physically Based Shading
========================

The material system is built with physically-based rendering in mind,
separating how a material looks and which rendering algorithm is used to render it.
This makes it easier to achieve realistic results and balanced lighting,
though there are a few things to keep in mind.

In order for materials to work well with global illumination, they should be energy conserving.
That means they cannot reflect more light than comes in.
This property is not strictly enforced, but if colors are in the range 0.0 to 1.0, and
:abbr:`BSDF (Bidirectional Scattering Distribution Function)`\ s are only mixed together with
the Mix Shader node, this will automatically be true.

It is however, possible to break this,
with color values higher than 1.0 or using the Add Shader node, but one must be careful when
doing this to keep materials behaving predictably under various lighting conditions.


## Line Art

.. _bpy.types.MaterialLineArt:

********
Line Art
********

.. reference::

   :Panel:     :menuselection:`Material --> Line Art`

.. figure:: /images/render_materials_line-art_panel.png
   :align: right

   Line Art material properties.

.. _bpy.types.MaterialLineArt.use_material_mask:

Material Mask
   Material masks are a way to provide Line Art extra information about faces that caused the occlusion.
   So edges occluded by those faces can be selected to have different styles.

.. _bpy.types.MaterialLineArt.use_material_mask_bits:

Masks
   The layer to include faces of the current material.

.. _bpy.types.MaterialLineArt.mat_occlusion:

Levels
   Faces with this material will behave as if it has set number of layers in occlusion.

.. _bpy.types.MaterialLineArt.use_intersection_priority_override:
.. _bpy.types.MaterialLineArt.intersection_priority:

Intersection Priority
   Assigns an intersection priority value for this material.
   Note that this priority takes precedent over :ref:`Object <bpy.types.ObjectLineArt.intersection_priority>`
   or :ref:`Collection <bpy.types.Collection.lineart_intersection_priority>` priority values.
   The intersection line will be included into the object with the higher intersection priority value.


## Preview

.. _bpy.types.Material.preview_render_type:

*******
Preview
*******

The Preview panel gives a quick visualization of the active material applied in a simple scene.

Shape
   Preview the material on a Plane, Sphere, Cube, Hair, Shader Ball, Cloth or Fluid object.
   This shape is also used for previews when linking and appending materials.

Preview World :guilabel:`Cycles Only`
   Use the world from the current scene for lighting in the material preview.

.. figure:: /images/render_materials_preview_shapes.png

   Preview shapes.


## Settings


********
Settings
********

.. reference::

   :Panel:     :menuselection:`Material --> Settings`


Renderer Settings
=================

While shading nodes control the appearance, these settings control the quality and algorithms
that each renderer uses to render the material.

- :doc:`EEVEE specific settings </render/eevee/material_settings>`
- :doc:`Cycles specific settings </render/cycles/material_settings>`


Pass Index
==========

.. _bpy.types.Material.pass_index:

Pass Index
   Index number for the *Material Index* :doc:`render pass </render/layers/passes>`.
   This can be used to give a mask to a material and then be read with
   the :doc:`ID Mask Node </compositing/types/mask/id_mask>` in the Compositor.

.. note::

   :doc:`Volume Objects </modeling/volumes/introduction>` are not supported.


.. _render-materials-settings-viewport-display:

Viewport Display
================

These settings control the 3D Viewport display in solid shading.
They provide a faster alternative to full shader nodes,
which may be too heavy or distracting for tasks like modeling, layout or sculpting.

.. _bpy.types.Material.diffuse_color:

Color
   Diffuse or metal surface color.

.. _bpy.types.Material.metallic:

Metallic
   Blends between a non-metallic and metallic material model.
   A value of 1.0 gives a fully specular reflection tinted with the base color,
   without diffuse reflection or transmission.
   At 0.0 the material consists of a diffuse or transmissive base layer, with a specular reflection layer on top.

.. _bpy.types.Material.roughness:

Roughness
   Specifies microfacet roughness of the surface for metal and specular reflection.


## Displacement


************
Displacement
************

Detail can be added to the shape of a surface with displacement shaders.

To create displacement, connect a :doc:`Displacement </render/shader_nodes/vector/displacement>`
or :doc:`Vector Displacement </render/shader_nodes/vector/vector_displacement>` node
to the displacement input of the Material Output node. Procedural, painted or baked textures can
then be connected to these nodes.

.. figure:: /images/render_materials_components_displacement_node-setup.png

   Typical displacement node setup.

Three displacement methods exist, with varying accuracy, performance and memory usage.
The displacement method can be set per material in the
:ref:`Material Settings <bpy.types.Material.displacement_method>`.

.. figure:: /images/render_materials_components_displacement_example.jpg

   Bump only, displacement only, and displacement and bump combined.


Bump Only
=========

The least accurate but most memory efficient method is bump mapping.
This method does not actually alter the mesh surface, but merely changes the shading to make it seem so.

Bump maps are often used to add smaller details on a model, for example pores or wrinkles on skin.

For baked bump maps, 8-bit images are commonly used. However, 16 or 32-bit float maps can provide
better looking results. When using image textures use Cubic interpolation to avoid stepping artifacts,
these are more visible for bump maps than other types of textures.

.. important::

   Because bump mapping is a fake effect, it can cause artifacts if the actual shape of the geometry
   is too different from the bump mapped shape. If this happens the strength of bump mapping should
   be reduced or actual displacement should be used.


Displacement Only
=================

:guilabel:`Cycles Only`

The most accurate and memory intensive displacement method is to apply true displacement to
the mesh surface.

It requires the mesh to be finely subdivided, which can be memory intensive.
:doc:`Adaptive Subdivision </render/cycles/object_settings/adaptive_subdiv>` is the best way
to subdivide the mesh, so that exactly the right amount of subdivision is used depending on
the distance of the object to the camera.

For baked displacement maps, best results are achieved with 16 or 32-bit float maps,
as 8-bit images often can not represent all the necessary detail.

.. seealso::

   The :doc:`Displace Modifier </modeling/modifiers/deform/displace>` can also be used to displace a mesh.


Displacement and Bump
=====================

:guilabel:`Cycles Only`

Both methods can be combined to use actual displacement for the bigger displacement and
bump for the finer details. This can provide a good balance to reduce memory usage.

Once you subdivide the mesh very finely, it is better to use only actual displacement.
Keeping bump maps will then only increase memory usage and slow down renders.


## Index


##############
  Components
##############

.. toctree::
   :maxdepth: 2

   surface.rst
   volume.rst
   displacement.rst


## Surface


********
Surfaces
********

The surface shader defines the light interaction at the surface of the mesh.
One or more :abbr:`BSDF (Bidirectional Scattering Distribution Function)`\ s specify
if incoming light is reflected back, refracted into the mesh, or absorbed.

Emission defines how light is emitted from the surface,
allowing any surface to become a light source.


Terminology
===========

BSDF
   Stands for Bidirectional Scattering Distribution Function.
   It defines how light is reflected and refracted at a surface.
Reflection
   BSDFs reflect an incoming ray on the same side of the surface.
Transmission
   BSDFs transmit an incoming ray through the surface, leaving on the other side.
Refraction
   BSDFs are a type of *Transmission*, transmitting an incoming ray and
   changing its direction as it exits on the other side of the surface.


BSDF Parameters
===============

A major difference from non-physically-based renderers is that direct light reflection from
lights and indirect light reflection of other surfaces are not decoupled, but rather handled
using a single :abbr:`BSDF (Bidirectional Scattering Distribution Function)`.
This limits the possibilities a bit, but we believe overall it is helpful in creating
consistent-looking renders with fewer parameters to tune.

Roughness
   For the glossy :abbr:`BSDF (Bidirectional Scattering Distribution Function)`\ s,
   the *roughness* parameter controls the sharpness of the reflection, from 0.0 (perfectly sharp)
   to 1.0 (very soft).


## Volume


*******
Volumes
*******

Volume rendering is used to render various effects that cannot be represented by hard surfaces alone.

- Smoke, fire or clouds are set up using a volume object or fluid simulation,
  with only a volume shader.
- Meshes can also be used to create such shapes by removing the default surface shader
  and using a volume shader with the mesh shape defining the volume bounds
  and textures defining the volume density.
- Mist is created with a volume shader for the world,
  or with a large mesh object encompassing the scene.
- Absorption in glass is simulated by combining a glass surface shader with refraction
  and a volume absorption shader for the interior of the object.


Shading
=======

Principled Volume
-----------------

:doc:`Principled Volume </render/shader_nodes/shader/volume_principled>`
is a physically-based volume shader that can be used to create a wide range of volume materials.
It supports scattering, absorption and emission in one easy to use node.
Fire can be rendered with blackbody emission.

.. figure:: /images/render_materials_components_volume_principled.jpg
   :align: center

   Smoke and fire rendered with Principled Volume shader.


Volume Components
-----------------

For more control, volume shading components can be manually combined into a custom shader setup.

- :doc:`Volume Absorption </render/shader_nodes/shader/volume_absorption>`
  will absorb part of the light as it passes through the volume.
  This can be used to shade for example black smoke or colored glass objects, or mixed with the Volume Scatter node.
  This node is similar to the transparent BSDF node,
  it blocks part of the light and lets other light pass straight through.
- :doc:`Volume Scatter </render/shader_nodes/shader/volume_scatter>`
  lets light scatter in other directions as it hits particles in the volume.
  The anisotropy defines in which direction the light is more likely to scatter.
  A value of 0 will let light scatter evenly in all directions (similar to the diffuse BSDF node),
  negative values let light scatter mostly backwards, and positive values let light scatter mostly forward.
  This can be used to shade white smoke or clouds for example.
- :doc:`Emission </render/shader_nodes/shader/emission>`
  will emit light from the volume, for example for fire.

.. figure:: /images/render_materials_components_volume_node.jpg
   :align: center

   Volume Absorption, Scatter and Emission


Attributes
----------

When rendering smoke and fire, volume attributes are used to define the shape and shading of the volume.
The Principled Volume shader will use them by default, while custom volume shaders can use
the Attribute node to get attributes such as density, color and temperature.


Density
-------

All volume shaders have a density input.
The density defines how much of the light will interact with the volume,
getting absorbed or scattered, and how much will pass straight through. For effects such as
smoke you would specify a density field to indicate where in the volume there is smoke and
how much (density bigger than 0), and where there is no smoke (density equals 0).

Volumes in the real world consist of particles,
a higher density means there are more particles per unit volume. More particles means there is
a higher chance for light to collide with a particle and get absorbed or scattered,
rather than passing straight through.


Mesh Volumes
============

Meshes used for volume render should be closed and :term:`Manifold`.
That means that there should be no holes in the mesh.
Each edge must be connected to exactly two faces
such that there are no holes or T-shaped faces
where three or more faces are connected to an edge.

Normals must point outside for correct results.
The normals are used to determine if a ray enters or exits a volume,
and if they point in a wrong direction, or there is a hole in the mesh,
then the renderer is unable to decide what is the inside or outside of the volume.

These rules are the same as for rendering glass refraction correctly.


World Volume
============

A volume shader can also be applied to the world, filling the entire space.

Currently, this is most useful for night time or other dark scenes,
as the world surface shader or sun lights will have no effect if a volume shader is used.
This is because the world background is assumed to be infinitely far away,
which is accurate enough for the sun for example.
However, for modeling effects such as fog or atmospheric scattering,
it is not a good assumption that the volume fills the entire space,
as most of the distance between the sun and the earth is empty space.
For such effects it is be better to create a volume object surrounding the scene.
The size of this object will determine how much light is scattered or absorbed.


Multiple Scattering
===================

Real-world effects such as scattering in clouds or subsurface scattering require many
scattering bounces. However, unbiased rendering of such effects can be noisy, so by default
the number of bounces is zero in Cycles, and no support is available in EEVEE.
The effect you get when rendering with zero volume bounces is what is known as
"single scattering", the effect from more bounces is "multiple scattering".

For rendering materials like skin or milk that require multiple scattering,
subsurface scattering is more efficient and easier to control. Particularly the random walk
method can accurately render such materials.

For materials such as clouds or smoke that do not have a well-defined surface,
volume rendering is required. These look best with many scattering bounces,
but in practice one might have to limit the number of bounces to keep render times acceptable.


## Colors


******
Colors
******

The color of a texture can be modified with the *Brightness*, *Contrast*,
and *Saturation* buttons. All textures with RGB values, including
*Images* and *Environment Maps*, may be modified with the RGB sliders.

Clamp
   Set negative texture RGB and intensity values to zero,
   for some uses like displacement this option can be disabled to get the full range.

Multiply R, G, B
   Tint the color of a texture by brightening each red, green and blue channel.

Brightness
   Change the overall brightness/intensity of the texture.

.. _bpy.types.Texture.contrast:

Contrast
   Change the contrast of the texture.

Saturation
   Change the saturation of the texture.


Color Ramp
==========

Activates a color ramp which allows you to remap the colors of a texture to new ones.


## Index

.. _textures-index:
.. _bpy.types.Texture:
.. _bpy.ops.texture:

###################
  Legacy Textures
###################

.. toctree::
   :titlesonly:
   :maxdepth: 2

   introduction.rst
   colors.rst


Types
=====

.. toctree::
   :maxdepth: 1

   types/blend.rst
   types/clouds.rst
   types/distorted_noise.rst
   types/image_movie.rst
   types/magic.rst
   types/marble.rst
   types/musgrave.rst
   types/noise.rst
   types/stucci.rst
   types/voronoi.rst
   types/wood.rst


## Introduction


************
Introduction
************

.. figure:: /images/render_materials_legacy-textures_introduction_menu.png
   :width: 230px

   The Texture Type list in the Texture panel of the Texture buttons.

Procedural textures are textures that are defined mathematically.
They are generally relatively simple to use,
because they do not need to be mapped in a special way.
This does not mean that procedural textures cannot become very complex.

These types of textures are 'real' 3D. By that we mean that they fit together perfectly at
the edges and continue to look like what they are meant to look like even when they are cut;
as if a block of wood had really been cut in two.
Procedural textures are not filtered or anti-aliased. This is hardly ever a problem:
the user can easily keep the specified frequencies within acceptable limits.


Common Options
==============

Noise Basis
-----------

Each noise-based Blender texture (except Voronoi and Simple Noise) has
a *Noise Basis* setting that allows the user to select
which algorithm is used to generate the texture.
This list includes the original Blender noise algorithm.
The *Noise Basis* settings makes the procedural textures extremely flexible (especially *Musgrave*).

The *Noise Basis* governs the structural appearance of the texture:

.. list-table::

   * - .. figure:: /images/render_materials_legacy-textures_introduction_noise-basis-blender-original.jpg
          :width: 160px

          Blender Original.

     - .. figure:: /images/render_materials_legacy-textures_introduction_noise-basis-voronoi-f1.jpg
          :width: 160px

          Voronoi F1.

     - .. figure:: /images/render_materials_legacy-textures_introduction_noise-basis-voronoi-f2-f1.jpg
          :width: 160px

          Voronoi F2-F1.

   * - .. figure:: /images/render_materials_legacy-textures_introduction_noise-basis-original-perlin.jpg
          :width: 160px

          Original Perlin.

     - .. figure:: /images/render_materials_legacy-textures_introduction_noise-basis-voronoi-f2.jpg
          :width: 160px

          Voronoi F2.

     - .. figure:: /images/render_materials_legacy-textures_introduction_noise-basis-voronoi-crackle.jpg
          :width: 160px

          Voronoi Crackle.

   * - .. figure:: /images/render_materials_legacy-textures_introduction_noise-basis-improved-perlin.jpg
          :width: 160px

          Improved Perlin.

     - .. figure:: /images/render_materials_legacy-textures_introduction_noise-basis-voronoi-f3.jpg
          :width: 160px

          Voronoi F3.

     - .. figure:: /images/render_materials_legacy-textures_introduction_noise-basis-cell-noise.jpg
          :width: 160px

          Cell Noise.

   * - .. figure:: /images/render_materials_legacy-textures_introduction_noise-basis-voronoi-f4.jpg
          :width: 160px

          Voronoi F4.

     - ..

     - ..

There are two more possible settings for *Noise Basis*, which are relatively similar to *Blender Original*:
Improved Perlin and Original Perlin.


Nabla
-----

Almost all procedural textures in Blender use derivatives for calculating normals for texture mapping
(except *Blend* and *Magic*). This is important for Normal and Displacement Maps.
The strength of the effect is controlled with the *Nabla* number field.


Hints
=====

Procedural textures can either produce colored textures, intensity only textures,
textures with alpha values and normal textures.
If intensity only ones are used the result is a black-and-white texture,
which can be greatly enhanced by the use of ramps.
If on the other hand you use ramps and need an intensity value,
you have to switch on *No RGB* in the *Mapping* panel.


## Blend

.. _bpy.types.BlendTexture:

*****
Blend
*****

The Blend texture generates a smoothly interpolated progression.
This is one of the most frequently used procedural textures.
You can use blend textures to blend other textures together (with *Stencil*),
or to create nice effects (especially with the *Mapping: Normal* trick).

.. note::

   Remember that if you use a ramp to create a custom blending, you may have to use *No RGB*,
   if the *Mapping* value needs an intensity input.

.. figure:: /images/render_materials_legacy-textures_types_blend_panel.png

   Blend Texture panels.


Options
=======

Progression
   Profile of blend.

   Linear
      A linear progression.

Quadratic
   A quadratic progression.
Easing
   A flowing, nonlinear progression.
Diagonal
   A diagonal progression.
Spherical
   A progression with the shape of a three-dimensional ball.
Quadratic Sphere
   A quadratic progression with the shape of a three-dimensional ball.
Radial
   A radial progression: *Horizontal* / *Vertical*.
   The direction of the progression is flipped a quarter turn.


## Clouds

.. _bpy.types.CloudsTexture:

******
Clouds
******

Clouds represent Perlin noise. In addition, each noise-based Blender texture
(with the exception of Voronoi and simple noise) has a *Noise Basis* setting that allows
the user to select which algorithm is used to generate the texture. This is often used for
Clouds, Fire, Smoke. Well-suited to be used as a Bump map, giving an overall irregularity to the material.

.. figure:: /images/render_materials_legacy-textures_types_clouds_panel.png

   Clouds Texture panels.


Options
=======

Grayscale
   The standard noise, gives an intensity.
Color
   The noise gives an RGB value.
Noise
   *Soft* or *Hard*, changes contrast and sharpness.
Size
   The dimension of the Noise table.
Depth
   The depth of the *Clouds* calculation.
   A higher number results in a long calculation time, but also in finer details.


## Distorted Noise

.. _bpy.types.DistortedNoiseTexture:

***************
Distorted Noise
***************

*Distortion Noise* takes the option that you pick from *Noise Basis* and filters it, to create hybrid pattern.
It is often used for grunge but is also very complex and versatile.

.. figure:: /images/render_materials_legacy-textures_types_distorted-noise_panel.png

   Distorted Noise Texture panels.


Options
=======

Noise Distortion
   The texture to use to distort another.
Basis
   The texture to be distorted.
Noise
   The size of the noise generated.
Distortion
   The amount that *Distortion Noise* affects *Basis*.


## Image Movie

.. _bpy.types.ImageTexture:

**************
Image or Movie
**************

The term *Image Texture* simply means that a graphic image,
which is a pixel grid composed of R, G, B, and sometimes Alpha values.
It is used as the input source to the texture.
As with other types of textures, this information can be used in a number of ways,
not only as a simple "decal".

*Video textures* are a some kind of Image textures and
based on movie file or sequence of successive numbered separate images.
They are added in the same way that image textures are.

When the Texture Type *Image or Movie* is selected, three new panels present
themselves allowing to control most aspects of how image textures are applied:
*Image*, *Image Sampling*, and *Image Mapping*.


About Image-Based Texturing
===========================

Texture images take up precious memory space,
often being loaded into a special video memory bank that is very fast and very expensive,
so it is often very small. So, keep the images as small as possible.
A 64×64 image takes up only one fourth the memory of a 128×128 image.

For photorealistic rendering of objects in animations, often larger image textures are used,
because the object might be zoomed in on in camera moves. In general, you want to use
a texture sized proportionally to the number of pixels that it will occupy in the final render.
Ultimately, you only have a certain amount of physical RAM to hold an image texture and
the model and to provide workspace when rendering your image.

For the most efficient memory usage, image textures should be square, with dimensions as powers of 2,
such as 32×32, 64×64, 128×128, 256×256, 1024×1024, 2048×2048, and 4096×4096.

If you can reuse images across different meshes, this greatly reduces memory requirements.
You can reuse images if you map those areas of the meshes that "look alike" to a layout that
uses the common image.

When using file textures, it is very important that you have
:doc:`Mapped the UVs </modeling/meshes/uv/unwrapping/index>`
of the mesh, and they are laid out appropriately.

You do not have to UV map the *entire* mesh.
The sphere above on the left has some faces mapped,
but other faces use procedural materials and textures.
Only use UV textures for those portions of your mesh where you want very graphic,
precise detail. For example,
a model of a vase only needs UV texture for the rim where decorative artwork is incorporated.
A throw pillow does not need a different image for the back as the front;
in fact many throw pillows have a fabric (procedural material) back.

As another example, you should UV map both eyes of a head to the same image
(unless you want one bloodshot and the other clear).
Mapping both sides of a face to the same image might not be advisable,
because the location of freckles and skin defects are not symmetrical.
You could of course change the UV map for one side of the face to slightly offset,
but it might be noticeable.
Ears are another example where images or section of an image can be mapped to similar faces.


Options
=======

Image
   The Image :ref:`ui-data-block`.


.. _bpy.types.ImageTexture.use_alpha:

Alpha
-----

Use the alpha channel information stored in the image.
Where the alpha value in the image is less than 1.0,
the object will be partially transparent and things behind it will be visible.
Works with :ref:`image formats <files-media-image_formats>` that store transparency information.

.. _bpy.types.ImageTexture.use_calculate_alpha:

Calculate
   Calculate an alpha based on the RGB values of the Image.
   Black (0, 0, 0) is transparent, white (1, 1, 1) opaque.
   Enable this option if the image texture is a mask.
   Note that mask images can use shades of gray that result in semi-transparency,
   like ghosts, flames, and smoke/fog.

   .. list-table:: The image with various alpha and gray-scale values.

      * - .. figure:: /images/render_materials_legacy-textures_types_image-movie_alpha-use.png
             :width: 320px

             Image with *Use* alpha. The alpha values of the pixels are evaluated.

        - .. figure:: /images/render_materials_legacy-textures_types_image-movie_alpha-calculate.png
             :width: 320px

             Image with *Calculate* alpha only, *Use Alpha* in the *Image* panel is disabled.

.. _bpy.types.ImageTexture.invert_alpha:

Invert
   Reverses the alpha value.
   Use this option if the mask image has white where you want it transparent and vice versa.


Mapping
-------

.. figure:: /images/render_materials_legacy-textures_types_image-movie_image-mapping-panel.png

   Image Mapping panel.

In the *Mapping* panel,
you can control how the image is mapped or projected onto the 3D model.

.. _bpy.types.ImageTexture.use_flip_axis:

Flip Axes
   Rotates the image 90 degrees counterclockwise when rendered.

.. _bpy.types.ImageTexture.extension:

Extension
   How the image is extrapolated beyond its original bounds.

   :Extend:
      Outside the image the colors of the edges are extended.
   :Clip:
      Clip to image size and set exterior pixels as transparent.
      Outside the image, an alpha value of 0.0 is returned.
      This allows you to 'paste' a small logo on a large object.
   :Clip Cube:
      Clips to cubic-shaped area around the images and sets exterior pixels as transparent.
      The same as Clip, but now the 'Z' coordinate is calculated as well.
      An alpha value of 0.0 is returned outside a cube-shaped area around the image.
   :Repeat:
      The image is repeated horizontally and vertically.

      Repeat X, Y
         X/Y repetition multiplier.
      Mirror X, Y
         Mirror on X/Y axes. These buttons allow you to map the texture as a mirror, or automatic flip of the image,
         in the corresponding X and/or Y direction.
   :Checker:
      Checkerboards quickly made.
      You can use the option *size* on the *Mapping* panel as well to create the desired number of checkers.

      Tiles Even/Odd
         Set even/odd tiles.
      Distance
         Governs the distance between the checkers in parts of the texture size.


Crop
^^^^

Minimum X, Y / Maximum X, Y
   The offset and the size of the texture in relation to the texture space.
   Pixels outside this space are ignored.
   Use these to crop, or choose a portion of a larger image to use as the texture.


Sampling
--------

In the *Sampling* panel you can control how the information is retrieved from the image.

.. figure:: /images/render_materials_legacy-textures_types_image-movie_image-sampling-panel.png

   Image Sampling panel.

.. _bpy.types.ImageTexture.use_interpolation:

Interpolation
   This option interpolates the pixels of an image.
   This becomes visible when you enlarge the picture. By default, this option is on.
   Turn this option off to keep the individual pixels visible and if they are correctly anti-aliased.
   This last feature is useful for regular patterns, such as lines and tiles;
   they remain 'sharp' even when enlarged considerably.
   Turn this image off if you are using digital photos to preserve crispness.

   .. list-table::

      * - .. figure:: /images/render_materials_legacy-textures_types_image-movie_interpolation-off.png
             :width: 320px

             Enlarged Image texture without *Interpolation*.

        - .. figure:: /images/render_materials_legacy-textures_types_image-movie_interpolation-on.png
             :width: 320px

             Enlarged Image texture with *Interpolation*.

.. _bpy.types.ImageTexture.use_mipmap:

MIP Map
   :term:`Mip-maps <Mip-map>` are precalculated, smaller, filtered textures for a certain size.
   A series of pictures is generated, each half the size of the former one.
   This optimizes the filtering process. By default, this option is enabled and speeds up rendering.
   When this option is off,
   you generally get a sharper image, but this can significantly increase calculation time if the filter dimension
   (see below) becomes large. Without mip-maps you may get varying pictures from slightly different camera angles,
   when the textures become very small. This would be noticeable in an animation.

.. _bpy.types.ImageTexture.use_mipmap_gauss:

Gaussian Filter
   Used in conjunction with mip-mapping, it enables the mip-map to be made smaller based on color similarities.
   In game engines, you want your textures, especially your mip-map textures,
   to be as small as possible to increase rendering speed and frame rate.

.. _bpy.types.ImageTexture.filter_type:

Filter Type
   Texture filter to use for image sampling.
   Just like a *pixel* represents a *pic* ture *el* ement, a *texel* represents a *tex* ture *el* ement.
   When a texture (2D texture space) is mapped onto a 3D model (3D model space),
   different algorithms can be used to compute a value for each pixel based on samples from several texels.

   :Box:
      A fast and simple nearest-neighbor interpolation known as Monte Carlo integration.
   :EWA (Elliptical Weighted Average):
      One of the most efficient direct
      convolution algorithms developed by Paul Heckbert and Ned Greene in the 1980s.
      For each texel, EWA samples, weights, and accumulates texels within an elliptical footprint
      and then divides the result by the sum of the weights.

      Eccentricity
         Maximum Eccentricity. Higher values give less blur at distant/oblique angles, but is slower.
   :FELINE (Fast Elliptical Lines):
      Uses several isotropic probes at several points along a line in texture space to produce
      an anisotropic filter to reduce aliasing artifacts without considerably increasing rendering time.

      Light Probes
         Number of probes to use. An integer between 1 and 256.
         Further reading: McCormack, J; Farkas, KI; Perry, R; Jouppi, NP (1999)
         `Simple and Table Feline: Fast Elliptical Lines for Anisotropic Texture Mapping
         <https://www.hpl.hp.com/techreports/Compaq-DEC/WRL-99-1.pdf>`__, WRL
   :Area:
      Area filter to use for image sampling.

      Eccentricity
         Maximum Eccentricity. Higher values give less blur at distant/oblique angles, but is slower.

.. _bpy.types.ImageTexture.filter_size:

Size
   The filter size used in rendering, and also by the options *Mip Map* and *Interpolation*.
   If you notice gray lines or outlines around the textured object, particularly where the image is transparent,
   turn this value down from 1.0 to 0.1 or so.

.. _bpy.types.ImageTexture.use_filter_size_min:

Minimum Size
   Use Filter Size as a minimal filter value in pixels.


## Magic

.. _bpy.types.MagicTexture:

*****
Magic
*****

The Magic Texture node is used to add a psychedelic color texture.
It can be used for "Thin Film Interference" if you set *Mapping*
to *Reflection* and use a relatively high *Turbulence*.
The RGB components are generated independently with a sine formula.

.. figure:: /images/render_materials_legacy-textures_types_magic_panel.png

   Magic Texture panels.


Options
=======

Depth
   The depth of the calculation. A higher number results in a long calculation time, but also in finer details.
Turbulence
   The strength of the pattern.


## Marble

.. _bpy.types.MarbleTexture:

******
Marble
******

The marble texture is used to generate marble, fire, or noise with a structure.
Bands are generated based on the sine, saw, or triangular formula and noise turbulence.

.. figure:: /images/render_materials_legacy-textures_types_marble_panel.png

   Marble Texture panels.


Options
=======

Marble Type
   Three settings for soft to more clearly defined *Marble*.

   Soft, Sharp, Sharper
Noise basis
   Shape of wave to produce bands.

   Sine, Saw, Triangle
Noise Type
   The noise function works with two methods.

   Soft, Hard
Size
   The dimensions of the noise table.
Depth
   The depth of the *Marble* calculation.
   A higher value results in greater calculation time, but also in finer details.
Turbulence
   The turbulence of the sine bands.


## Musgrave

.. _bpy.types.MusgraveTexture:

********
Musgrave
********

The musgrave texture is used to generate organic materials,
but it is very flexible. You can do nearly everything with it.

.. figure:: /images/render_materials_legacy-textures_types_musgrave_panel.png

   Musgrave Texture panels.


Options
=======

Type
   This procedural texture has five noise types on which the resulting pattern can be based
   and they are selectable from a select menu at the top of the tab. The five types are:

   - Hetero Terrain
   - Fractal Brownian Motion (fBm)
   - Hybrid Multifractal
   - Ridged Multifractal
   - Multifractal

   These noise types determine the manner in which Blender layers successive copies of the same
   pattern on top of each other at varying contrasts and scales.

Examples with Basis: Voronoi: F1, Dimension: 0.5, Lacunarity: 0.15, Octave: 2.0.

.. list-table::

   * - .. figure:: /images/render_materials_legacy-textures_types_musgrave_heteroterrain.jpg
          :width: 120px

          Hetero Terrain.

     - .. figure:: /images/render_materials_legacy-textures_types_musgrave_fbm.jpg
          :width: 120px

          Fractal Brownian Motion.

     - .. figure:: /images/render_materials_legacy-textures_types_musgrave_hybridmultifractal.jpg
          :width: 120px

          Hybrid Multifractal.

     - .. figure:: /images/render_materials_legacy-textures_types_musgrave_ridgedmultifractal.jpg
          :width: 120px

          Ridged Multifractal.

     - .. figure:: /images/render_materials_legacy-textures_types_musgrave_multifractal.jpg
          :width: 120px

          Multifractal.

.. not implemented yet?
   In addition to the five noise types, Musgrave has a noise basis setting which determines
   the algorithm that generates the noise itself.
   These are the same noise basis options found in the other procedural textures.

The main noise types have four characteristics:

Dimension
   Fractal dimension controls the contrast of a layer relative to the previous layer in the texture.
   The higher the fractal dimension, the higher the contrast between each layer,
   and thus the more detail shows in the texture.
Lacunarity
   Lacunarity controls the scaling of each layer of the Musgrave texture,
   meaning that each additional layer will have a scale that is the inverse of the value which shows on the button.
   i.e. Lacunarity = 2 --> Scale = 1/2 original.
Octaves
   Octave controls the number of times the original noise pattern is overlaid on itself and
   scaled/contrasted with the fractal dimension and lacunarity settings.
Intensity
   Light intensity. Called *Offset* for *Hetero Terrain*.

The *Hybrid Multifractal* and *Ridged Multifractal* types have these additional settings:

Offset
   Both have a "Fractal Offset" button that serves as a "sea level"
   adjustment and indicates the base height of the resulting bump map.
   Bump values below this threshold will be returned as zero.
Gain
   Setting which determines the range of values created by the function.
   The higher the number, the greater the range.
   This is a fast way to bring out additional details in a texture where extremes are normally clipped off.


## Noise


*****
Noise
*****

.. figure:: /images/render_materials_legacy-textures_types_noise_panel.png

   Noise Texture panel.

Although this looks great, it is not Perlin Noise! This is a true, randomly generated Noise.
This gives a different result every time, for every frame, for every pixel.


Options
=======

There are no options for this noise.

Often used for
   White noise in an animation. This is not well suited if you do not want an animation.
   For material displacement or bump, use clouds instead.
Result(s)
   Intensity.


## Stucci

.. _bpy.types.StucciTexture:

******
Stucci
******

.. figure:: /images/render_materials_legacy-textures_types_stucci_panel.png

   Stucci Texture panels.

The *Stucci* texture is based on noise functions. It is often used for stone, asphalt, or oranges,
normally for bump mapping to create grainy surfaces.


Options
=======

Plastic / Wall In / Wall out
   Plastic is the standard Stucci, while the "walls" is where Stucci gets it name.
   This is a typical wall structure with holes or bumps.
Soft / Hard
   There are two methods available for working with Noise.
Size
   Dimension of the Noise table.
Turbulence
   Depth of the *Stucci* calculations.


## Voronoi

.. _bpy.types.VoronoiTexture:

*******
Voronoi
*******

The Voronoi texture is used to generate very convincing Metal,
especially the "Hammered" effect. Organic shaders (e.g. scales, veins in skin).

.. figure:: /images/render_materials_legacy-textures_types_voronoi_panel.png

   Voronoi Texture panels.


Options
=======

Distance Metric
   This procedural texture has seven Distance Metric options.
   These determine the algorithm to find the distance between cells of the texture. These options are:

   - Minkowski
   - Minkowski 4
   - Minkowski 1/2
   - Chebychev
   - Manhattan
   - Distance Squared
   - Actual Distance

   The *Minkowski* setting has a user definable value (the *Exponent* button)
   which determines the Minkowski exponent *e* of the distance function:

      (*x*\ :sup:`e` + *y*\ :sup:`e` + *z*\ :sup:`e`)\ :sup:`1/e`

   A value of one produces the *Manhattan* distance metric, a value less than one produces stars
   (at 0.5, it gives a *Minkowski 1/2*), and higher values produce square cells
   (at 4.0, it gives a *Minkowski 4*, at 10.0, a *Chebychev*).
   So nearly all Distance Settings are basically the same -- a variation of *Minkowski*.

   You can get irregularly-shaped rounded cells with
   the *Actual Distance* / *Distance Squared* options.

.. list-table::

   * - .. figure:: /images/render_materials_legacy-textures_types_voronoi_minkowski0-5.jpg
          :width: 200px

          Minkowski Exponent: 0.5 (Minkowski 1/2).

     - .. figure:: /images/render_materials_legacy-textures_types_voronoi_minkowski1.jpg
          :width: 200px

          Minkowski Exponent: 1 (Manhattan).

     - .. figure:: /images/render_materials_legacy-textures_types_voronoi_minkowski2.jpg
          :width: 200px

          Minkowski Exponent: 2 (Actual Distance).

   * - .. figure:: /images/render_materials_legacy-textures_types_voronoi_minkowski4.jpg
          :width: 200px

          Minkowski Exponent: 4 (Minkowski 4).

     - .. figure:: /images/render_materials_legacy-textures_types_voronoi_minkowski10.jpg
          :width: 200px

          Minkowski Exponent: 10 (Chebychev).

     - .. figure:: /images/render_materials_legacy-textures_types_voronoi_distancesquared.jpg
          :width: 200px

          Distance Squared (more contrast than Actual Distance).

Feature Weights
   These four sliders at the bottom of the Voronoi panel represent the values of the four Worley constants,
   which are used to calculate the distances between each cell in the texture based on the distance metric.
   Adjusting these values can have some interesting effects on the end result...

.. (no gallery yet) Check the Samples Gallery for some examples of these settings and what textures they produce.

Coloring
   Four settings (*Intensity*, *Position*, *Position and Outline*, and *Position, Outline, and Intensity*)
   that can use four different noise basis as methods to calculate color and intensity of the texture output.
   This gives the Voronoi texture you create with the "Worley Sliders"
   a completely different appearance and is the equivalent of the noise basis setting found on the other textures.


## Wood

.. _bpy.types.WoodTexture:

****
Wood
****

The wood texture is used to generate wood and ring-shaped patterns.

.. figure:: /images/render_materials_legacy-textures_types_wood_panel.png

   Wood Texture panels.


Options
=======

Noise Basis
   Shape of wave to produce bands.

   Sine, Saw, Triangle
Wood Type
   Set the bands to either straight or ring-shaped, with or without turbulence.

   Bands, Rings, Band Noise, Ring Noise
Noise Type
   There are two methods available for the Noise function.

   Soft, Hard
Size
   Dimension of the Noise table.
Turbulence
   Turbulence of the Band Noise and Ring Noise types.


## Animation


********************
Rendering Animations
********************

While rendering stills will allow you to view and save the image from the render buffer when
it is complete, animations are a series of images, or frames,
and are automatically saved directly out to a drive after being rendered.

After rendering the frames, you may need to edit the clips,
or first use the Compositor to do green-screen masking, matting, color correction, DOF,
and so on to the images. That result is then fed to the Sequencer where the strips are cut and
mixed and a final overlay is done.

Finally you can render out from the Sequencer and compress the frames into a playable movie clip.


Workflow
========

Generally, you do a lot of intermediate renders of different frames in your animation to check
for timing, lighting, placement, materials, and so on. At some point,
you are ready to make a final render of the complete animation for publication.

There are two approaches you can use when making a movie, or animation, with or without sound.
The approach you should use depends on the amount of CPU time you will need to render the movie.
You can render a "typical" frame at the desired resolution,
and then multiply by the number of frames that will ultimately go into the movie, to arrive at a total render time.

If the total render time is an hour or more, you want to use the "Frame Sequence" approach.
For example, if you are rendering a one-minute video clip for film, there will be
(60 seconds per minute) X (24 frames per second) or 1440 frames per minute.
If each frame takes 30 seconds to render,
then you will be able to render two frames per minute, or need 720 minutes (12 hours)
of render time.

Rendering takes all available CPU time; you should render overnight,
when the computer is not needed, or set Blender to a low priority while rendering,
and work on other things (be careful with the RAM space!).


.. rubric:: Direct Approach

The Direct Approach, which is highly **not** recommended and not a standard practice,
is where you set your output format to an AVI or MOV format,
and click *Animation* to render your scene directly out to a movie file.
Blender creates one file that holds all the frames of your animation.
You can then use Blender's :doc:`Video Sequencer </editors/video_sequencer/index>`
to add an audio track to the animation and render out to an MPEG format to complete your movie.


.. rubric:: Frame Sequence

The Frame Sequence is a much more stable approach,
where you set your output format to a still format (such as JPG, PNG or a multi-layer format).
Click *Animation* to render your scene out to a set of images,
where each image is a frame in the sequence.

Blender creates a file for each frame of the animation.
You can then use Blender's Compositor to perform any frame manipulation (post-processing).
You can then use Blender's :doc:`Video Sequencer </editors/video_sequencer/index>` to load that final image sequence,
add an audio track to the animation, and render out to an MPEG format to complete your movie.
The Frame Sequence approach is a little more complicated and takes more drive space,
but gives you more flexibility.

Here are some guidelines to help you choose an approach.


.. rubric:: Direct Approach

- Short segments with total render time under one hour.
- Stable power supply.
- Computer not needed for other uses.


.. rubric:: Frame Sequence Approach

- Total render time over one hour.
- Post-production work needed:

  - Color/lighting adjustment
  - Green screen/matte replacement
  - Layering/compositing
  - Multiple formats and resolutions of the final product
- Intermediate frames/adjustments needed for compression/codec.
- Precise timing (e.g. lip-sync to audio track) needed in parts.
- May need to interrupt rendering to use the computer, and want to be able to resume rendering where you left off.


Frame Sequence Workflow
=======================

#. First prepare your animation.
#. In the *Format* panel, choose the render size, Pixel Aspect Ratio, and the Range of Frames to use,
   as well as the frame rate, which should already be set.
#. In the Output panel set up your animation to be rendered out as images,
   generally using a format that does not compromise any quality.
#. Choose the output path and file type in the Output panel as well, for example ``//render/my-anim-``.
#. Confirm the range of your animation (frame Start and End).
#. Save your blend-file.
#. Press the *Animation* button and once the animation is finished,
   use your file manager to navigate to the output folder (``render`` in this example).
   You will see lots of images that have a sequence number attached to. These are the single frames.
#. In Blender, open the :doc:`Video Sequencer </video_editing/index>`.

   .. note::

      The Video Sequencer does not support multi-layer EXR files.
      To render to a video format you will have to skip the next three steps and
      instead use an :doc:`Image Input node </compositing/types/input/image>`
      in the :doc:`Compositor </compositing/types/input/image>`.

#. Choose *Add Image* from the add menu. Select all the frames from your output folder that you want to include
   in your animation. They will be added as a strip in the Sequence editor.
#. Now you can edit the strip and add effects or leave it like it is.
   You can add other strips, like an audio strip.
#. Scrub through the animation to check if you have included all the frames.
#. In the Output panel, choose the container and codec you want (e.g. ``MPEG H.264``) and configure them.
   The video codecs are described in :doc:`Output Options </render/output/properties/output>`.
#. Click the *Animation* render button and Blender will render out the Sequence editor output into a movie.


Hints
=====

Your computer accidentally turns off in the middle of rendering your movie!
   Unless your animation renders in a few minutes,
   it is best to render the animation as separate image files.
   Instead of rendering directly to a compressed movie file, use a lossless format (e.g. ``PNG``).

   This allows you an easy recovery if there is a problem and you have to re-start the rendering,
   since the frames you have already rendered will still be in the output directory.

   Just disable the *Overwrite* option to start rendering where you left off.

   You can then make a movie out of the separate frames with Blender's Sequence editor
   or use 3rd party encoding software.

Animation Preview
   It can be useful to render a subset of the animated sequence,
   since only part of an animation may have an error.

   Using an image format for output,
   you can use the *Frame Step* option to render every *N'th* frame.
   Then disable *Overwrite* and re-render with *Frame Step* set to 1.


## Animation Player

.. _bpy.ops.render.play_rendered_anim:

.. |numsp| unicode:: U+2007

****************
Animation Player
****************

.. reference::

   :Menu:      :menuselection:`Topbar --> Render --> View Animation`
   :Shortcut:  :kbd:`Ctrl-F11`

The animation player is a utility typically used for previewing rendered animations,
supporting all image and video formats also supported by Blender.
This is a convenient way to play back image sequences at the correct frame rate.

Launching the animation player opens a new window,
playing back images or a video located at the render output of the current scene.
You can also drop images or movie files in a running animation player.
It will then restart the player with the new data.

.. tip::

   An external player can also be used instead of the one included in Blender.
   To do this, select it in the :doc:`Preferences </editors/preferences/file_paths>`.


Player Options
==============

Ping Pong
   When enabled, playback loops forwards than backwards.
X/Y Flip
   Flip the image horizontally or vertically.

   *Viewing the animation from a different perspective can help you see the animation with "fresh eyes".*


Hotkeys
=======

The following table shows the available hotkeys for the animation player.


.. rubric:: Playback

.. list-table::
   :header-rows: 1

   * - Action
     - Hotkey
   * - Start/Pause:
     - :kbd:`Spacebar`
   * - Start playback (when paused):
     - :kbd:`Return`
   * - Quit:
     - :kbd:`Esc`


.. rubric:: Timeline

.. list-table::
   :header-rows: 1

   * - Action
     - Hotkey
   * - Scrub in time:
     - :kbd:`LMB`
   * - Step back one frame:
     - :kbd:`Left`
   * - Step forward one frame:
     - :kbd:`Right`
   * - Step back 10 frames:
     - :kbd:`Down`
   * - Step forward 10 frames:
     - :kbd:`Up`
   * - Manual frame stepping:
     - :kbd:`NumpadPeriod`


.. rubric:: Playback Options

.. list-table::
   :header-rows: 1

   * - Action
     - Hotkey
   * - Backward playback:
     - :kbd:`Shift-Down`
   * - Forward playback
     - :kbd:`Shift-Up`
   * - Slow down playback:
     - :kbd:`NumpadMinus`
   * - Speed up playback:
     - :kbd:`NumpadPlus`
   * - Toggle looping:
     - :kbd:`Numpad0`
   * - Toggle frame skipping:
     - :kbd:`A`
   * - Toggle ping-pong:
     - :kbd:`P`


.. rubric:: Display

.. list-table::
   :header-rows: 1

   * - Action
     - Hotkey
   * - Toggle Playhead (Indicator):
     - :kbd:`I`
   * - Flip image on the X axis:
     - :kbd:`F`
   * - Flip image on the Y axis:
     - :kbd:`Shift-F`
   * - Hold to show frame numbers:
     - :kbd:`Shift`
   * - Zoom in:
     - :kbd:`Ctrl-NumpadPlus`
   * - Zoom out:
     - :kbd:`Ctrl-NumpadMinus`


.. rubric:: Frame Rate

.. list-table::
   :header-rows: 1

   * - Action
     - Hotkey
   * - 60 fps
     - :kbd:`Numpad1`
   * - 50 fps
     - :kbd:`Numpad2`
   * - 30 fps
     - :kbd:`Numpad3`
   * - 25 fps
     - :kbd:`Numpad4`
   * - 24 fps
     - :kbd:`Shift-Numpad4`
   * - 20 fps
     - :kbd:`Numpad5`
   * - 15 fps
     - :kbd:`Numpad6`
   * - 12 fps
     - :kbd:`Numpad7`
   * - 10 fps
     - :kbd:`Numpad8`
   * - |numsp|\ 6 fps
     - :kbd:`Numpad9`
   * - |numsp|\ 5 fps
     - :kbd:`NumpadSlash`


Frame Cache
===========

Image files are cached during playback for faster access.

While loading images is rarely a bottleneck,
there are situations where high resolution images may slow down playback causing frame skipping.

.. seealso::

   :ref:`Memory Cache Limit <bpy.types.PreferencesSystem.memory_cache_limit>` preference to control this limit,
   which may be increased to cache more images during playback.
   :ref:`command-line-args-animation-playback-options` to specify this value when launching from the command line.


## Index


#################
  Render Output
#################

.. toctree::
   :maxdepth: 2

   introduction.rst
   properties/index.rst
   audio/index.rst
   animation.rst
   animation_player.rst


## Introduction

.. todo: describe the steps to output renders

************
Introduction
************

The first step in the rendering process is to determine and set the output settings.
This includes render size, frame rate, pixel aspect ratio, output location, and file type.

When rendering a single frame, the output should be a single image format and not a video.
Several :doc:`image formats </files/media/image_formats>` are available.

Images can also be used for rendering animations which has a couple advantages.
For example, when rendering animations to image file formats the render job can be canceled
and resumed at the last rendered frame by changing the frame range.
This is useful if the animation takes a long time to render
and the computers resources are needed for something else.

Images can then be encoded to a video by adding the rendered image sequence into
the :doc:`Video Sequencer </editors/video_sequencer/sequencer/index>` and choosing an appropriate
:doc:`Video Output </files/media/video_formats>`.

.. tip::

   Rendered image sequences can be played back in the :ref:`Animation Player <bpy.ops.render.play_rendered_anim>`.

.. seealso::

   See :ref:`topbar-render`


## Index

.. _bpy.types.Sound:
.. _bpy.ops.sound:

###################
  Audio Rendering
###################

.. toctree::
   :maxdepth: 2

   introduction.rst
   speaker.rst


## Introduction


************
Introduction
************

Audio can be rendered from the :ref:`topbar-render`.


Options
=======

Relative Path
   Select the file path relative to the blend-file.

Accuracy
   Sample accuracy, important for animation data (the lower the value, the more accurate).

Audio Containers
   See :doc:`here </files/media/video_formats>`.

Codec
   Some *Audio Containers* also have option to choose a codec.
   For more information see :doc:`here </files/media/video_formats>`.

Split Channels
   Each audio channel will be rendered into a separate file.

.. seealso::

   - See :ref:`Scene Audio <data-scenes-audio>` settings.
   - See :ref:`Audio Output <render-output-video-encoding-audio>` settings.
   - See :ref:`Audio Preferences <prefs-system-sound>`.


## Speaker

.. _bpy.types.Speaker:
.. _bpy.ops.object.speaker:

*******
Speaker
*******

.. figure:: /images/render_output_audio_speaker_objects.png

   Speaker objects.

The speaker object is used to give sound in the 3D Viewport.
After adding the object, the various settings can be changed in the Properties.


Options
=======

Sound
-----

Open
   The :doc:`/interface/controls/templates/data_block` for loading audio files.
   There are two properties you can check when loading a sound:

   Cache
      This means the whole sound will be decoded and the raw audio data will be buffered in memory,
      which results in faster playback, but uses quite a lot of memory. So this should be used
      for short sound effects that get played more often, but not for longer audio files such as music.
   Mono
      For any 3D audio or panning effects the sound source has to be single channel,
      otherwise it's assumed that the 3D audio and panning information is already present in the multichannel file.
      Enable this if you want to use those effects for a file with multiple channels.
Mute
   Toggles whether or not the sound can be heard.
Volume
   Adjust the loudness of the sound.
Pitch
   Can be used to bend the pitch of the sound to be either deeper or higher.
   This basically changes the playback speed of the sound which also results in a pitch change.


Playback Time
^^^^^^^^^^^^^

There is no setting to choose the start time when the speaker should start playing,
because you might want a single speaker to play multiple times.
Therefore, you have to open the *NLA Editor* where you can add Sound strips
that define when the sound should start (nothing else,
so any other properties of the strips, like length don't matter).
When you add a speaker object such a strip will be added at the current frame.


Distance
--------

.. figure:: /images/render_output_audio_speaker_properties.png
   :align: right

   Speaker properties.

Distance attenuation relevant settings.

Volume
   Minimum/Maximum
      No matter how far away/close the object is, the distance-based volume won't be lower/higher than this value.
   Attenuation
      How strong the distance affects the volume.
      This factor sets the strength of the distance-based volume change,
      depending on the chosen distance model (see :ref:`scene settings <data-scenes-audio>`).

Distance
   Maximum
      If the object is farther away than this distance, this distance is used to calculate the distance-based volume.
      Influence of this value also depends on the distance model.
   Reference
      The distance at which the volume is full (1.0). Set this value to the distance used for recording the sound.
      Usually sound effects recordings should be made exactly 1 m away from sound to get an accurate volume.


Cone
----

Directionality relevant settings.

Imagine a cone with the top at the original of the speaker object
and the main axis of it facing in the same direction as the speaker.
There are two cones an inner and an outer cone. The angles represent their opening angles,
so 360° mean the cone is fully open and there is no directionality anymore.
Inside the inner cone the volume is full (1.0),
outside the outer cone the volume is, whatever one sets for the outer cone volume
and the volume between those two cones, linearly interpolated between this two volumes.

Angle
   Outer
      Angle of the outer cone in degrees. Outside this cone, the volume is equal to the *Outer* volume.
   Inner
      Angle of the inner cone in degrees. Inside the cone, the volume is full.
Volume
   Outer
      Volume outside the outer cone.


## Format


******
Format
******

.. figure:: /images/render_output_properties_format_panel.png

   Format panel.

Several render presets exist with common resolution and frame rates
for TVs and screens can be selected in the panel header.

.. _bpy.types.RenderSettings.resolution_x:
.. _bpy.types.RenderSettings.resolution_y:

Resolution X, Y
   The number of pixels horizontally and vertically in the image.

.. _bpy.types.RenderSettings.resolution_percentage:

Percentage
   Slider to reduce or increase the size of the rendered image relative to the *Resolution* values.
   This is useful for small test renders that have the same proportions as the final image.

.. _bpy.types.RenderSettings.pixel_aspect_x:
.. _bpy.types.RenderSettings.pixel_aspect_y:

Aspect X, Y
   Older televisions may have non-square pixels,
   so this can be used to control the shape of the pixels along the respective axis.
   This will *pre-distort* the images which will look stretched on a computer screen,
   but which will display correctly on a TV set.
   It is important that you use the correct pixel aspect ratio when rendering to prevent re-scaling,
   resulting in lowered image quality.

.. _bpy.types.RenderSettings.use_border:

Render Region
   Renders just a portion of the view instead of the entire frame.
   See the :ref:`Render Region <editors-3dview-navigate-render-region>`
   documentation to see how to define the size of the render region.

.. _bpy.types.RenderSettings.use_crop_to_border:

Crop to Render Region
   Crops the rendered image to the size of the render region,
   instead of rendering a transparent background around it.

.. _bpy.types.RenderSettings.fps:
.. _bpy.types.RenderSettings.fps_base:

Frame Rate
   The number of frames that are displayed per second, relevant for :doc:`Animation </render/output/animation>`.
   The menu gives several common frame rates, custom frame rates can be used by selecting *Custom*
   which gives access to the following properties:

   FPS
      The frame rate, expressed in frames per second.
   Base
      Some standards require a more precise frame rate, for example NTSC.
      These can be represent as a fraction where the *Base* value
      is used as the fraction's denominator and the FPS being the numerator:
      :math:`\frac{FPS}{Base}`.

   .. seealso::

      :ref:`Viewport Playback Frame Rate Limited <troubleshooting-fps_limit>`


## Frame Range


***********
Frame Range
***********

.. figure:: /images/render_output_properties_frame_range_panel.png

   Frame Range panel.

This panel defines how long an animation will last in terms of frames.
Frames can be divided by the scene's :ref:`Frame Rate <bpy.types.RenderSettings.fps>`
to get the animation duration in terms of time.
For example, a 250 frame animation at a frame rate of 30 will last 8.3 seconds.

.. _bpy.types.Scene.frame_start:
.. _bpy.types.Scene.frame_end:

Frame Start, End
   Set the *Start* and *End* frames for :doc:`Rendering Animations </render/output/animation>`.

.. _bpy.types.Scene.frame_step:

Step
   Controls the number of frames to advance by for each frame in the timeline.


Time Stretching
===============

Use to remap the length of an animation; making it run slower or faster.
The *Old* and *New* settings may either be used as absolute values or as a ratio:
For example, setting *Old* to a value of 2 and *New* to 1 will run the animation twice as fast.

.. warning::

   Using *Time Stretching* will not influence the *Start* or *End* frames set above,
   so make sure that your animation is not cut off or has extraneous still frames at the end.

.. _bpy.types.RenderSettings.frame_map_old:

Old
   The length in frames of the original animation.

.. _bpy.types.RenderSettings.frame_map_new:

New
   The length in frames the new animation will last.


## Index


#####################
  Output Properties
#####################

.. toctree::
   :maxdepth: 2

   format.rst
   frame_range.rst
   stereoscopy/index.rst
   output.rst
   metadata.rst
   post_processing.rst


## Metadata


********
Metadata
********

.. figure:: /images/render_output_properties_metadata_panel.png

   Metadata panel.

The *Metadata* panel includes options for writing metadata into render output.

.. note::

   Only some image formats support metadata:
   See :doc:`image formats </files/media/image_formats>`.

Metadata Input
   Where to grab metadata from.

   :Scene: Use metadata from the current scene.
   :Sequencer Strips: Use metadata from the strips in the Sequencer.

Include
   Date
      Includes the current date and time.
   Time
      Includes the current scene time and render frame at ``HH:MM:SS.FF``.
   Render Time
      Includes the render time.
   Frame
      Includes the frame number.
   Frame Range
      Includes the start and end frame numbers.
   Memory
      Includes the peak memory usage.
   Hostname
      Includes the rendering machine's `hostname <https://en.wikipedia.org/wiki/Hostname>`__.
   Camera
      Includes the name of the active camera.
   Lens
      Includes the name of the active camera's lens value.
   Scene
      Includes the name of the active scene.
   Marker
      Includes the name of the last marker.
   Filename
      Includes the filename of the blend-file.
   Strip Name
      Includes the name of the foreground sequence strip.


Note
====

Includes a custom note.

.. hint::

   It can be useful to use the *Note* field if you are setting up a render farm.
   Since you can script any information you like into it,
   such as an identifier for the render node or the job number.
   For details on stamping arbitrary values,
   see: `this page <https://blender.stackexchange.com/questions/26643>`__.


Burn into Image
===============

Add metadata as text to the render.

Font Size
   Set the size of the text.
Text Color
   Set the color and alpha of the stamp text.
Background
   Set the color and alpha of the color behind the text.
Include Labels
   Displays the labels before the metadata text. For example,
   "Camera" in front of the camera name, etc.


## Output


******
Output
******

.. figure:: /images/render_output_properties_output_panel.png

   Output panel.

This panel provides options for setting the location of rendered frames for animations,
and the quality of the saved images.

.. _bpy.types.RenderSettings.filepath:

File Path
   Choose the location to save rendered frames.

   When rendering an animation,
   the frame number is appended at the end of the file name with four padded zeros (e.g. ``image0001.png``).
   You can set a custom padding size by adding the appropriate number of ``#`` anywhere in the file name
   (e.g. ``image_##_test.png`` translates to ``image_01_test.png``).

   This setting expands :ref:`files-blend-relative_paths`
   where a ``//`` prefix represents the directory of the current blend-file.

Saving
   .. _bpy.types.RenderSettings.use_file_extension:

   File Extensions
      Adds the correct file extensions per file type to the output files.

   .. _bpy.types.RenderSettings.use_render_cache:

   Cache Result
      Saves the rendered view layers and their :doc:`passes </render/layers/passes>` to a multi-layer OpenEXR image.
      The Compositor can then use this file to improve performance, especially for heavy compositing.

      The image is stored in the *Render Cache* folder as specified in the
      :doc:`File Paths Preferences </editors/preferences/file_paths>`.
      You can also load it back into the Image Editor's Render Result, even after closing
      and reopening Blender; see :ref:`Open Cached Render <bpy.ops.image.read_viewlayers>`.

.. _bpy.types.ImageFormatSettings.file_format:

File Format
   Choose the file format to save to. Based on which format is used,
   other options such as channels, bit depth and compression level are available.

   For rendering out to images see: :ref:`saving images <bpy.types.ImageFormatSettings>`,
   for rendering to videos see the `Encoding`_ panel.

.. _bpy.types.ImageFormatSettings.color_mode:

Color
   Choose the color format to save the image to.
   Note that *RGBA* will not be available for all image formats.

   BW, RGB, RGBA

Image Sequence
   .. _bpy.types.RenderSettings.use_overwrite:

   Overwrite
      Overwrite existing files when rendering.

   .. _bpy.types.RenderSettings.use_placeholder:

   Placeholders
      Create empty placeholder frames while rendering.

.. hint:: Primitive Render Farm

   An easy way to get multiple machines to share the rendering workload is to:

   - Set up a shared directory over a network file system.
   - Disable *Overwrite*, enable *Placeholders* in the Render *Output* panel.
   - Start as many machines as you wish rendering to that directory.


.. _render-output-color-management-panel:

Color Management
================

This panel controls how :doc:`/render/color_management` is applied when saving images.

.. _bpy.types.ImageFormatSettings.color_management:

:Follow Scene:
   Uses the same color management settings defined by the active Scene.
   These properties are defined in the Render Settings
:Override:
   Uses custom color management settings defined by the properties below in the panel;
   disregarding any color management settings set at the Scene level.

For a detailed description of color management properties,
see the :ref:`Color Management <render-post-color-management>` page.


.. _render-output-video-encoding-panel:
.. _bpy.types.FFmpegSettings:

Encoding
========

.. reference::

   :Panel:     :menuselection:`Properties --> Output --> Encoding`

.. figure:: /images/render_output_properties_output_video-encoding-panel.png

   Encoding panel.

Here you choose which video container, codec, and compression settings you want to use.
With all of these compression choices, there is a trade-off between file size,
compatibility across platforms, and playback quality.
In the header, you can use the presets, which choose optimum settings for you for that type of output.

.. tip::

   When you view the :doc:`System Console </advanced/command_line/index>`,
   you can see some of the output of the encoding process.
   You will see even more output if you execute Blender as ``blender -d``.

.. _bpy.types.FFmpegSettings.format:

Container
   Video container or file type. For a list of all available options, see
   :doc:`video formats </files/media/video_formats>`.

.. _bpy.types.FFmpegSettings.use_autosplit:

Autosplit Output
   If your video is huge and exceeds 2GiB, enable Autosplit Output.
   This will automatically split the output into multiple files after the first file is 2GiB in size.


Video
-----

.. _bpy.types.FFmpegSettings.codec:

Video Codec
   Chooses the method of compression and encoding.
   For a list of all available options see :doc:`video formats </files/media/video_formats>`.

.. note:: Standards

   Some containers and codecs are not compatible with each other,
   so if you are getting errors check that your container and codec are compatible.
   Like containers and codecs are sometimes not compatible with each other, some codecs
   do not work with arbitrary dimensions. So, try to stick with common dimensions
   or research the limitations of the codec you are trying to use.

.. _bpy.types.FFmpegSettings.constant_rate_factor:

Output Quality
   These are preset `Rate`_.

.. _bpy.types.FFmpegSettings.ffmpeg_preset:

Encoding Speed
   Presets to change between a fast encode (bigger file size) and more compression (smaller file size).

.. _bpy.types.FFmpegSettings.gopsize:

Keyframe Interval
   The number of pictures per `Group of Pictures <https://en.wikipedia.org/wiki/Group_of_pictures>`__.
   Set to 0 for "intra_only", which disables `inter-frame <https://en.wikipedia.org/wiki/Inter-frame>`__ video.
   A higher number generally leads to a smaller file but needs a higher-powered device to replay it.

.. _bpy.types.FFmpegSettings.use_max_b_frames:

Max B-frames
   Enables the use of :term:`B‑frames <Frame Types>`.

   .. _bpy.types.FFmpegSettings.max_b_frames:

   Interval
      The maximum number of :term:`B‑frames <Frame Types>` between non-B-frames.


Rate
^^^^

.. _bpy.types.FFmpegSettings.video_bitrate:

Bitrate
   Sets the average `bit rate <https://en.wikipedia.org/wiki/Bit_rate>`__ (quality),
   which is the count of binary digits per frame.
   See also: `FFmpeg -b:v <https://ffmpeg.org/ffmpeg.html#Description>`__.

.. _bpy.types.FFmpegSettings.minrate:
.. _bpy.types.FFmpegSettings.maxrate:

Minimum / Maximum
   Video files can use what is called variable bit rate (VBR).
   This is used to give some segments of the video less compressing to frames that need more data
   and less to frames with less data. This can be controlled by the *Minimum* and *Maximum* values.

.. _bpy.types.FFmpegSettings.buffersize:

Buffer
   The `decoder bitstream buffer <https://en.wikipedia.org/wiki/Video_buffering_verifier>`__ size.

.. _bpy.types.FFmpegSettings.muxrate:

Mux Rate
   Maximum bit rate of the multiplexed stream.
   `Multiplexing <https://www.afterdawn.com/glossary/term.cfm/multiplexing>`__
   is the process of combining separate video and audio streams into a single file,
   similar to packing a video file and MP3 audio file in a zip-file.

.. _bpy.types.FFmpegSettings.packetsize:

Mux Packet Size
   Reduces data fragmentation or muxer overhead depending on the source.


.. _render-output-video-encoding-audio:

Audio
-----

These settings change how sound is exported while rendering.
To control how sound is played back from within Blender, see the audio settings
in the :ref:`Preferences <prefs-system-sound>`.

.. _bpy.types.FFmpegSettings.audio_codec:

Audio Codec
   Audio format to use. For a list of all available options, see
   :doc:`video formats </files/media/video_formats>`.

.. _bpy.types.FFmpegSettings.audio_channels:

Audio Channels
   Sets the audio channel count.

.. _bpy.types.FFmpegSettings.audio_mixrate:

Sample Rate
   Sets the audio `sampling rate <https://en.wikipedia.org/wiki/Sampling_(signal_processing)#Sampling_rate>`__.

.. _bpy.types.FFmpegSettings.audio_bitrate:

Bitrate
   For each codec, you can control the bit rate (quality) of the sound in the movie.
   Higher bit rates are bigger files that stream worse but sound better.
   Use powers of 2 for compatibility.

.. _bpy.types.FFmpegSettings.audio_volume:

Volume
   Sets the output volume of the audio.


Tips
----

.. tip::

   The choice of video format depends on what you are planning to do.

   It's not recommended to render directly to a video format in the first instance.
   If a problem occurs while rendering, the file might become unplayable and you will
   have to re-render all frames from the beginning. If you first render out a set
   of static images such as the default PNG format or the higher-quality OpenEXR
   (which can retain HDR pixel data), you can combine them as
   an :doc:`Image Strip </video_editing/edit/montage/strips/image>`
   in the Video Sequencer. This way, you can easily:

   - Restart the rendering from the place (the frame) where any problem occurred.
   - Try out different video encoding options in seconds,
     rather than minutes or hours as encoding is usually much faster than rendering the 3D scene.
   - Enjoy the rest of the features of the Video Sequencer, such as adding
     :doc:`Image Strips </video_editing/edit/montage/strips/image>`
     from previous renders, audio, video clips, etc.

.. tip::

   You shouldn't post-process a lossy-compressed file as the compression artifacts may become visible.
   Lossy compression should be reserved as a final 'delivery format'.


## Post Processing

.. _render-output-postprocess:

***************
Post Processing
***************

.. reference::

   :Panel:     :menuselection:`Properties --> Output --> Post Processing`

The Post Processing panel is used to control different options used to process your image after rendering.

.. figure:: /images/render_output_properties_post-processing_panel.png
   :align: right

   Post Processing panel.

Pipeline
   Todo.

   .. _bpy.types.RenderSettings.use_compositing:

   Compositing
      Renders the output from the compositing node setup,
      and then applies the Composite node tree on all images,
      displaying the image inputted in the Composite Output node.

   .. _bpy.types.RenderSettings.use_sequencer:

   Sequencer
      Renders the output of the Video Sequence editor, instead of the view from the 3D scene's active camera.
      If the sequence contains Scene strips, these will also be rendered as part of the pipeline.
      If *Compositing* is also enabled, the Scene strip will be the output of the Compositor.

.. _bpy.types.RenderSettings.dither_intensity:

Dither
   Dithering is a technique for blurring pixels to prevent banding that is seen in areas of
   gradients, where stair-stepping appears between colors.
   Banding artifacts are more noticeable when gradients are longer, or less steep.
   Dithering was developed for graphics with low bit depths,
   meaning they had a limited range of possible colors.

   Dithering works by taking pixel values and comparing them with a threshold and
   neighboring pixels then does calculations to generate the appropriate color.
   Dithering creates the perceived effect of a larger color palette by creating a sort of visual color mixing.
   For example, if you take a grid and distribute red and yellow pixels evenly across it,
   the image would appear to be orange.


## Index

.. _bpy.types.RenderView:

###############
  Stereoscopy
###############

.. toctree::
   :maxdepth: 2

   introduction.rst
   usage.rst


## Introduction


************
Introduction
************

.. figure:: /images/render_output_properties_stereoscopy_usage_viewport.png

Multi-view is a complete toolset for working with stereoscopic rendering in Blender.
It works with both the EEVEE and Cycles rendering engines.
Cycles additionally supports stereoscopic panoramic cameras.
There is support for many different stereo 3D visualization types.

.. note::

   If you have a real 3D display at some point you can change the 3D display mode in the Window menu,
   by calling the *Stereo 3D* operator.
   Be aware that some modes require a fullscreen editor to work, and this can be taxing on your CPU.


## Usage


*****
Usage
*****

For example, we will take an existing blend-file
that was made for monoscopic rendering and transform it to be stereo 3D ready.

.. figure:: /images/render_output_properties_stereoscopy_usage_anaglyph.png

   Creature Factory 2 by Andy Goralczyk rendered in stereo 3D (anaglyph).


Introduction
============

Start opening up your project file, in this case ``turntable.blend`` from the *Creature Factory 2*
Open Movie Workshop series from the Blender Institute by Andy Goralczyk.

.. figure:: /images/render_output_properties_stereoscopy_usage_turntable-creature.png

   Turntable Creature Factory 2.


Stereoscopy Setup
=================

Go to the :doc:`Output Properties </render/output/index>` and enable *Stereoscopy* for this scene.

.. figure:: /images/render_output_properties_stereoscopy_usage_views-panel.png

   Scene render views.

.. note::

   When you turn on *Stereoscopy* in the scene, you get 3D preview in the viewport,
   as well as multiple panels that are now accessible all over the user interface.

.. figure:: /images/render_output_properties_stereoscopy_usage_viewport.png

   Viewport with 3D visualization.


Camera
======

To tweak the stereo 3D parameters, select the camera in the Outliner.
In the Camera panel go to the Stereoscopy tab and change the *Convergence Distance*.

The viewport will respond in real-time to those changes allowing you to preview the current depth value of the scene.

.. figure:: /images/render_output_properties_stereoscopy_usage_camera-stereoscopy-panel.png

   Stereo convergence distance.


Viewport
========

Before fine-tuning the camera parameters,
you can set the convergence plane in the viewport based in your scene depth layout.
Go outside the camera view and you will instantly see the convergence plane in front of the camera.

You can toggle this and other display settings in the Stereoscopy panel of the 3D Viewport's Sidebar.
In the following image, the camera's frustum volumes are also visible.

.. figure:: /images/render_output_properties_stereoscopy_usage_stereo-preview.png
   :width: 700px

   Viewport plane and volume stereo preview.


Stereo 3D Display
=================

If you have a real 3D display at some point, you can change the 3D display mode in the Window menu,
by calling the Stereo 3D operator.
Be aware that some modes require a fullscreen editor to work.

.. figure:: /images/render_output_properties_stereoscopy_usage_window-stereo-3d.png

   Window menu, stereo 3D operator.


Viewport Preview
================

Before rendering your scene, you can save a Viewport Preview of the animation for testing in the final display.
In the Render Output panel you can choose the output *Views Format*.

The options include individual files per view, top-bottom, anaglyph among others.
Pick the one that fits your display requirements.

.. peertube:: 47448bc1-0cc0-4bd1-b6c8-9115d8f7e08c


Rendering and Image Editor
==========================

Once you are happy with the results, you can render out the final animation.
In the Image Editor you can inspect the individual views and the stereo result.


Image Formats
=============

Your final animation can be saved in more robust formats.
In this example we saved as cross-eyed side-by-side stereo 3D.

.. figure:: /images/render_output_properties_stereoscopy_usage_render-sidebyside.png

   Side-by-side cross-eye format.


Final Considerations
====================

As this guide showed, there is more to stereo 3D rendering than just generate two images.
The earlier the stereo pipeline is considered the smoother it will get.
The following sections are a more in-depth view of the individual components we visited in the workflow.


Window Stereo 3D Display
========================

An essential component of the Stereoscopy pipeline is the ability to display the stereo image in a proper display.
Blender supports from high-end 3D displays to simple red-cyan glasses.
On top of that, you can set a different display mode for each window.

The display mode can be changed via the Window menu
or if you create your own shortcuts for the ``wm.set_stereo_3d`` operator.

.. figure:: /images/render_output_properties_stereoscopy_usage_window-stereo-3d.png

   Window menu, stereo 3D operator.


Display Mode
------------

Anaglyph
   Render two differently filtered colored images for each eye.
   Anaglyph glasses are required. We support red-cyan, green-magenta and yellow-blue glasses.
Interlace
   Render two images for each eye into one interlaced image.
   A 3D-ready monitor is required. We support Row, Column and Checkerboard Interleaved.
   An option to Swap Left/Right helps to adjust the image for the screen. This method works better in fullscreen.
Time Sequential
   Render alternate eyes.
   This method is also known as Page Flip.
   This requires the graphic card to support Quad Buffer and it only works in fullscreen.
Side-by-Side
   Render images for left and right eye side-by-side.
   There is an option to support Cross-Eye glasses.
   It works only in fullscreen, and it should be used with the Full Editor operator.
Top-Bottom
   Render images for left and right eye one above another.
   It works only in fullscreen, and it should be used with the Full Editor operator.

.. note:: Full Screen Stereo 3D Modes

   If you have a 3D display most of the time,
   you will use it to see in stereo 3D, you will have to go to the fullscreen mode.
   In fact some modes will only work in the full window mode that hides most of
   the user interface from the work area.
   In this case it is recommended to work with two monitors,
   using the 3D screen for visualizing the stereo result
   while the other screen can be used for the regular Blender work.


Stereo 3D Camera
================

When using the Stereo 3D scene view setup, a stereo pair is created
on-the-fly and used for rendering and previsualization.
For all the purposes this works as two cameras that share most parameters (focal length, clipping, ...).
The stereo pair, however, is offset, and can have unique rotation and shift between itself.

.. figure:: /images/render_output_properties_stereoscopy_usage_camera-stereoscopy-panel.png

   Stereo 3D camera settings.

Interocular Distance
   Set the distance between the camera pair.
   Although the convergence of a stereo pair can be changed in post-production,
   different interocular distances will produce different results
   due to the parts of the scene being occluded from each point of view.
Convergence Plane Distance
   The converge point for the stereo cameras.
   This is often the distance between a projector and the projection screen.
   You can visualize this in the 3D Viewport.

.. (TODO) Spherical Stereo
   https://en.blender.org/index.php/Dev:Ref/Release_Notes/2.78/Cycles

   (here tooltips copy)

Spherical Stereo
   Render every pixel rotating the camera around the middle of the interocular distance.
Use Pole Merge
   Fade interocular distance to 0 after the given cutoff angle.

   Pole Merge Start Angle
      Angle at which interocular distance starts to fade to 0.
   Pole Merge End Angle
      Angle at which interocular distance is 0.


Convergence Mode
----------------

Off-Axis
   The stereo camera pair is separated by the interocular distance,
   and shifted inwards so it converges in the convergence plane.
   This is the ideal format since it is the one closest to how the human vision works.
Parallel
   This method produces two parallel cameras that do not converge.
   Since this method needs to be manually converged it cannot be used for viewing.
   This method is common when combining real footage with rendered elements.
Toe-in
   A less common approach is to rotate the cameras instead of shifting their frustum.
   The Toe-in method is rarely used in modern 3D productions.
Pivot
   The stereo pair can be constructed around the active camera with a new camera built for each eye
   (Center Pivot) or using the existing camera and creating (Left or Right).
   The latter is what is used when only one eye needs to be rendered for an existing mono 2D project.


Viewport Stereo 3D
==================

When you enable *Views* in the Render Layer panel, a new area is available in the 3D Viewport Sidebar region.
In this panel you can pick whether to see the stereo 3D in the viewport, or which camera to see.
It also allow you to see the *Cameras*, the *Plane* and the *Volume* of the stereo cameras.

.. figure:: /images/render_output_properties_stereoscopy_usage_3d-view-stereoscopy-panel.png

   Viewport stereo 3D settings.

Cameras
   When working with the Stereo 3D Viewports setup, you can inspect what
   each individual generated camera is looking or the combined result of them.
   In the Multi-View mode you can see the combined result of the left and right cameras
   (when available) or the current selected camera.
Plane
   The convergence plane represents the screen as it is perceived by the audience.
   Visualizing it in the 3D Viewport allows you to layout your scene
   based on your depth script outside the camera view.
Volume
   The intersection of the stereo cameras frustums helps planning the show
   by avoiding elements being visible by only one camera.
   The volume is defined by the camera's start and end clipping distances.
   The areas that are in the frustum of one camera only are known as *retinal rivalry areas*.
   They are tolerated in the negative space (the region from the convergence plane into the image)
   but are to be avoided at all costs in the positive space (the area from the convergence plane to the camera).

   .. figure:: /images/render_output_properties_stereoscopy_usage_volume.png
      :width: 402px

      Viewport 3D: convergence plane and volume display.


Multi-View and Stereo 3D Image I/O
==================================

Multi-View and Stereo 3D
   Multi-view images can be saved in special formats according to the production requirements.
   By default the system saves each view as an individual file,
   thus generating as many files as views to be rendered.
   In stereo 3D productions, for the final deployment or
   even intermediary previews it is convenient to save stereo 3D images,
   that are ready to use with 3D displays or simple anaglyph glasses.
   The formats supported match the display modes available for the window.
Lossy-Formats
   Some stereo 3D formats represent a considerable loss of data.
   For example, the Anaglyph format will cap out entire color channels from the original image.
   The Top-Bottom compressed will discard half of your vertical resolution data.
   The Interlace will mash your data considerably.
   Once you export in those formats, you can still import the image
   back in Blender, for it to be treated as Stereo 3D.
   You will need to match the window stereo 3D display mode to the image stereo 3D format though.
Lossless Formats
   Some formats will preserve the original data,
   leading to no problems on exporting and importing the files back in Blender.
   The Individual option will produce separate images that
   (if saved in a lossless encoding such as ``PNG`` or ``OpenEXR``)
   can be loaded back in production with no loss of data.
   For the Stereo 3D formats the only lossless options are
   *Top-Bottom* and *Side-by-Side* without the Squeezed Frame option.
Multi-View OpenEXR
   Another option is to use multi-view OpenEXR files. This format can save multiple views in
   a single file and is backward compatible with old OpenEXR viewers (you see only one view though).
   Multi-view native support is only available to OpenEXR.


Image Editor
============

View Menu
   After you render your scene with Stereo 3D you will be able to see
   the rendered result in the combined stereo 3D or to inspect the individual views.
   This works for Viewer nodes, render results or opened images.

   .. figure:: /images/render_output_properties_stereoscopy_usage_image-editor-header.png

      Stereo 3D and view menu.

Views Format
   When you drag and drop an image into the Image Editor, Blender will open it as an individual images at first.
   If your image was saved with one of the Stereo 3D formats, you can change how
   Blender should interpret the image by switching the mode to Stereo 3D,
   turning on *Use Multi-View* and picking the corresponding stereo method.

   .. figure:: /images/render_output_properties_stereoscopy_usage_image-editor-multi-view.png

      Views formats and stereo 3D.


Compositor
==========

The Compositor works smoothly with multi-view images.
The compositing of a view is completed before the remaining views start to be composited.
The pipeline is the same as the single-view workflow, with the difference that you can use images,
movies or image sequences in any of the supported multi-view formats.

.. figure:: /images/render_output_properties_stereoscopy_usage_compositor.png

   Compositor, backdrop and Split Viewer node.

The views to render are defined in the current scene views,
in a similar way as you define the composite output resolution in the current scene render panel,
regardless of the Image nodes resolutions or Render Layers from different scenes.

.. note:: Single-View Images

   If the image from an Image node does not have the view you are trying to render,
   the image will be treated as a single-view image.

Switch View Node
   If you need to treat the views separately, you can use
   the :doc:`Switch View node </compositing/types/utilities/switch_stereo_view>`
   to combine the views before an Output node.

.. tip:: Performance

   By default, when compositing and rendering from the user interface all views are rendered and then composited.
   During test iterations you can disable all but one view from the Scene Views panel,
   and re-enable it after you get the final look.


## Groups

.. _bpy.types.ShaderNodeGroup:
.. DO NOT EDIT FILE. This is simply a stub which copies everything from the link below.
.. include:: /compositing/types/groups.rst
   :start-after: .. --- copy below this line ---


## Index

.. _render-shaders-index:

################
  Shader Nodes
################

.. toctree::
   :maxdepth: 2

   introduction.rst
   input/index.rst
   output/index.rst
   shader/index.rst
   textures/index.rst
   color/index.rst
   vector/index.rst
   converter/index.rst
   groups.rst
   osl.rst


## Introduction

.. index:: Nodes; Shader Nodes

************
Introduction
************

Materials, lights and backgrounds are all defined using a network of shading nodes.
These nodes output values, vectors, colors and shaders.


Shaders
=======

An important concept to understand when building node setups is
that of the *shader socket*. The output of all surface and
volume shaders is a shader, describing lighting interaction at the surface or of the volume,
rather than the color of the surface.

There are a few types of shaders available as nodes:

:abbr:`BSDF (Bidirectional Scattering Distribution Function)` shader
   Describe light reflection, refraction and absorption at an object surface.
Emission shader
   Describe light emission at an object surface or in a volume.
Volume shader
   Describe light scattering inside a volume.
Background shader
   Describe light emission from the environment.

Each shader node has a color input, and outputs a shader.
These can then be mixed and added together using Mix and Add Shader nodes.
No other operations are permitted.
The resulting output can then be used by the renderer to compute all light interactions,
for direct lighting or global illumination.

.. seealso::

   :doc:`Shaders </render/shader_nodes/shader/index>`


Textures
========

Blender has various built in procedural texture nodes,
with texture coordinates and various parameters as input, and a color or value as output.
No texture data-blocks are needed; instead node groups can be used for reusing texture setups.

For UV mapping and texture painting in the 3D Viewport, the Image Texture node must be used.
When setting such a node as active, it will be displayed in the 3D Viewport
while using :doc:`Texture color </render/workbench/color>` mode.
This method can be used to preview painted textures while texture painting.

The default texture coordinates for all nodes are Generated coordinates,
except for Image textures that use UV coordinates by default.
Each node includes some options to modify the texture mapping and resulting color,
and these can be edited in the texture properties.

.. seealso::

   :doc:`Textures </render/shader_nodes/textures/index>`


More
====

Nodes for geometric data, texture coordinates,
layering shaders and non-physically-based tricks can be found in:

- :doc:`Vector Nodes </render/shader_nodes/vector/index>`
- :doc:`Color Nodes </render/shader_nodes/color/index>`
- :doc:`Converter Nodes </render/shader_nodes/converter/index>`


Open Shading Language
=====================

In Cycles, custom nodes can be written using the Open Shading Language.

.. seealso::

   :doc:`Open Shading Language </render/shader_nodes/osl>`


## Osl


*********************
Open Shading Language
*********************

:guilabel:`Cycles Only`

It is also possible to create your own nodes using
`Open Shading Language <https://github.com/AcademySoftwareFoundation/OpenShadingLanguage>`__ (OSL).
These nodes will only work with the CPU and OptiX rendering backend.

To enable it, select *Open Shading Language* as the shading system in the render settings.

.. note::

   Some OSL features are not available when using the OptiX backend. Examples include:

   - Memory usage reductions offered by features like on-demand texture loading and
     mip-mapping are not available.
   - Texture lookups require OSL to be able to determine a constant image file path for each
     texture call.
   - Some noise functions are not available. Examples include *Cell*, *Simplex*, and *Gabor*.
   - The :ref:`trace <render-shader-nodes-osl-trace>` function is not functional.
     As a result of this, the :ref:`Ambient Occlusion <bpy.types.ShaderNodeAmbientOcclusion>`
     and :ref:`Bevel <bpy.types.ShaderNodeBevel>` nodes do not work.


.. _bpy.types.ShaderNodeScript:

Script Node
===========

.. figure:: /images/node-types_ShaderNodeScript.webp
   :align: right
   :alt: Script Node.

OSL was designed for node-based shading,
and *each* OSL shader corresponds to *one* node in a node setup.
To add an OSL shader, add a script node and link it to a text data-block or an external file.
Input and output sockets will be created from the shader parameters on
clicking the update button in the Node or the Text editor.

OSL shaders can be linked to the node in a few different ways. With the *Internal* mode,
a text data-block is used to store the OSL shader, and the OSO bytecode is stored in the node itself.
This is useful for distributing a blend-file with everything packed into it.

The *External* mode can be used to specify a ``.osl`` file from a drive,
and this will then be automatically compiled into a ``.oso`` file in the same directory.
It is also possible to specify a path to a ``.oso`` file, which will then be used directly,
with compilation done manually by the user. The third option is to specify just the module name,
which will be looked up in the shader search path.

The shader search path is located in the same place as the scripts or configuration path, under:

Linux
   .. parsed-literal:: $HOME/.config/blender/|BLENDER_VERSION|/shaders/
Windows
   .. parsed-literal:: C:\\Users\\$user\\AppData\\Roaming\\Blender Foundation\\Blender\\\ |BLENDER_VERSION|\\shaders\\
macOS
   .. parsed-literal:: /Users/$USER/Library/Application Support/Blender/|BLENDER_VERSION|/shaders/

.. tip::

   For use in production, we suggest to use a node group to wrap shader script nodes,
   and link that into other blend-files.
   This makes it easier to make changes to the node afterwards as sockets are added or removed,
   without having to update the script nodes in all files.


Writing Shaders
===============

For more details on how to write shaders, see the
`OSL Documentation <https://open-shading-language.readthedocs.io/en/latest/>`__.

Here is a simple example:

.. code-block:: cpp

   shader simple_material(
       color Diffuse_Color = color(0.6, 0.8, 0.6),
       float Noise_Factor = 0.5,
       output closure color BSDF = diffuse(N))
   {
       color material_color = Diffuse_Color * mix(1.0, noise(P * 10.0), Noise_Factor);
       BSDF = material_color * diffuse(N);
   }


Closures
========

OSL is different from, for example, RSL or GLSL, in that it does not have a light loop.
There is no access to lights in the scene,
and the material must be built from closures that are implemented in the renderer itself.
This is more limited, but also makes it possible for the renderer to do optimizations and
ensure all shaders can be importance sampled.

The available closures in Cycles correspond to the shader nodes and their sockets;
for more details on what they do and the meaning of the parameters,
see the :doc:`shader nodes manual </render/shader_nodes/shader/index>`.

.. seealso::

   Documentation on OSL's `built-in closures
   <https://open-shading-language.readthedocs.io/en/latest/stdlib.html#material-closures>`__.


BSDF
----

- ``diffuse(N)``
- ``oren_nayar(N, roughness)``
- ``diffuse_ramp(N, colors[8])``
- ``phong_ramp(N, exponent, colors[8])``
- ``diffuse_toon(N, size, smooth)``
- ``glossy_toon(N, size, smooth)``
- ``translucent(N)``
- ``reflection(N)``
- ``refraction(N, ior)``
- ``transparent()``
- ``microfacet_ggx(N, roughness)``
- ``microfacet_ggx_aniso(N, T, ax, ay)``
- ``microfacet_ggx_refraction(N, roughness, ior)``
- ``microfacet_beckmann(N, roughness)``
- ``microfacet_beckmann_aniso(N, T, ax, ay)``
- ``microfacet_beckmann_refraction(N, roughness, ior)``
- ``ashikhmin_shirley(N, T, ax, ay)``
- ``ashikhmin_velvet(N, roughness)``


Hair
----

- ``hair_reflection(N, roughnessu, roughnessv, T, offset)``
- ``hair_transmission(N, roughnessu, roughnessv, T, offset)``
- ``principled_hair(N, absorption, roughness, radial_roughness, coat, offset, IOR)``


BSSRDF
------

Used to simulate subsurface scattering.

.. function:: bssrdf(method, N, radius, albedo)

   :type method: string
   :arg method:
      Rendering method to simulate subsurface scattering.

      - ``burley``:
        An approximation to physically-based volume scattering.
        This method is less accurate than ``random_walk`` however,
        in some situations this method will resolve noise faster.
      - ``random_walk_skin``:
        Provides accurate results for thin and curved objects.
        Random Walk uses true volumetric scattering inside the mesh,
        which means that it works best for closed meshes.
        Overlapping faces and holes in the mesh can cause problems.
      - ``random_walk``:
        Behaves similarly to ``random_walk_skin`` but modulates
        the *Radius* based on the *Color*, *Anisotropy*, and *IOR*.
        This method thereby attempts to retain greater surface detail and color
        than ``random_walk_skin``.
   :type N: vector
   :arg N: Normal vector of the surface point being shaded.
   :type radius: vector
   :arg radius:
      Average distance that light scatters below the surface.
      Higher radius gives a softer appearance, as light bleeds into shadows and through the object.
      The scattering distance is specified separately for the RGB channels,
      to render materials such as skin where red light scatters deeper.
      The X, Y and Z values are mapped to the R, G and B values, respectively.
   :type albedo: color
   :arg albedo:
      Color of the surface, or physically speaking, the probability that light is reflected for each wavelength.


Volume
------

- ``henyey_greenstein(g)``
- ``absorption()``


Other
-----

- ``emission()``
- ``ambient_occlusion()``
- ``holdout()``
- ``background()``


Attributes
==========

Geometry attributes can be read through the ``getattribute()`` function.
This includes UV maps, color attributes and any attributes output from geometry nodes.

The following built-in attributes are available through ``getattribute()`` as well.

``geom:generated``
   Automatically generated texture coordinates, from undeformed mesh.
``geom:uv``
   Default render UV map.
``geom:tangent``
   Default tangent vector along surface, in object space.
``geom:undisplaced``
   Position before displacement, in object space.
``geom:dupli_generated``
   For instances, generated coordinate from instancer object.
``geom:dupli_uv``
   For instances, UV coordinate from instancer object.
``geom:trianglevertices``
   Three vertex coordinates of the triangle.
``geom:numpolyvertices``
   Number of vertices in the polygon (always returns three currently).
``geom:polyvertices``
   Vertex coordinates array of the polygon (always three vertices currently).
``geom:name``
   Name of the object.
``geom:is_smooth``
   Is mesh face smooth or flat shaded.
``geom:is_curve``
   Is object a curve or not.
``geom:curve_intercept``
   0..1 coordinate for point along the curve, from root to tip.
``geom:curve_thickness``
   Thickness of the curve in object space.
``geom:curve_length``
   Length of the curve in object space.
``geom:curve_tangent_normal``
   Tangent Normal of the strand.
``geom:is_point``
   Is point in a point cloud or not.
``geom:point_radius``
   Radius of point in point cloud.
``geom:point_position``
   Center position of point in point cloud.
``geom:point_random``
   Random number, different for every point in point cloud.
``path:ray_length``
   Ray distance since last hit.
``object:random``
   Random number, different for every object instance.
``object:index``
   Object unique instance index.
``object:location``
   Object location.
``material:index``
   Material unique index number.
``particle:index``
   Particle unique instance number.
``particle:age``
   Particle age in frames.
``particle:lifetime``
   Total lifespan of particle in frames.
``particle:location``
   Location of the particle.
``particle:size``
   Size of the particle.
``particle:velocity``
   Velocity of the particle.
``particle:angular_velocity``
   Angular velocity of the particle.


.. _render-shader-nodes-osl-trace:

Trace
=====

:guilabel:`CPU Only`

We support the ``trace(point pos, vector dir, ...)`` function,
to trace rays from the OSL shader. The "shade" parameter is not supported currently,
but attributes can be retrieved from the object that was hit using the
``getmessage("trace", ..)`` function. See the OSL specification for details on how to use this.

This function cannot be used instead of lighting;
the main purpose is to allow shaders to "probe" nearby geometry,
for example to apply a projected texture that can be blocked by geometry,
apply more "wear" to exposed geometry, or make other ambient occlusion-like effects.


Metadata
========

Metadata on parameters controls their display in the user interface. The following
metadata is supported:

``[[ string label = "My Label" ]]``
  Name of parameter in in the user interface
``[[ string widget = "null" ]]``
  Hide parameter in the user interface.
``[[ string widget = "boolean" ]]`` and ``[[ string widget = "checkbox" ]]``
  Display integer parameter as a boolean checkbox.


## Bright Contrast

.. _bpy.types.ShaderNodeBrightContrast:
.. DO NOT EDIT FILE. This is simply a stub which copies everything from the link below.
.. include:: /compositing/types/color/adjust/bright_contrast.rst
   :start-after: .. --- copy below this line ---


## Gamma

.. _bpy.types.ShaderNodeGamma:
.. DO NOT EDIT FILE. This is simply a stub which copies everything from the link below.
.. include:: /compositing/types/color/adjust/gamma.rst
   :start-after: .. --- copy below this line ---


## Hue Saturation

.. _bpy.types.ShaderNodeHueSaturation:
.. DO NOT EDIT FILE. This is simply a stub which copies everything from the link below.
.. include:: /compositing/types/color/adjust/hue_saturation.rst
   :start-after: .. --- copy below this line ---


## Index


#########
  Color
#########

.. toctree::
   :maxdepth: 1

   bright_contrast.rst
   gamma.rst
   hue_saturation.rst
   invert_color.rst
   light_falloff.rst
   mix.rst
   rgb_curves.rst


## Invert Color

.. _bpy.types.ShaderNodeInvert:
.. DO NOT EDIT FILE. This is simply a stub which copies everything from the link below.
.. include:: /compositing/types/color/invert_color.rst
   :start-after: .. --- copy below this line ---


## Light Falloff

.. _bpy.types.ShaderNodeLightFalloff:

******************
Light Falloff Node
******************

:guilabel:`Cycles Only`

.. figure:: /images/node-types_ShaderNodeLightFalloff.webp
   :align: right
   :alt: Light Falloff Node.

The *Light Falloff* node allows you to manipulate how light intensity decreases over distance.
In reality light will always fall off quadratically; however,
it can be useful to manipulate as a non-physically-based lighting trick.
Note that using Linear or Constant falloff may cause more light to be introduced with every global
illumination bounce, making the resulting image extremely bright if many bounces are used.


Inputs
======

Strength
   Light strength before applying falloff modification.
Smooth
   Smooth intensity of light near light sources. This can avoid harsh highlights,
   and reduce global illumination noise. 0.0 corresponds to no smoothing; higher values smooth more.
   The maximum light strength will be strength/smooth.


Properties
==========

This node has no properties.


Outputs
=======

Quadratic
   Quadratic light falloff; this will leave strength unmodified if smooth is 0.0 and corresponds to reality.
Linear
   Linear light falloff, giving a slower decrease in intensity over distance.
Constant
   Constant light falloff, where the distance to the light has no influence on its intensity.


Examples
========

Todo <2.8 add.


## Mix

.. _bpy.types.ShaderNodeMixRGB:
.. DO NOT EDIT FILE. This is simply a stub which copies everything from the link below.
.. include:: /compositing/types/color/mix/mix_color.rst
   :start-after: .. --- copy below this line ---


## Rgb Curves

.. _bpy.types.ShaderNodeRGBCurve:
.. DO NOT EDIT FILE. This is simply a stub which copies everything from the link below.
.. include:: /compositing/types/color/adjust/rgb_curves.rst
   :start-after: .. --- copy below this line ---


## Blackbody

.. _bpy.types.ShaderNodeBlackbody:
.. --- copy below this line ---

**************
Blackbody Node
**************

.. figure:: /images/node-types_ShaderNodeBlackbody.webp
   :align: right
   :alt: Blackbody Node.

The *Blackbody* node converts a blackbody temperature to RGB value.
This can be useful for materials that emit light at natural occurring frequencies.


Inputs
======

Temperature
   The temperature in Kelvin.


Properties
==========

This node has no properties.


Outputs
=======

Color
   RGB color output.


Examples
========

.. figure:: /images/render_shader-nodes_converter_blackbody_example.jpg

   Example of the color ranges of the Blackbody node.


## Clamp

.. _bpy.types.ShaderNodeClamp:
.. Editor's Note: This page gets copied into:
.. - :doc:`</modeling/nodes/utilities/clamp>`

.. --- copy below this line ---

**********
Clamp Node
**********

.. figure:: /images/node-types_ShaderNodeClamp.webp
   :align: right
   :alt: Clamp Node.

The *Clamp* node clamps a value between a minimum and a maximum.


Inputs
======

Value
   The input value to be clamped.
Min
   The minimum value.
Max
   The maximum value.


Properties
==========

Clamp Type
   Method to clamp.

   :Min Max:
      Constrain values between Min and Max.
   :Range:
      Constrain values between Min and Max. When Min is greater than Max,
      constrain between Max and Min instead.


Outputs
=======

Result
   The input value after clamping.


Examples
========

The *Voronoi Texture* node outputs a value whose minimum is zero.
We can use the *Clamp* node to clamp this value such that the minimum is 0.2.

.. figure:: /images/render_shader-nodes_converter_clamp_example.jpg

   Example of Clamp node.


## Color Ramp

.. _bpy.types.ShaderNodeValToRGB:
.. DO NOT EDIT FILE. This is simply a stub which copies everything from the link below (for Cycles).
.. include:: /compositing/types/color/color_ramp.rst
   :start-after: .. --- copy below this line ---


## Combine Color

.. _bpy.types.ShaderNodeCombineColor:
.. DO NOT EDIT FILE. This is simply a stub which copies everything from the link below.
.. include:: /modeling/geometry_nodes/utilities/color/combine_color.rst


## Combine Xyz

.. _bpy.types.ShaderNodeCombineXYZ:
.. index:: Shader Nodes; Combine XYZ

****************
Combine XYZ Node
****************

.. figure:: /images/compositing_node-types_CompositorNodeCombineXYZ.webp
   :align: right
   :alt: Combine XYZ Node.

The *Combine XYZ Node* combines a vector from its individual components.


Inputs
======

- X
- Y
- Z


Properties
==========

This node has no properties.


Output
======

Vector
   Standard vector output.

.. note::

   The vector is not normalized.


## Float Curve

.. _bpy.types.ShaderNodeFloatCurve:
.. --- copy below this line ---

***********
Float Curve
***********

.. figure:: /images/node-types_ShaderNodeFloatCurve.webp
   :align: right
   :alt: Float Curve node.

The Float Curve node maps an input float to a curve and outputs a float value.


Inputs
======

Factor
   Controls the amount of influence the node exerts on the output value.
Value
   Standard float input.


Properties
==========

Curve
   For the curve controls see: :ref:`Curve widget <ui-curve-widget>`.


Outputs
=======

Float
   Standard float output.


## Index


#############
  Converter
#############

.. toctree::
   :maxdepth: 1

   blackbody.rst
   clamp.rst
   color_ramp.rst
   combine_color.rst
   combine_xyz.rst
   float_curve.rst
   map_range.rst
   math.rst
   mix.rst
   rgb_to_bw.rst
   separate_color.rst
   separate_xyz.rst
   shader_to_rgb.rst
   vector_math.rst
   wavelength.rst


## Map Range

.. _bpy.types.ShaderNodeMapRange:
.. Editor's Note: This page gets copied into:
.. - :doc:`</modeling/geometry_nodes/utilities/math/map_range>`

.. --- copy below this line ---

**************
Map Range Node
**************

.. figure:: /images/node-types_ShaderNodeMapRange.webp
   :align: right
   :alt: Map Range Node.

The *Map Range* node remaps a value from a range to a target range.


Inputs
======

Value/Vector
   The input value or vector to be remapped.
From Min
   The lower bound of the range to remap from.
From Max
   The higher bound of the range to remap from.
To Min
   The lower bound of the target range.
To Max
   The higher bound of the target range.
Steps
   The number of values allowed between *To Min* and *To Max* when using *Stepped Linear* interpolation.
   A higher value will give a smoother interpolation while lower values will progressively quantize the input.


Properties
==========

Data Type
   Map Range supports both Float and Vector data types. Changing the data type will
   also update the sockets to reflect the data type chosen.

Interpolation Type
   The mathematical method used to transition between gaps in the numerical inputs.

   :Linear: Linear interpolation between From Min and From Max values.
   :Stepped Linear: Stepped linear interpolation between From Min and From Max values.
   :Smooth Step: Smooth Hermite edge interpolation between From Min and From Max values.
   :Smoother Step: Smoother Hermite edge interpolation between From Min and From Max values.

Clamp
   If enabled, the output is clamped to the target range.


Outputs
=======

Result/Vector
   The input value after remapping.


Examples
========

The *Noise Texture* node outputs a value in the range [0, 1].
We can use the *Map Range* node to remap this value into the range [-1, 1].

.. figure:: /images/render_shader-nodes_converter_map-range_example.jpg

   Example of Map Range node.


## Math

.. _bpy.types.ShaderNodeMath:
.. DO NOT EDIT FILE. This is simply a stub which copies everything from the link below.
.. include:: /compositing/types/utilities/math.rst
   :start-after: .. --- copy below this line ---
   :end-before: Examples


## Mix

.. _bpy.types.ShaderNodeMix:
.. Editor's Note: This page gets copied into:
.. - :doc:`</modeling/geometry_nodes/utilities/mix>`

.. --- copy below this line ---

********
Mix Node
********

The *Mix Node* mixes values, colors and vectors inputs
using a factor to control the amount of interpolation.
The *Color* mode has additional blending modes.

.. figure:: /images/render_shader-nodes_shader_mix_node.jpg
   :align: right
   :alt: Mix Node.

.. important::

   This is a new node as of Blender 3.4. The previous *Mix RGB* node
   is automatically converted to the Color version of this node in
   Shader and Geometry node editors.
   Files saved with the new node are not forward compatible.

Inputs
======

Factor
   Controls the amount of mixing between the A and B inputs.
A/B
   The two inputs that are mixed together.


Properties
==========

Data Type
   The data type that is used for mixing.
   The node supports float, vector, color, and rotation data types.

Factor Mode (Vector only)
   The factor mode can be set to *Uniform* and *Non-Uniform*.
   In uniform mode, a single float controls the factor.
   In non-uniform mode, a vector controls the factor for
   each XYZ channel separately.

Mix (Color only)
   The Blend modes can be selected in the select menu.
   See :term:`Color Blend Modes` for details on each blending mode.

   Add, Subtract, Multiply, Screen, Divide, Difference,
   Darken, Lighten, Overlay, Color Dodge, Color Burn,
   Hue, Saturation, Value, Color, Soft Light, Linear Light

Clamp Factor
   Limit the factor value between 0.0 and 1.0. If this option is
   unchecked then the node operates using *Extrapolation*.

Clamp Result (Color only)
   Limit the Result to the range between 0.0 and 1.0.

Outputs
=======

Result
   Output the result of the mix using the data type selected.


Examples
========

See the Color > Mix page for additional examples:
:doc:`Mix Color Node </render/shader_nodes/color/mix>`



## Rgb To Bw

.. _bpy.types.ShaderNodeRGBToBW:
.. DO NOT EDIT FILE. This is simply a stub which copies everything from the link below.
.. include:: /compositing/types/color/rgb_to_bw.rst
   :start-after: .. --- copy below this line ---


## Separate Color

.. _bpy.types.ShaderNodeSeparateColor:
.. DO NOT EDIT FILE. This is simply a stub which copies everything from the link below.
.. include:: /modeling/geometry_nodes/utilities/color/separate_color.rst
   :start-after: .. --- copy below this line ---


## Separate Xyz

.. _bpy.types.ShaderNodeSeparateXYZ:
.. index:: Shader Nodes; Separate XYZ

*****************
Separate XYZ Node
*****************

.. figure:: /images/compositing_node-types_CompositorNodeSeparateXYZ.webp
   :align: right
   :alt: Separate XYZ Node.

The *Separate XYZ Node* splits a vector into its individual components.


Input
=====

Vector
   Standard vector input.


Properties
==========

This node has no properties.


Outputs
=======

- X
- Y
- Z


## Shader To Rgb

.. _bpy.types.ShaderNodeShaderToRGB:

*************
Shader To RGB
*************

.. figure:: /images/node-types_ShaderNodeShaderToRGB.webp
   :align: right
   :alt: Shader to RGB.

:guilabel:`EEVEE Only`

The Shader to RGB node is typically used for non-photorealistic rendering,
to apply additional effects on the output of BSDFs.
For example, a color ramp on the output of a diffuse BSDF can be used to create a flexible toon shader.

Using this conversion breaks the :abbr:`PBR (Physically Based Rendering)` pipeline and
thus makes the result unpredictable when used in combination with effects such as
ambient occlusion, contact shadows, soft shadows and screen space refraction.

Some effects require multiple samples to converge, and applying arbitrary changes to
noisy input may not convert to a smooth result.

.. warning::

   If a Shader to RGB node is used, any upstream BSDF will be invisible to the following effects:

   - Screen Space Reflection
   - Subsurface Scattering
   - Alpha Clip and Alpha Hashed blend modes

   Shader to RGB node doesn't give expected results in render passes.


Inputs
======

Shader
   Any shader such as a BSDF or Emission node can be linked here.


Properties
==========

This node has no properties.


Outputs
=======

Color
   Surface color computed from BSDFs and lighting.
Alpha
   Alpha transparency from any Transparent BSDFs in the input.


Examples
========

.. figure:: /images/render_shader-nodes_converter_shader-to-rgb_example.jpg

   Simple toon shading with Shader to RGB and Freestyle.


## Vector Math

.. _bpy.types.ShaderNodeVectorMath:
.. Editor's Note: This page gets copied into:
.. - :doc:`</modeling/nodes/vector/vector_math>`

.. --- copy below this line ---

****************
Vector Math Node
****************

.. figure:: /images/node-types_ShaderNodeVectorMath.webp
   :align: right
   :alt: Vector Math Node.

The *Vector Math* node performs the selected math operation on the input vectors.


Inputs
======

The inputs of the node are dynamic. Some inputs are only available in certain operations.
For instance, the *Scale* input is only available in the *Scale* operator.

Vector
   Input vector :math:`A = \begin{pmatrix} A_x \\ A_y \\ A_z \end{pmatrix}`.
Vector
   Input vector :math:`B = \begin{pmatrix} B_x \\ B_y \\ B_z \end{pmatrix}`.
Scale
   Input Scale :math:`s`.


Properties
==========

Operation
   The vector math operator to be applied on the input vectors.

   :Add: The sum of A and B.
      :math:`\begin{pmatrix} A_x + B_x \\ A_y + B_y \\ A_z + B_z \end{pmatrix}`
   :Subtract: The difference between A and B.
      :math:`\begin{pmatrix} A_x - B_x \\ A_y - B_y \\ A_z - B_z \end{pmatrix}`
   :Multiply: The entrywise product of A and B.
      :math:`\begin{pmatrix} A_x \cdot B_x \\ A_y \cdot B_y \\ A_z \cdot B_z \end{pmatrix}`
   :Divide: The entrywise division of A by B. Division by zero results in zero.
      :math:`\begin{pmatrix} A_x / B_x \\ A_y / B_y \\ A_z / B_z \end{pmatrix}`
   :Multiply Add:
      The entrywise combination of the multiply and addition operations.
      :math:`A × B + C`
   :Cross Product: The cross product of A and B.
      :math:`\begin{pmatrix} A_y \cdot B_z - A_z \cdot B_y \\ A_z \cdot B_x - A_x \cdot B_z
      \\ A_x \cdot B_y - A_y \cdot B_x \end{pmatrix}`
   :Project: The projection of A onto B.
   :Reflect: The reflection of A around the normal B. B need not be normalized.
   :Refract:
      For a given incident vector A, surface normal B and ratio of indices of refraction (IOR),
      refract outputs the refraction vector R.
   :Faceforward: Orients a vector A to point away from a surface B as defined by its normal C.
      Computes :math:`(dot(B, C) < 0) ? A : -A`.
   :Dot Product: The dot product of A and B.
      :math:`A_x \cdot B_x + A_y \cdot B_y + A_z \cdot B_z`
   :Distance: The distance between A and B.
   :Length: The length of A.
      :math:`\sqrt{A_x^2 + A_y^2 + A_z^2}`
   :Scale: The result of multiplying A by the scalar input *Scale*.
      :math:`\begin{pmatrix} s \cdot A_x \\ s \cdot A_y \\ s \cdot A_z \end{pmatrix}`
   :Normalize: The result of normalizing A. The result vector points to the same direction as A and
      has a length of 1. If A is (0, 0, 0), the result is (0, 0, 0) as well.
   :Wrap:
      The entrywise output of a value between Min and Max based on the absolute difference
      between the input value and the nearest integer multiple of Max less than the value.
   :Snap: The result of rounding A to the largest integer multiple of B less than or equal A.
   :Floor: Rounds the input value entrywise down to the nearest integer.
   :Ceil: Rounds the input value entrywise up to the nearest integer.
   :Modulo: The entrywise modulo of A by B.
   :Fraction: Returns the fractional part of the *value* entrywise.
   :Absolute: The entrywise absolute value of A.
   :Minimum: The entrywise minimum value from A and B.
   :Maximum: The entrywise maximum value from A and B.
   :Sine: The entrywise `Sine <https://en.wikipedia.org/wiki/Sine>`__ of A.
   :Cosine: The entrywise `Cosine <https://en.wikipedia.org/wiki/Trigonometric_functions>`__ of A.
   :Tangent: The entrywise `Tangent <https://en.wikipedia.org/wiki/Trigonometric_functions>`__ of A.


Outputs
=======

The output of the node is dynamic. It is either a vector or a scalar depending on the operator.
For instance, the *Length* operator has a scalar output while the *Add* operator has a vector output.

Vector
   Output vector.
Value
   Output value.


## Wavelength

.. _bpy.types.ShaderNodeWavelength:

***************
Wavelength Node
***************

.. figure:: /images/node-types_ShaderNodeWavelength.webp
   :align: right
   :alt: Wavelength Node.

The *Wavelength* node converts a wavelength value to an RGB value.
This can be used to achieve a specific color on the light spectrum.


Inputs
======

Wavelength
   The color wavelength from 380 to 780 nanometers.


Properties
==========

This node has no properties.


Outputs
=======

Color
   RGB color output.


Examples
========

.. figure:: /images/render_shader-nodes_converter_wavelength_example.jpg

   Example of Wavelength node.


## Ao

.. _bpy.types.ShaderNodeAmbientOcclusion:

*****************
Ambient Occlusion
*****************

.. figure:: /images/node-types_ShaderNodeAmbientOcclusion.webp
   :align: right
   :alt: Ambient Occlusion node.

The *Ambient Occlusion* shader computes how much the hemisphere above the shading point is occluded.
This can be used for procedural texturing, for example to add weathering effects to corners only.

For Cycles, this is an expensive shader and can slow down render significantly.
If render time is a concern, using Pointiness from the Geometry node or baking Ambient Occlusion will result
in faster renders.

.. note::

   :guilabel:`Cycles Only`
    The Ambient Occlusion node will not produce a valid result when:
     - The object is either a :ref:`Caustic caster <bpy.types.CyclesObjectSettings.is_caustics_caster>`
       or :ref:`Caustic receiver <bpy.types.CyclesObjectSettings.is_caustics_receiver>` while the scene
       contains an active :ref:`Caustic caster <bpy.types.CyclesObjectSettings.is_caustics_caster>`,
       :ref:`Caustic receiver <bpy.types.CyclesObjectSettings.is_caustics_receiver>`, and
       :ref:`Shadow Caustic Light <bpy.types.CyclesLightSettings.is_caustics_light>`.
     - :doc:`/render/shader_nodes/osl` is active while using the OptiX rendering backend.

Inputs
======

Color
   Tint for AO output color.
Distance
   Distance up to which other objects are considered to occlude the shading point.
Normal
   Normal used for ambient occlusion; if nothing is connected the default shading normal is used.


Properties
==========

Samples
   Number of samples to use for ray-traced ambient occlusion sampling.
   Keep as low as possible for optimal performance.
Inside
   Detect convex rather than concave shapes, by computing occlusion inside mesh.
Only Local :guilabel:`Cycles Only`
   Only detect occlusion from the object itself, and not others.


Outputs
=======

Color
   Ambient occlusion with color tint.
AO
   Ambient occlusion factor without color tint.


Example
=======

.. figure:: /images/render_shader-nodes_input_ao_example.jpg

   White AO shader.


## Attribute

.. _bpy.types.ShaderNodeAttribute:

**************
Attribute Node
**************

.. figure:: /images/node-types_ShaderNodeAttribute.webp
   :align: right
   :alt: Attribute Node.

The *Attribute* node allows you to retrieve attributes attached to an object or mesh.


Inputs
======

This node has no inputs.


Properties
==========

Name
   Name of the attribute.

Type
   Specifies the type of the attribute.

   :Geometry:
      The attribute is associated with the geometry of the object, and its value varies from
      vertex to vertex, or within the volume of the object.

      Most geometry attributes are directly accessible through the various input nodes, except for these:

      Ocean Foam
         Gives a scalar defining where foam might appear when using
         an :doc:`Ocean Modifier </modeling/modifiers/physics/ocean>`.
         This depends on the name you give this property.

      .. seealso::

         For a full list of options see `This Thread <https://blender.stackexchange.com/questions/14262#14267>`__
         on the Blender Stack Exchange.
   :Object:
      The attribute name specifies a :ref:`custom property <files-data_blocks-custom-properties>` name,
      or an RNA path to a built-in property (like the single property :ref:`driver variables <drivers-variables>`).

      The values of attributes of this type are defined once per object. The name or path is looked up
      first in the object data-block, followed by the mesh data-block if not found.
      Custom properties have priority over built-in ones.

      The property value must be an integer, float, boolean, or a vector of 1 to 4 floats or ints; properties of other
      types are ignored. If a suitable property is not found, all sockets of the node, including *Alpha*, output 0.

      .. tip::

         The ``color`` attribute will output the value of the Color field in
         the :ref:`Viewport Display <properties-object-viewport-display>` panel of
         the object, unless overridden by a custom property.
   :Instancer:
      Similar to *Object*, but the attribute is looked up in the instancer particle system settings,
      followed by :doc:`Geometry Node instance </modeling/geometry_nodes/instances>` attributes
      (searching from the innermost instancing layer to outer ones), and finally in the instancer object.
      If the current object is not instanced, or the property is not found, it falls back to the *Object* mode.

      .. warning::

         Currently only up to 4 layers of Geometry Node instancing are searched.
   :View Layer:
      The attribute is looked up in the current :doc:`View Layer </scene_layout/view_layers/introduction>`,
      :doc:`Scene </scene_layout/scene/introduction>` and :doc:`World </render/lights/world>`, using the same lookup
      logic as *Object*, and likewise producing all zero outputs including *Alpha* if not found. Attributes of this
      type have the same uniform value throughout the whole Render Layer.

      .. tip::

         This gives access to a number of useful built-in properties, for example:

         ``color`` or ``world.color``
            Outputs the value of the :ref:`Color <bpy.types.World.color>` field in the Viewport Display
            panel of the World properties.
         ``render.resolution_x``, ``render.resolution_y``
            Outputs the current :doc:`rendering resolution </render/output/properties/format>`.
         ``camera.data.angle_x``, ``camera.data.angle_y``,
            Outputs the effective field of view of the active :doc:`Camera </render/cameras>`.

      .. seealso::

         An alternative method to access the same set of properties is to use driver
         :ref:`Context Properties <bpy.types.DriverVariable.type.CONTEXT_PROP>`,
         possibly with a :ref:`manually emulated <driver-attribute-node-emulation>` lookup fallback chain.


Outputs
=======

Color
   RGB color interpolated from the attribute.
Vector
   XYZ vector interpolated from the attribute.
Factor
   Scalar value interpolated from the attribute.
Alpha
   Alpha channel of the attribute, when available. If the attribute has no alpha channel, generally defaults to 1.

.. warning::

   Currently, only *View Layer* attributes are supported in shaders used for the :doc:`World </render/lights/world>`
   or :doc:`Light Objects </render/lights/light_object>`.


## Bevel

.. _bpy.types.ShaderNodeBevel:

**********
Bevel Node
**********

:guilabel:`Cycles Only`

.. figure:: /images/node-types_ShaderNodeBevel.webp
   :align: right
   :alt: Bevel Node.

The Bevel shader node can be used for rendering rounded corners.
Like bump mapping, this does not modify the actual geometry, only the shading is affected.
Slight rounding on edges helps to capture specular highlights that you would also see in the real world.

Note that this is a very expensive shader, and may slow down renders
by 20% even if there is a lot of other complexity in the scene.
For that reason, we suggest to mainly use this for baking or
still frame renders where render time is not as much of an issue.
The :doc:`/modeling/modifiers/generate/bevel` is a faster option when it works,
but sometimes fails on complex or messy geometry.

.. note::

   **The Bevel node will not produce a valid result when:**

   - The object is either a :ref:`Caustic caster <bpy.types.CyclesObjectSettings.is_caustics_caster>`
      or :ref:`Caustic receiver <bpy.types.CyclesObjectSettings.is_caustics_receiver>` while the scene
      contains an active :ref:`Caustic caster <bpy.types.CyclesObjectSettings.is_caustics_caster>`,
      :ref:`Caustic receiver <bpy.types.CyclesObjectSettings.is_caustics_receiver>`, and
      :ref:`Shadow Caustic Light <bpy.types.CyclesLightSettings.is_caustics_light>`.
   - :doc:`/render/shader_nodes/osl` is active while using the OptiX rendering backend.


Inputs
======

Radius
   Width of the bevel effect on edges.
Normal
   Normal to apply bevel on top of, to be combined with a :doc:`/render/shader_nodes/vector/bump`
   for example. When not connected, uses the surface normal.


Properties
==========

Samples
   Number of samples to take for each shader evaluation.
   More samples give more accurate results, but are also slower to render.
   The default value of 4 works well for most cases, with any noise resolved by using more AA samples.


Outputs
=======

Normal
   Standard normal output.


Examples
========

.. figure:: /images/render_shader-nodes_input_bevel_example_setup.jpg

   A minimal node setup for working with the Bevel node.

.. figure:: /images/render_shader-nodes_input_bevel_example.jpg

   Bevel shader bringing out specular highlights on the edges.


## Camera Data

.. _bpy.types.ShaderNodeCameraData:

****************
Camera Data Node
****************

.. figure:: /images/node-types_ShaderNodeCameraData.webp
   :align: right
   :alt: Camera Data Node.

The *Camera Data* node returns information about the shading point relative to the camera.
This could be used for example to change the shading
of objects further away from the camera, or make custom fog effects.


Inputs
======

This node has no inputs.


Properties
==========

This node has no properties.


Outputs
=======

View Vector
   A normalized vector, in camera space, from the camera to the shading point.
View Z Depth
   The distance each pixel is away from the camera.
View Distance
   Distance from the camera to the shading point.


## Fresnel

.. _bpy.types.ShaderNodeFresnel:

************
Fresnel Node
************

.. figure:: /images/node-types_ShaderNodeFresnel.webp
   :align: right
   :alt: Fresnel Node.

The *Fresnel* or *Dielectric Fresnel* node computes how much light is reflected off a layer,
where the rest will be refracted through the layer.
The resulting weight can be used for layering shaders with the *Mix Shader* node.
It is dependent on the angle between the surface normal and the viewing direction.

The most common use is to mix between two BSDFs using it as a blending factor in a Mix Shader node.
For a simple glass material you would mix between a glossy refraction and glossy reflection.
At grazing angles more light will be reflected than refracted as happens in reality.

For a two-layered material with a diffuse base and a glossy coating,
you can use the same setup, mixing between a diffuse and glossy BSDF. By using the Fresnel as
the blending factor you are specifying that any light which is refracted through the glossy
coating layer would hit the diffuse base and be reflected off that.


Inputs
======

IOR
   Index of refraction (:term:`IOR`) of the material being entered.
Normal
   Input meant for plugging in bump or normal maps which will affect the output.


Properties
==========

This node has no properties.


Outputs
=======

Factor
   Fresnel weight, indicating the probability with which light
   will reflect off the layer rather than passing through.


## Geometry

.. _bpy.types.ShaderNodeNewGeometry:

*************
Geometry Node
*************

.. figure:: /images/node-types_ShaderNodeNewGeometry.webp
   :align: right
   :alt: Geometry Node.

The *Geometry* node gives geometric information about the current shading point.
All vector coordinates are in *World Space*. For volume shaders,
only the position and incoming vector are available.


Inputs
======

This node has no inputs.


Properties
==========

This node has no properties.


Outputs
=======

Position
   Position of the shading point.
Normal
   Shading normal at the surface (includes smooth normals and bump mapping).
Tangent
   Tangent at the surface.
True Normal
   Geometry or flat normal of the surface.
Incoming
   Vector pointing towards the point the shading point is being viewed from.
Parametric
   Parametric coordinates of the shading point on the surface.
   To area lights it outputs its UV coordinates in planar mapping and
   in spherical coordinates to point lights.
Backfacing
   1.0 if the face is being viewed from the back side, 0.0 for the front side.
Pointiness :guilabel:`Cycles Only`
   An approximation of the curvature of the mesh per vertex.
   Lighter values indicate convex angles, darker values indicate concave angles.
   It allows you to do effects like dirt maps and wear-off effects.
Random per Island :guilabel:`Cycles Only`
   A random value for each connected component (island) of the mesh.
   It is useful to add variations to meshes composed of separated units
   like tree leaves, wood planks, or curves of multiple splines.

   .. figure:: /images/render_shader-nodes_input_geometry_example-random-per-island.png

      Get a random value for each instance of the mesh when using an Array modifier.


## Hair Info

.. _bpy.types.ShaderNodeHairInfo:

****************
Curves Info Node
****************

.. figure:: /images/node-types_ShaderNodeHairInfo.webp
   :align: right
   :alt: Curves Info Node.

The *Curves Info* node gives access to :doc:`Hair </physics/particles/hair/index>` information.


Inputs
======

This node has no inputs.


Properties
==========

This node has no properties.


Outputs
=======

Is Strand
   Outputs 1 when the shader is acting on a strand, otherwise 0.
Intercept
   The point along the strand where the ray hits the strand (1 at the tip and 0 at the root).
Length
   The total measurement from the root to the tip of the strand,
   interpreted as a grayscale value from 0 to infinity.
Thickness
   The thickness of the strand at the point where the ray hits the strand.
Tangent Normal
   Tangent normal of the strand.
Random
   A random per-curve value in the range from 0 to 1.
   It can for example be used in combination with a color ramp, to randomize the curve's color.


## Index


#########
  Input
#########

.. toctree::
   :maxdepth: 1

   ao.rst
   attribute.rst
   bevel.rst
   camera_data.rst
   fresnel.rst
   geometry.rst
   hair_info.rst
   layer_weight.rst
   light_path.rst
   object_info.rst
   particle_info.rst
   point_info.rst
   rgb.rst
   tangent.rst
   texture_coordinate.rst
   uv_map.rst
   value.rst
   vertex_color.rst
   volume_info.rst
   wireframe.rst


## Layer Weight

.. _bpy.types.ShaderNodeLayerWeight:

*****************
Layer Weight Node
*****************

.. figure:: /images/node-types_ShaderNodeLayerWeight.webp
   :align: right
   :alt: Layer Weight Node.

The *Layer Weight* node outputs a weight typically used for layering shaders with the *Mix Shader* node.


Inputs
======

Blend
   Bias the output towards all 0 or all 1. Useful for uneven mixing of shaders.
Normal
   Input meant for plugging in bump or normal maps which will affect the output.


Properties
==========

This node has no properties.


Outputs
=======

Fresnel
   Dielectric Fresnel weight, useful for example for layering diffuse and
   glossy shaders to create a plastic material. This is like the Fresnel node,
   except that the input of this node is in the often more convenient 0.0 to 1.0 range.
Facing
   Weight that blends from the first to the second shader
   as the surface goes from facing the viewer to viewing it at a grazing angle.


## Light Path

.. _bpy.types.ShaderNodeLightPath:

***************
Light Path Node
***************

.. figure:: /images/node-types_ShaderNodeLightPath.webp
   :align: right
   :alt: Light Path Node.

The *Light Path* node is used to find out for which kind of incoming ray the shader is being executed;
particularly useful for non-physically-based tricks. More information about the meaning of each type
is in the :doc:`Light Paths </render/cycles/render_settings/light_paths>` documentation.


Inputs
======

This node has no inputs.


Properties
==========

This node has no properties.


Outputs
=======

Is Camera Ray
   1.0 if shading is executed for a camera ray, otherwise 0.0.
Is Shadow Ray
   1.0 if shading is executed for a shadow ray, otherwise 0.0.
Is Diffuse Ray
   1.0 if shading is executed for a diffuse ray, otherwise 0.0.
Is Glossy Ray
   1.0 if shading is executed for a glossy ray, otherwise 0.0.
Is Singular Ray :guilabel:`Cycles Only`
   1.0 if shading is executed for a singular ray, otherwise 0.0.
Is Reflection Ray :guilabel:`Cycles Only`
   1.0 if shading is executed for a reflection ray, otherwise 0.0.
Is Transmission Ray :guilabel:`Cycles Only`
   1.0 if shading is executed for a transmission ray, otherwise 0.0.
Ray Length :guilabel:`Cycles Only`
   Distance traveled by the light ray from the last bounce or camera.
Ray Depth
   Number of times the ray has been reflected or transmitted on interaction with a surface.

   .. note::

      Passing through a transparent shader
      :ref:`does not count as a normal "bounce" <render-cycles-light-paths-transparency>`.

Diffuse Depth :guilabel:`Cycles Only`
   Number of times the ray has gone through diffuse reflection or transmission.
Glossy Depth :guilabel:`Cycles Only`
   Number of times the ray has gone through glossy reflection or transmission.
Transparent Depth :guilabel:`Cycles Only`
   Returns the number of transparent surfaces passed through.
Transmission Depth :guilabel:`Cycles Only`
   Replace a Transmission light path after X bounces with another shader, e.g. a Diffuse one.
   This can be used to avoid black surfaces, due to low amount of max bounces.


EEVEE Support
=============

EEVEE has no real concept of rays. But in order to ease the workflow between Cycles and EEVEE
some of the outputs are only supported in particular cases.
This node makes it possible to tweak indirect lighting in the shader.

Only a subset of the outputs are supported and the ray depth does not exactly have the same meaning.

- *Is Camera*: Supported.
- *Is Shadow*: Supported.
- *Is Diffuse*: Supported.
- *Is Glossy*: Supported.
- *Is Singular*: Not supported. Same as Is Glossy.
- *Is Reflection*: Not supported. Same as Is Glossy.
- *Is Transmission*: Not supported. Same as Is Glossy.
- *Ray Length*: Not supported. Defaults to 1.0.
- *Ray Depth*: Indicates the current bounce when baking the light cache.
- *Diffuse Depth*: Same as Ray Depth but only when baking diffuse light.
- *Glossy Depth*: Same as Ray Depth but only when baking specular light.
- *Transparent Depth*: Not supported. Defaults to 0.
- *Transmission Depth*: Not supported. Same as Glossy Depth.

.. note::

   *Is Glossy* does not work with Screen Space Reflections/Refractions
   but does work with reflection planes (whether used with SSR or not).


## Object Info

.. _bpy.types.ShaderNodeObjectInfo:

****************
Object Info Node
****************

.. figure:: /images/node-types_ShaderNodeObjectInfo.webp
   :align: right
   :alt: Object Info Node.

The *Object Info* node gives information about the object instance.
This can be useful to give some variation to a single material assigned to multiple instances,
either manually controlled through the object index, based on the object location,
or randomized for each instance. For example a Noise texture can give random colors or a Color
Ramp can give a range of colors to be randomly picked from.


Inputs
======

This node has no inputs.


Properties
==========

This node has no properties.


Outputs
=======

Location
   Location of the object in world space.
Color
   Object color, same as *Color* in the :menuselection:`Properties --> Object Properties --> Viewport Display`.
Alpha
   The :term:`Alpha Channel` component of the object's viewport display color (see the *Color* output for
   more details).
Object Index
   Object pass index, same as *Pass Index*
   in the :menuselection:`Properties --> Object Properties --> Relations`.
Material Index
   Material pass index, same as *Pass Index*
   in the :menuselection:`Properties --> Material --> Settings`.
Random
   Random number unique to a single object instance. Output is a Float between 0.0 and 1.0

.. note::

   Note that this node only works for material shading nodes;
   it does nothing for light and world shading nodes.


Example
=======

.. figure:: /images/render_shader-nodes_input_object-info_example.png
   :width: 640px

.. figure:: /images/render_cycles_render-settings_motion-blur_example-cubes.jpg
   :width: 640px

   `Example blend-file <https://archive.blender.org/wiki/2015/uploads/0/03/Blender2.65_motion_blur.blend>`__.


## Particle Info

.. _bpy.types.ShaderNodeParticleInfo:

******************
Particle Info Node
******************

:guilabel:`Cycles Only`

.. figure:: /images/node-types_ShaderNodeParticleInfo.webp
   :align: right
   :alt: Particle Info Node.

The *Particle Info* node can be used in the material node tree for objects that are used as the instancing objects,
when you use *Object* or *Group* :doc:`Render mode </physics/particles/emitter/render>` of a particle system.

This node gives access to the data of the particle that spawned the object instance.
It can be useful to give some variation to a single material assigned to multiple instances of instancing object.

.. note::

   This node currently only supports parent particles. Info from child particles is not available.


Inputs
======

This node has no inputs.


Properties
==========

This node has no properties.


Outputs
=======

Index
   Index number of the particle (from 0 to number of particles).
Random
   A random per-particle value in the range from 0 to 1.
   It can for example be used in combination with a color ramp, to randomize the particle color.
Age
   Age of the particle in frames.
Lifetime
   Total lifespan of the particle in frames.
Location
   Location of the particle.
Size
   Size of the particle.
Velocity
   Velocity of the particle.
Angular Velocity
   Angular velocity of the particle.


## Point Info

.. _bpy.types.ShaderNodePointInfo:

**********
Point Info
**********

:guilabel:`Cycles Only`

.. figure:: /images/node-types_ShaderNodePointInfo.webp
   :align: right
   :alt: Point Info Node.

The *Point Info* node can be used in the material node tree for point cloud objects
and gives access to the data of individual points.
It can be useful to give some variation to a single material assigned a point cloud object.


Inputs
======

This node has no inputs.


Properties
==========

This node has no properties.


Outputs
=======

Location
   Location of the particle.
Radius
   Size of the particle.
Random
   A random per-point value in the range from 0 to 1.
   It can for example be used in combination with a color ramp, to randomize the point color.


## Rgb

.. _bpy.types.ShaderNodeRGB:
.. DO NOT EDIT FILE. This is simply a stub which copies everything from the link below.
.. include:: /compositing/types/input/constant/rgb.rst
   :start-after: .. --- copy below this line ---


## Tangent

.. _bpy.types.ShaderNodeTangent:

************
Tangent Node
************

.. figure:: /images/node-types_ShaderNodeTangent.webp
   :align: right
   :alt: Tangent Node.

The *Tangent* node generates a tangent direction for the Anisotropic BSDF.


Inputs
======

This node has no inputs.


Properties
==========

Direction Type
   The tangent direction can be derived from a cylindrical projection around the X,
   Y, or Z axis (radial), or from a manually created UV Map for full control.


Outputs
=======

Tangent
   The tangent direction vector.


## Texture Coordinate

.. _bpy.types.ShaderNodeTexCoord:

***********************
Texture Coordinate Node
***********************

.. figure:: /images/node-types_ShaderNodeTexCoord.webp
   :align: right
   :alt: Texture Coordinate Node.

The *Texture Coordinate* node is commonly used for the coordinates of textures,
typically used as inputs for the *Vector* input for texture nodes.


Inputs
======

This node has no inputs.


Properties
==========

Object
   Specific object to use for object space coordinates.
   This only affects the *Object* output.

.. _cycles-nodes-input-texture-coordinate-from-instancer:

From Instancer :guilabel:`Cycles Only`
   If the object is generated by instancing from vertices or faces, use texture coordinates from instancer.
   This only affects the *Generated* and *UV* outputs.

   .. figure:: /images/render_shader-nodes_input_texture-coordinate_from-dupli-comparison.png

      From left to right: Sphere with a UV-mapped texture.
      Small spheres instanced to the faces of the textured sphere using
      :doc:`instancing from faces </scene_layout/object/properties/instancing/faces>`.
      Small spheres with *From Instancer* enabled, using the UV map of the large sphere.

   .. note::

      *From Instancer* only works with the UV output when the object is instanced,
      either :doc:`from particles </physics/particles/introduction>` or
      :doc:`from faces </scene_layout/object/properties/instancing/faces>`.


Outputs
=======

Generated
   Automatically-generated texture coordinates from the vertex positions of the mesh without deformation,
   keeping them sticking to the surface under animation.
   Range from 0.0 to 1.0 over the bounding box of the undeformed mesh.
   See :ref:`Texture Spaces <properties-texture-space>` for more information.
Normal
   Object space normal, for texturing objects with the texture staying fixed on the object as it transformed.
   The Normal output can be used on Point and Spot lights. The coordinates will take
   the rotation of the light into account.
UV
   UV texture coordinates from the active render UV map.
   See :ref:`UV Mapping <editors-uv-index>` for more information.

   .. note::

      In order to select UV map other than the active map you must use
      the :doc:`UV Map node </render/shader_nodes/input/uv_map>`.
Object
   Uses an object as a source for coordinates. Often used with an empty,
   this is an easy way to place a small image at a given point on the object.
   This object can also be animated, to move a texture around or through a surface.
Camera
   Position coordinate in camera space.
Window
   Location of shading point on the screen, ranging from 0.0 to 1.0
   from the left to right side and bottom to top of the render.
   This is well suited for blending two objects.
Reflection
   Uses the direction of the reflection vector as coordinates.
   This is useful for adding reflection maps. You will need this input when using environment maps.


## Uv Map

.. _bpy.types.ShaderNodeUVMap:

***********
UV Map Node
***********

.. figure:: /images/node-types_ShaderNodeUVMap.webp
   :align: right
   :alt: UV Map Node.

The *UV Map* node is used to retrieve specific UV maps.
Unlike the :doc:`Texture Coordinate Node </render/shader_nodes/input/texture_coordinate>`
which only provides the active UV map,
this node can retrieve any UV map belonging to the object using the material.


Inputs
======

This node has no inputs.


Properties
==========

From Instancer :guilabel:`Cycles Only`
   See the :ref:`From Instancer <cycles-nodes-input-texture-coordinate-from-instancer>`
   option of the :doc:`Texture Coordinate Node </render/shader_nodes/input/texture_coordinate>`.

UV Map
   UV map to use.


Outputs
=======

UV
   UV mapping coordinates from the specified UV map.


## Value

.. _bpy.types.ShaderNodeValue:
.. DO NOT EDIT FILE. This is simply a stub which copies everything from the link below.
.. include:: /compositing/types/input/constant/value.rst
   :start-after: .. --- copy below this line ---


## Vertex Color

.. _bpy.types.ShaderNodeVertexColor:

********************
Color Attribute Node
********************

.. figure:: /images/node-types_ShaderNodeVertexColor.webp
   :align: right
   :alt: Color Attribute node.

The *Color Attribute* node provides access to Color Attributes as well as their alpha value.


Inputs
======

This node has no inputs.


Properties
==========

Color Attribute
   The target Color Attribute.
   The listed Color Attributes are those of the mesh of the active object.
   If the active object has no mesh, a warning will be displayed.
   If the property is marked in red, it means the Color Attribute is not available in
   the mesh of the active object, but it may be available in other meshes of
   objects that share this material!


Outputs
=======

Color
   Standard color output.
Alpha
   Alpha value.


## Volume Info

.. _bpy.types.ShaderNodeVolumeInfo:

****************
Volume Info Node
****************

.. figure:: /images/node-types_ShaderNodeVolumeInfo.webp
   :align: right
   :alt: Volume Info Node.

The *Volume Info* node provides information about *Smoke Domains*.


Inputs
======

This node has no inputs.


Properties
==========

This node has no properties.


Outputs
=======

Color
   Gives the color of the smoke inside the :doc:`Fluid Domain </physics/fluid/type/domain/index>`.
   The color and vector outputs are the same. The Factor output is an average of the channels.
Density
   Gives a scalar defining the density of any smoke inside
   the :doc:`Fluid Domain </physics/fluid/type/domain/index>`.
Flame
   Gives a scalar defining the density of any fire inside
   the :doc:`Fluid Domain </physics/fluid/type/domain/index>`.
   All three outputs are the same.
Temperature
   Gives a scalar defining the temperature of the volume. Values in the range 0 - 1 map to 0 - 1000 kelvin.
   This may be used to render physically-based fire with the Blackbody or Principled Volume shaders.
   All three outputs are the same.


Example
=======

.. figure:: /images/render_shader-nodes_input_volume-info_example.jpg

   Smoke density.

.. figure:: /images/render_shader-nodes_input_volume-info_example-fire.jpg

   Computing the color of fire using the *Blackbody* node.
   Since the *Blackbody* node expects its input in Kelvin,
   the temperature output has to be remapped first.


## Wireframe

.. _bpy.types.ShaderNodeWireframe:

**************
Wireframe Node
**************

.. figure:: /images/node-types_ShaderNodeWireframe.webp
   :align: right
   :alt: Wireframe Node.

The Wireframe node is used to retrieve the edges of an object as it appears to Cycles.
As meshes are triangulated before being processed by Cycles,
topology will always appear triangulated when viewed with the Wireframe node.


Inputs
======

This node has no inputs.


Properties
==========

Pixel Size
   When enabled, the size of edge lines is set in screen space.
Size
   Thickness of the edge lines.


Outputs
=======

Factor
   Black-and-white mask showing white lines representing edges according to the object's :term:`Topology`.


Examples
========

.. figure:: /images/render_shader-nodes_input_wireframe_example.jpg

   Using the Wireframe node to showcase the topology of a mesh.


## Aov

.. _bpy.types.ShaderNodeOutputAOV:

***************
AOV Output Node
***************

.. figure:: /images/node-types_ShaderNodeOutputAOV.webp
   :align: right
   :alt: AOV Output Node.

Shader AOVs (Arbitrary Output Variables) provide custom render passes for arbitrary shader node components.
As an artist this can be a good way to debug or tweak very fine details of a scene in post-processing.
To use shader AOVs create the pass in the :ref:`Shader AOV <bpy.types.AOV>` panel
then reference that pass with the *AOV Output* shading node.
Shader AOVs can be added or removed in the *Shader AOV* panel.

.. tip::

   The *AOV Output* node can be used in Material and World shader nodes.


Inputs
======

Color
   Output a color variable; as the name suggest can be used for a color but also a normal value.
Value
   Output a single numerical value.


Properties
==========

Name
   The name of the render pass to assign the input value to.
   This property has the same *Name* that is specified in the :ref:`Shader AOV <bpy.types.AOV>` panel.


Outputs
=======

This node has no outputs.


## Index


##########
  Output
##########

Output nodes are the final node in every node tree.
Although you can add more than one, only one will be used (indicated by a colored or darkened header).
Output nodes are always preceded by :doc:`Shaders </render/shader_nodes/shader/index>`
except in the case of the :doc:`Displacement </render/materials/components/displacement>` of a Material Output.

.. toctree::
   :maxdepth: 1

   aov.rst
   material.rst
   light.rst
   world.rst


## Light

.. _bpy.types.ShaderNodeOutputLight:

**********
Light Node
**********

.. figure:: /images/node-types_ShaderNodeOutputLight.webp
   :align: right
   :alt: Light Node.

The *Light Output* node is used to output light information to a light object.


Inputs
======

Surface
   Shading for the surface of the :doc:`light object </render/lights/light_object>`.


Properties
==========

Target
   Render engine the input shaders are used for.
   By default shaders are shared between Cycles and EEVEE,
   with multiple output nodes specialized shader setups can be created for each.


Outputs
=======

This node has no outputs.


## Material

.. _bpy.types.ShaderNodeOutputMaterial:

*************
Material Node
*************

.. figure:: /images/node-types_ShaderNodeOutputMaterial.webp
   :align: right
   :alt: Material Node.

The *Material Output* node is used to output surface material information to a surface object.


Inputs
======

Surface
   Shading for the :doc:`surface </render/materials/components/surface>` of the object.
Volume
   Shading for the :doc:`volume </render/materials/components/volume>` inside the object.
Displacement
   Used to create bump mapping or actual subdivided :doc:`displacement </render/materials/components/displacement>`.


Properties
==========

Target
   Render engine the input shaders are used for.
   By default shaders are shared between Cycles and EEVEE,
   with multiple output nodes specialized shader setups can be created for each.


Outputs
=======

This node has no outputs.


## World

.. _bpy.types.ShaderNodeOutputWorld:

**********
World Node
**********

.. figure:: /images/node-types_ShaderNodeOutputWorld.webp
   :align: right
   :alt: World Node.

The *World Output* node is used to output light a color information
to the scene's :doc:`World </render/lights/world>`.


Inputs
======

Surface
   The appearance of the environment,
   usually preceded by a :doc:`Background </render/shader_nodes/shader/background>` shader.
Volume
   Used to add volumetric effects to the world.
   See the shaders :doc:`Volume Absorption </render/shader_nodes/shader/volume_absorption>`
   and :doc:`Volume Scatter </render/shader_nodes/shader/volume_scatter>` for more information.

   .. note::

      It is not possible to have an HDR and volumetric due to the fact that
      HDR's are assumed to be an infinite distance from the camera.


Properties
==========

Target
   Render engine the input shaders are used for.
   By default shaders are shared between Cycles and EEVEE,
   with multiple output nodes specialized shader setups can be created for each.


Outputs
=======

This node has no outputs.


## Add

.. _bpy.types.ShaderNodeAddShader:

**********
Add Shader
**********

.. figure:: /images/node-types_ShaderNodeAddShader.webp
   :align: right
   :alt: Add Shader node.

The *Add* node is used to add two *Shaders* together.

.. (TODO) explain the difference Add vs Mix shaders (it's not obvious)
   adds lightness values... aren't necessarily physically correct...
   should be used with Emission and Background shaders...

   check the example image, is it correct to show the Mix shader here?


Inputs
======

Shaders
   Standard shader inputs.


Properties
==========

This node has no properties.


Outputs
=======

Shader
   Standard shader output.


Example
=======

.. figure:: /images/render_shader-nodes_shader_mix_example.jpg

   A mix of a glossy and a diffuse shader makes a nice ceramic material.


## Background

.. _bpy.types.ShaderNodeBackground:

**********
Background
**********

.. figure:: /images/node-types_ShaderNodeBackground.webp
   :align: right
   :alt: Background Shader node.

The *Background* shader node is used to add background light emission.
This node should only be used for the world surface output.


Inputs
======

Color
   Color of the emitted light.
Strength
   Strength of the emitted light.


Properties
==========

This node has no properties.


Outputs
=======

Background
   Standard shader output.


## Diffuse

.. _bpy.types.ShaderNodeBsdfDiffuse:

************
Diffuse BSDF
************

.. figure:: /images/node-types_ShaderNodeBsdfDiffuse.webp
   :align: right
   :alt: Diffuse BSDF node.

The *Diffuse* :abbr:`BSDF (Bidirectional Scattering Distribution Function)`
node is used to add Lambertian and Oren-Nayar diffuse reflection.


Inputs
======

Color
   Color of the surface, or physically speaking,
   the probability that light is reflected or transmitted for each wavelength.
Roughness :guilabel:`Cycles Only`
   Surface roughness; 0.0 gives standard Lambertian reflection, higher values activate the Oren-Nayar BSDF.
Normal
   Normal used for shading; if nothing is connected the default shading normal is used.


Properties
==========

This node has no properties.


Outputs
=======

BSDF
   Standard shader output.


Examples
========

.. list-table::
   :widths: auto

   * - .. figure:: /images/render_shader-nodes_shader_diffuse_example.jpg

          Lambertian reflection.

     - .. figure:: /images/render_shader-nodes_shader_diffuse_behavior.svg
          :width: 308px

          Diffuse shader behavior.

   * - .. figure:: /images/render_shader-nodes_shader_diffuse_example-oren-nayar.jpg

          Oren-Nayar reflection.

     - ..


## Emission

.. _bpy.types.ShaderNodeEmission:

********
Emission
********

.. figure:: /images/node-types_ShaderNodeEmission.webp
   :align: right
   :alt: Emission Shader node.

The *Emission* node is used to add Lambertian emission shader.
This can for example, be used for material and light surface outputs.

Light strength for point, spot and area lights is specified in Watts.

Sun lights are specified in Watts/m\ :sup:`2`, which require much smaller values like 1 W/m\ :sup:`2`.
This can be confusing, but specifying strength in Watts would not have been convenient;
the real sun for example has strength 384.6×10\ :sup:`24`\ W.
Emission shaders on meshes are also in Watts/m\ :sup:`2`.


Inputs
======

Color
   Color of the emitted light.
Strength
   Strength of the emitted light. For point and area lights, the unit is Watts.
   For materials, a value of 1.0 will ensure that the object in the image has
   the exact same color as the Color input, i.e. make it 'shadeless'.


Properties
==========

This node has no properties.


Outputs
=======

Emission
   The Emission shader output can both be plugged into the *Surface Input* as well as
   the *Volume Input* of the :doc:`Material Output </render/shader_nodes/output/material>` node.


Examples
========

.. list-table::

   * - .. figure:: /images/render_shader-nodes_shader_emission_example.jpg

          Emission shader, with strength at 1.0.

     - .. figure:: /images/render_shader-nodes_shader_emission_example-bright.jpg

          Emission shader, with strength at 3.0.


## Glass

.. _bpy.types.ShaderNodeBsdfGlass:

**********
Glass BSDF
**********

.. figure:: /images/node-types_ShaderNodeBsdfGlass.webp
   :align: right
   :alt: Glass BSDF node.

The *Glass* :abbr:`BSDF (Bidirectional Scattering Distribution Function)`
is used to add a Glass-like shader mixing refraction and reflection at grazing angles.
Like the transparent shader, only pure white will make it transparent.
The glass shader tends to cause noise due to caustics.
Since the Cycles path tracing integrator is not very good at rendering caustics,
it helps to combine this with a transparent shader for shadows;
for :ref:`more details see here <render-cycles-reducing-noise-glass-and-transp-shadows>`.


Inputs
======

Color
   Color of the surface, or physically speaking, the probability that light is transmitted for each wavelength.
Roughness
   Influences sharpness of the refraction; perfectly sharp at 0.0 and smoother with higher values.
IOR
   Index of refraction (:term:`IOR`) defining how much the ray changes direction. At 1.
   0 rays pass straight through like transparent; higher values give more refraction.
Normal
   Normal used for shading.


Properties
==========

Distribution
   Microfacet distribution to use.

   :GGX: GGX microfacet distribution.
   :Multiscatter GGX:
      Takes multiple scattering events between microfacets into account.
      This gives more energy conserving results, which would otherwise be visible as excessive darkening.
   :Beckmann: :guilabel:`Cycles Only`
      Beckmann microfacet distribution.


Outputs
=======

BSDF
   Standard shader output.


Examples
========

.. list-table::
   :widths: auto

   * - .. figure:: /images/render_shader-nodes_shader_glass_example.jpg

          Sharp Glass example.

     - .. figure:: /images/render_shader-nodes_shader_glass_behavior-sharp.svg
          :width: 308px

          Sharp Glass behavior.

   * - .. figure:: /images/render_shader-nodes_shader_glass_example-rough.jpg

          Rough Glass example.

     - .. figure:: /images/render_shader-nodes_shader_glass_behavior.svg
          :width: 308px

          Rough Glass behavior.


## Glossy

.. _bpy.types.ShaderNodeBsdfAnisotropic:

***********
Glossy BSDF
***********

.. figure:: /images/node-types_ShaderNodeBsdfAnisotropic.webp
   :align: right
   :alt: Glossy BSDF node.

The *Glossy* :abbr:`BSDF (Bidirectional Scattering Distribution Function)`
node is used to add reflection with microfacet distribution, used for materials such as metal or mirrors.


Inputs
======

Color
   Color of the surface, or physically speaking, the probability that light is reflected for each wavelength.
Roughness
   Sharpness of the reflection; perfectly sharp at 0.0 and smoother with higher values.
Anisotropy :guilabel:`Cycles Only`
   Controls the amount the reflection stretches the reflection along the surface of the material.
   A value of 0.0 results in no anisotropic reflections.
   Higher values give elongated highlights orthogonal to the tangent direction;
   negative values give highlights shaped along the tangent direction.

   This is a phenomenon know as "Anisotropic Reflections" which is often seen in metallic materials.
Rotation
   Rotation of the anisotropic tangent direction.
   Value 0.0 equals 0° rotation, 0.25 equals 90° and 1.0 equals 360° = 0°.
   This can be used to texture the tangent direction.

   .. list-table::

      * - .. figure:: /images/render_shader-nodes_shader_anisotropic_rot0.jpg

             Anisotropic rotation on 0.

        - .. figure:: /images/render_shader-nodes_shader_anisotropic_rot025.jpg

             Anisotropic rotation on 0.25 (90°).

Normal
   Normal used for shading; if nothing is connected the default shading normal is used.
Tangent
   Tangent used for shading; if nothing is connected the default shading tangent is used.


Properties
==========

Distribution
   Microfacet distribution to use.

   :GGX: GGX microfacet distribution.
   :Multiscatter GGX:
      Takes multiple scattering events between microfacets into account.
      This gives more energy conserving results, which would otherwise be visible as excessive darkening.
   :Beckmann: :guilabel:`Cycles Only`
      Beckmann microfacet distribution.


Outputs
=======

BSDF
   Standard shader output.


Examples
========

.. list-table::
   :widths: auto

   * - .. figure:: /images/render_shader-nodes_shader_glossy_example.jpg

          Sharp Glossy example.

     - .. figure:: /images/render_shader-nodes_shader_glossy_behavior-sharp.svg
          :width: 308px

          Sharp Glossy behavior.

   * - .. figure:: /images/render_shader-nodes_shader_glossy_rough.jpg

          Rough Glossy example.

     - .. figure:: /images/render_shader-nodes_shader_glossy_behavior.svg
          :width: 308px

          Rough Glossy behavior.

.. figure:: /images/render_shader-nodes_shader_anisotropic_example.jpg

   Anisotropic shading with 0° rotation, 90° rotation and textured rotation of the tangent direction.
   `Example blend-file <https://archive.blender.org/wiki/2015/uploads/b/b7/Blender2.65_cycles_anisotropic.blend>`__.


## Hair

.. _bpy.types.ShaderNodeBsdfHair:

*********
Hair BSDF
*********

.. figure:: /images/node-types_ShaderNodeBsdfHair.webp
   :align: right
   :alt: Hair BSDF node.

:guilabel:`Cycles Only`

The *Hair* :abbr:`BSDF (Bidirectional Scattering Distribution Function)`
node is used to add shading for :doc:`Hair </physics/particles/hair/index>`.


Inputs
======

Color
   Color of the hair.
Offset
   Controls the way the light is rotated (angular shift) for the reflection/transmission.

   .. figure:: /images/render_shader-nodes_shader_hair_reflect-offset.png
      :align: center

      Reflection Offset.

   .. figure:: /images/render_shader-nodes_shader_hair_trans-offset.png
      :align: center

      Transmission Offset.

Roughness U/V
   Controls the roughness in the direction light is skewed, and perpendicular to it.

   .. list-table::

      * - .. figure:: /images/render_shader-nodes_shader_hair_reflect-roughness-uv.png

             Roughness when using the Reflection Component.

        - .. figure:: /images/render_shader-nodes_shader_hair_trans-roughness-uv.png

             Roughness when using the Transmission Component.

Tangent
   Input tangent.


Properties
==========

Component
   There are two components that can be used to control the look of the hair.
   Usually you are going to want each of these and use a :doc:`Mix Node </render/shader_nodes/shader/mix>`.

   :Reflection: The light that bounces off the surface of the hair.
   :Transmission: The light that passes through the hair and exits on the other side.

   .. figure:: /images/render_shader-nodes_shader_hair_mix-node.png
      :align: center

      With Mix node: 0 is full Reflection, 1 is full Transmission.


Outputs
=======

BSDF
   Standard shader output.


## Hair Principled

.. _bpy.types.ShaderNodeBsdfHairPrincipled:

********************
Principled Hair BSDF
********************

.. figure:: /images/node-types_ShaderNodeBsdfHairPrincipled.webp
   :align: right
   :alt: Principled Hair BSDF node under Melanin concentration.

:guilabel:`Cycles Only`

The *Principled Hair* :abbr:`BSDF (Bidirectional Scattering Distribution Function)` is a physically-based,
easy-to-use shader for rendering hair and fur.

.. tip::

   Realistic hair should have a minimum of variance between each strand.
   The shader allows for this by specifying two values, *Random Color*
   and *Random Roughness*, which remap the specified Melanin/Roughness values to
   the range :math:`Color/Roughness \pm Randomization\%`.


Inputs
======

Common
------

Color
   The RGB color of the strand. Only used in Direct coloring.

   .. hint::

      The chosen color is converted to an absorption coefficient with
      the following formula (section 4.2 of [CBTB16]_):

      .. math::

         \sigma_{a} = \frac{\ln(Color)}
         {\left(5.969 - 0.215\beta_{N} + 2.532\beta_{N}^{2} -
         10.73\beta_{N}^{3} + 5.574\beta_{N}^{4} + 0.245\beta_{N}^{5}\right)^{2}}

      where :math:`\beta_{N}` is the radial roughness of the hair after applying randomization (if specified).

   .. figure:: /images/render_shader-nodes_shader_hair-principled_demo-color.jpg
      :align: center

      Coloring hair using the Direct coloring parametrization. (The numbers on top are the RGB values.)

Melanin
   Absolute quantity of pigment.
   Range :math:`[0, 1]` equivalent to :math:`[0\%, 100\%]`.

   .. hint::

      This is a linear mapping to the underlying exponential function:

      .. math::

         melanin\_qty = -\ln(\max(1.0 - Melanin, 0.0001))

   .. figure:: /images/render_shader-nodes_shader_hair-principled_demo-melanin.jpg
      :align: center

      Melanin.

Melanin Redness
   Ratio of pheomelanin to eumelanin.
   Range :math:`[0, 1]` equivalent to :math:`[0\%, 100\%]`.

   .. hint::

      The ratio formula is: :math:`eumelanin = Melanin×(1.0-MelaninRedness)`,
      :math:`pheomelanin = Melanin×MelaninRedness`.

      The resulting quantities are converted (after randomization, if specified)
      to absorption concentration via the following formula
      (section 6.1 of [EFHLA11]_, adjusted for the range :math:`[0, 1]`):

      .. math::

         \sigma_{a} =
         eumelanin   × \left[\begin{matrix} 0.506 \\ 0.841 \\ 1.653 \\ \end{matrix}\right] +
         pheomelanin × \left[\begin{matrix} 0.343 \\ 0.733 \\ 1.924 \\ \end{matrix}\right]

   .. figure:: /images/render_shader-nodes_shader_hair-principled_demo-melanin-redness.jpg
      :align: center

      Melanin Redness.

Tint
   Color used for dyeing the hair after applying the melanin pigment.
   It is not subject to randomization.
   It can be disabled by setting the color to white.

   .. hint::

      This is converted via the Color mapping above and added to
      the absorption coefficient of the melanin concentration.

   .. figure:: /images/render_shader-nodes_shader_hair-principled_demo-tint.jpg
      :align: center

      Tint, using Melanin 0.1 and the corresponding RGB values.

Absorption Coefficient
   Attenuation coefficient :math:`\sigma`.

IOR
   Index of refraction (:term:`IOR`) defining how much the ray changes direction.
   At 1.0 rays pass straight through like in a transparent material;
   higher values give more refraction.
   Default value is :math:`1.55`.
Offset
   Tilts the glint of the hair by increasing the angle of the scales of
   the hair's cuticle with respect to the hair shaft.
   Human hair usually has low values.
Random Color
   For each strand, vary the melanin concentration by :math:`RandomFactor`.
   Range :math:`[0, 1]` equivalent to :math:`[0\%, 100\%]` of
   the initial melanin concentration.

   .. hint::

      The melanin concentration is multiplied by :math:`randomFactor`,
      where :math:`randomFactor = 1.0 + 2.0×(Random - 0.5) × RandomColor`.

   .. figure:: /images/render_shader-nodes_shader_hair-principled_demo-random-color.jpg
      :align: center

      Random Color.

Random Roughness
   For each strand, vary both Roughness values by :math:`RandomFactor`.
   Range :math:`[0, 1]` equivalent to :math:`[0\%, 100\%]` of
   the initial roughness values.

   .. hint::

      The applied formula is the same one as for *Random Color*.

   .. figure:: /images/render_shader-nodes_shader_hair-principled_demo-random-roughness.jpg
      :align: center

      Random Roughness.

Random
   Random number source. If no node is connected here, it is automatically
   instanced with the value obtained from :menuselection:`Hair Info --> Random`.


Chiang Model
------------

The Chiang model is based on a Gaussian distribution with separate roughness
along and orthogonal to the hair.

Roughness
   Specify how much the glints are smoothed in the direction of the hair shaft.
   Too low values will smoothen the hair to the point of looking almost metallic,
   making glints look like :term:`Fireflies`; while setting it too high will result in a Lambertian look.

   .. figure:: /images/render_shader-nodes_shader_hair-principled_demo-roughness.jpg
      :align: center

      Roughness.

Radial Roughness
   Specify how much the glints are smoothed in the direction of the hair normal.
   Too low values will concentrate the glint;
   while setting it too high will spread the light across the width of the strand.

   .. hint::

      Mathematically, this parameter is mapped to the logistic distribution's
      scale factor :math:`s` (section 4.1 of [CBTB16]_).

.. figure:: /images/render_shader-nodes_shader_hair-principled_demo-radial-roughness.jpg
   :align: center

   Radial Roughness.

Coat
   Simulate a shiny coat of fur, by reducing the Roughness to the given factor
   only for the first light bounce (diffuse).
   Range :math:`[0, 1]` equivalent to a reduction of :math:`[0\%, 100\%]` of the original Roughness.

   .. figure:: /images/render_shader-nodes_shader_hair-principled_demo-coat.jpg
      :align: center

      Coat.


Huang Model
-----------

The Huang model is based on microfacet based reflection and transmission,
and supports elliptically shaped hair.

Aspect Ratio
   The ratio of the minor axis to the major axis of an elliptical cross-section.
   Recommended values are 0.8~1 for Asian hair, 0.65~0.9 for Caucasian hair, 0.5~0.65 for
   African hair. The major axis is aligned with the curve normal, which can be created
   with geometry nodes, but is not supported in legacy particle hair.

Roughness
   Microfacet roughness for reflection and transmission.

Reflection
   Optional factor for modulating the first light bounce off the hair surface.
   The color of this component is always white. Keep this 1.0 for physical correctness.
Transmission
   Optional factor for modulating the transmission component. Picks up the color of the
   pigment inside the hair. Keep this 1.0 for physical correctness.
Secondary Reflection
   Optional factor for modulating the component which is transmitted into the hair,
   reflected off the backside of the hair and then transmitted out of the hair. This
   component is oriented approximately around the incoming direction, and picks up the
   color of the pigment inside the hair. Keep this 1.0 for physical correctness


Properties
==========

Color Parametrization
   The shader provides three different ways, or *parametrizations*, to color the hair strands.

   :Direct Coloring:
      Choose the desired RGB color and the shader will approximate
      the necessary *absorption coefficient* (below).

   :Melanin Concentration:
      This mode defines the color as the quantity and
      ratio of the pigments which are commonly found in hair and fur,
      *eumelanin* (prevalent in brown-black hair) and *pheomelanin* (red hair).
      The quantity is specified in the *Melanin* input, and the ratio between them in *Melanin Redness*.
      Increasing concentrations darken the hair (the following are with *Melanin Redness* :math:`1`):

      - White (Melanin :math:`0`)
      - Blonde (Melanin :math:`0.25`)
      - Reddish (Melanin :math:`0.5`)
      - Brown (Melanin :math:`0.75`)
      - Black (Melanin :math:`1`)

      Additionally, the *Tint* inputs allows to dye the hair with the desired color.

   :Absorption Coefficient:
      Specifies the attenuation coefficient :math:`\sigma_{a}`, as applied by the `Beer-Lambert law
      <https://en.wikipedia.org/wiki/Beer%E2%80%93Lambert_law#Expression_with_attenuation_coefficient>`__.
      This mode is intended mainly for technical users who want to use coefficients from the literature
      without any sort of conversion.


Outputs
=======

BSDF
   Standard shader output.


References
==========

This shader is an implementation of the papers by Chiang et al. [CBTB16]_ and Huang et al. [HHH22]_.

.. [CBTB16] Chiang, M. J. , Bitterli, B. , Tappan, C. and Burley, B. (2016),
   A Practical and Controllable Hair and Fur Model for Production Path Tracing. Computer Graphics Forum, 35: 275-283.
   `doi:10.1111/cgf.12830 <https://doi.org/10.1111/cgf.12830>`__

.. [EFHLA11] d'Eon, E. , Francois, G. , Hill, M. , Letteri, J. and Aubry, J. (2011),
   An Energy‐Conserving Hair Reflectance Model. Computer Graphics Forum, 30: 1181-1187.
   `doi:10.1111/j.1467-8659.2011.01976.x <https://doi.org/10.1111/j.1467-8659.2011.01976.x>`__

.. [HHH22] Huang W., Hullin M.B. Hanika J. (2022),
   A Microfacet-based Hair Scattering Model. Computer Graphics Forum, 41: 79-91.
   `doi:10.1111/cgf.14588 <https://doi.org/10.1111/cgf.14588>`__




## Holdout

.. _bpy.types.ShaderNodeHoldout:

*******
Holdout
*******

.. figure:: /images/node-types_ShaderNodeHoldout.webp
   :align: right
   :alt: Holdout node.

The *Holdout* shader node is used to create a "hole" in the image with zero alpha
transparency, which is useful for compositing (see :term:`Alpha Channel`).


Inputs
======

This node has no inputs.


Properties
==========

This node has no properties.


Outputs
=======

Holdout
   Standard shader output.


Examples
========

.. figure:: /images/render_shader-nodes_shader_holdout_example.jpg

   The checkered area is a region with zero alpha.


## Index

.. _bpy.types.Shader:

##########
  Shader
##########

.. toctree::
   :maxdepth: 1

   add.rst
   background.rst
   diffuse.rst
   emission.rst
   glass.rst
   glossy.rst
   hair.rst
   holdout.rst
   mix.rst
   principled.rst
   hair_principled.rst
   volume_principled.rst
   ray_portal.rst
   refraction.rst
   specular_bsdf.rst
   sss.rst
   toon.rst
   translucent.rst
   transparent.rst
   sheen.rst
   volume_absorption.rst
   volume_scatter.rst


## Mix

.. _bpy.types.ShaderNodeMixShader:

**********
Mix Shader
**********

.. figure:: /images/node-types_ShaderNodeMixShader.webp
   :align: right
   :alt: Mix Shader node.

The *Mix* node is used to mix two shaders together. Mixing can be used for material layering,
where the *Factor* input may, for example, be connected to a *Blend Weight* node.


Inputs
======

Shader
   Shaders to mix, such that incoming rays hit either with the specified probability in the *Factor* socket.
Factor
   Blend weight to use for mixing two shaders;
   at zero it uses the first shader entirely and at one the second shader.


Properties
==========

This node has no properties.


Outputs
=======

Shader
   Standard shader output.


Examples
========

.. figure:: /images/render_shader-nodes_shader_mix_example.jpg

   A mix of a glossy and a diffuse shader makes a nice ceramic material.


## Principled

.. _bpy.types.ShaderNodeBsdfPrincipled:

***************
Principled BSDF
***************

.. figure:: /images/node-types_ShaderNodeBsdfPrincipled.webp
   :align: right
   :alt: Principled BSDF node.

The *Principled* :abbr:`BSDF (Bidirectional Scattering Distribution Function)`
that combines multiple layers into a single easy to use node.
It can model a wide variety of materials.

It is based on the OpenPBR Surface shading model, and provides parameters
compatible with similar PBR shaders found in other software,
such as the Disney and Standard Surface models.
Image textures painted or baked from software like Substance Painter
may be directly linked to the corresponding input in this shader.


Layers
======

The base layer is a mix between metal, diffuse, subsurface, and transmission components.
Most materials will use one of these components, though it is possible to smoothly mix
between them.

.. figure:: /images/render_shader-nodes_principled_layers.svg

The metal component is opaque and only reflect lights. Diffuse is fully opaque, while
subsurface also involves light scattering just below the surface. Both diffuse and
subsurface sit below a specular layer. The transmission component includes both
specular reflection and refraction.

On top of all base layers there is an optional glossy coat. And finally the sheen layer
sits on top of all other layers, to add fuzz or dust.

Light emission can also be added. Light emits from below the coat and sheen layers,
to model for example emissive displays with a coat or dust.

Inputs
======

Base Color
   Overall color of the material used for diffuse, subsurface, metal and transmission.

   .. figure:: /images/render_shader-nodes_shader_principled-base-color.webp
      :align: center

      Same base color for multiple materials types

Roughness
   Specifies microfacet roughness of the surface for specular reflection and transmission.
   A value of 0.0 gives a perfectly sharp reflection, while 1.0 gives a diffuse reflection.

   .. figure:: /images/render_shader-nodes_shader_principled-roughness.webp
      :align: center

      Roughness from 0.0 to 1.0

Metallic
   Blends between a dielectric and metallic material model.
   At 0.0 the material consists of a diffuse or transmissive base layer, with a specular reflection layer on top.
   A value of 1.0 gives a fully specular reflection tinted with the base color,
   without diffuse reflection or transmission.

   .. figure:: /images/render_shader-nodes_shader_principled-metallic.webp
      :align: center

      Metallic from 0.0 to 1.0

IOR
   Index of refraction (:term:`IOR`) for specular reflection and transmission.
   For most materials, the IOR is between 1.0 (vacuum and air) and 4.0 (germanium).
   The default value of 1.5 is a good approximation for glass.

   .. figure:: /images/render_shader-nodes_shader_principled-ior.webp
      :align: center

      IOR from 1.0 to 2.0

Alpha
   Controls the transparency of the surface, with 1.0 fully opaque.
   Usually linked to the Alpha output of an Image Texture node.

   .. figure:: /images/render_shader-nodes_shader_principled-alpha.webp
      :align: center

      Alpha from 0.0 to 1.0

Normal
   Controls the normals of the base layers.


Subsurface
----------

:term:`Subsurface scattering` is used to render materials such as skin, milk and wax.
Light scatters below the surface to create a soft appearance.

Method
   Rendering method to simulate :term:`Subsurface scattering`.

   :Christensen-Burley:
      An approximation to physically-based volume scattering.
      This method is less accurate than *Random Walk* however,
      in some situations this method will resolve noise faster.
   :Random Walk: :guilabel:`Cycles Only`
      Provides accurate results for thin and curved objects.
      Random Walk uses true volumetric scattering inside the mesh,
      which means that it works best for closed meshes.
      Overlapping faces and holes in the mesh can cause problems.
   :Random Walk (Skin): :guilabel:`Cycles Only`
      Random walk method optimized for skin rendering. The radius
      is automatically adjusted based on the color texture, and
      the subsurface entry direction uses a mix of diffuse and
      specular transmission with custom :term:`IOR`. This tends to retain
      greater surface detail and color and matches measured skin
      more closely.

Weight
   Blend between diffuse surface and subsurface scattering.
   Typically should be zero or one (either fully diffuse or subsurface).

   .. figure:: /images/render_shader-nodes_shader_principled-subsurface-weight.webp
      :align: center

      Weight from 0.0 to 1.0

Radius
   Average distance that light scatters below the surface.
   Higher radius gives a softer appearance, as light bleeds into shadows and through the object.
   The scattering distance is specified separately for the RGB channels,
   to render materials such as skin where red light scatters deeper.
   The X, Y and Z values are mapped to the R, G and B values, respectively.

   .. figure:: /images/render_shader-nodes_shader_principled-subsurface-radius.webp
      :align: center

      Radius from white to red

Scale
   Scale applied to the radius.

   .. figure:: /images/render_shader-nodes_shader_principled-subsurface-scale.webp
      :align: center

      Scale from 0 cm to 50 cm

IOR :guilabel:`Cycles Only`
   Index of refraction (:term:`IOR`) used for rays that enter the subsurface component. This may be set to
   a different value than the global IOR to simulate different layers of skin.

   .. figure:: /images/render_shader-nodes_shader_principled-subsurface-ior.webp
      :align: center

      IOR from 1.0 to 2.0

Anisotropy :guilabel:`Cycles Only`
   Directionality of volume scattering within the subsurface medium. Zero scatters uniformly
   in all directions, with higher values scattering more strongly forward.
   For example, skin has been measured to have an anisotropy of 0.8.

   .. figure:: /images/render_shader-nodes_shader_principled-subsurface-anisotropy.webp
      :align: center

      Anisotropy from 0.0 to 1.0


Specular
--------

Controls for both the metallic component and specular layer on top of diffuse and subsurface.

Distribution
   Microfacet distribution to use.

   :GGX:
      A method that is faster than *Multiple-scattering GGX* but is less physically accurate.
   :Multiscatter GGX:
      Takes multiple scattering events between microfacets into account.
      This gives more energy conserving results,
      which would otherwise be visible as excessive darkening.

IOR Level
   Adjustment to the :term:`IOR` to increase or decrease intensity of the specular layer.
   0.5 means no adjustment, 0 removes all reflections, 1 doubles them at normal incidence.

   This input is designed for conveniently texturing the IOR and amount of specular
   reflection.

   .. figure:: /images/render_shader-nodes_shader_principled-specular-ior-level.webp
      :align: center

      IOR level from 0.0 to 1.0

Tint
   Color tint for specular and metallic reflection.

   For non-metallic tints provides artistic control over the color specular reflections at normal incidence,
   while grazing reflections remain white. In reality non-metallic specular reflection is fully white.

   For metallic materials tints the edges to simulate complex IOR as found in materials such as gold or copper.

   .. figure:: /images/render_shader-nodes_shader_principled-specular-tint.webp
      :align: center

      Tint from white to orange

Anisotropic :guilabel:`Cycles Only`
   Amount of anisotropy for specular reflection. Higher values give elongated highlights along the tangent direction;
   negative values give highlights shaped perpendicular to the tangent direction.

   .. figure:: /images/render_shader-nodes_shader_principled-specular-anisotropic.webp
      :align: center

      Anisotropic from 0.0 to 1.0

Anisotropic Rotation :guilabel:`Cycles Only`
   Rotates the direction of anisotropy, with 1.0 going full circle.

   Compared to the *Anisotropic BSDF* node, the direction of highlight elongation
   is rotated by 90°. Add 0.25 to the value to correct.

   .. figure:: /images/render_shader-nodes_shader_principled-specular-anisotropic-rotation.webp
      :align: center

      Anisotropic rotation from 0.0 to 1.0

Tangent
   Controls the tangent direction for anisotropy.


Transmission
------------

Transmission is used to render materials like glass and liquids, where the surface both
reflects light and transmits it into the interior of the object

Weight
   Mix between fully opaque surface at zero and fully transmissive at one.

   .. figure:: /images/render_shader-nodes_shader_principled-transmission-weight.webp
      :align: center

      Weight from 0.0 to 1.0

Coat
----

Coat on top of the materials, to simulate for example a clearcoat, lacquer or car paint.

Weight
   Controls the intensity of the coat layer, both the reflection and the tinting.
   Typically should be zero or one for physically-based materials, but may be textured
   to vary the amount of coating across the surface.

   .. figure:: /images/render_shader-nodes_shader_principled-coat-weight.webp
      :align: center

      Weight from 0.0 to 1.0

Roughness
   Roughness of the coat layer.

   .. figure:: /images/render_shader-nodes_shader_principled-coat-roughness.webp
      :align: center

      Roughness from 0.0 to 1.0

IOR
   Index of refraction (:term:`IOR`) of the coat layer.
   Affects its reflectivity as well as the falloff of coat tinting.

   .. figure:: /images/render_shader-nodes_shader_principled-coat-ior.webp
      :align: center

      IOR from 1.0 to 2.0

Tint
   Adds a colored tint to the coat layer by modeling absorption in the layer.
   Saturation increases at shallower angles, as the light travels farther
   through the medium, depending on the IOR.

   .. figure:: /images/render_shader-nodes_shader_principled-coat-tint.webp
      :align: center

      Tint from white to blue

Normal
   Controls the normals of the *Coat* layer, for example to add a smooth coating on a rough surface.


Sheen
-----

Sheen simulates very small fibers on the surface.
For cloth this adds a soft velvet like reflection near edges.
It can also be used to simulate dust on arbitrary materials.

Weight
   Controls in the intensity of the sheen layer.

   .. figure:: /images/render_shader-nodes_shader_principled-sheen-weight.webp
      :align: center

      Weight from 0.0 to 1.0

Roughness
   Roughness of the sheen reflection.

   .. figure:: /images/render_shader-nodes_shader_principled-sheen-roughness.webp
      :align: center

      Roughness from 0.0 to 1.0

Tint
   The color of the sheen reflection.

   .. figure:: /images/render_shader-nodes_shader_principled-sheen-tint.webp
      :align: center

      Tint from white to green.


Emission
--------

Light emission from the surface.

Color
   Color of light emission from the surface.

   .. figure:: /images/render_shader-nodes_shader_principled-emission-color.webp
      :align: center

      Emission color variations

Strength
   Strength of the emitted light. A value of 1.0 ensures that the object
   in the image has the exact same color as the *Emission Color*, i.e. make it 'shadeless'.


   .. figure:: /images/render_shader-nodes_shader_principled-emission-strength.webp
      :align: center

      Strength from 0.0 to 10.0


Outputs
=======

BSDF
   Standard shader output.


## Ray Portal

.. _bpy.types.ShaderNodeBsdfRayPortal:

***************
Ray Portal BSDF
***************

:guilabel:`Cycles Only`

The *Ray Portal BSDF* node transports rays that enter to another location
in the scene. It can be used to render portals for visual effects, and
other production rendering tricks.

It acts much like a :doc:`Transparent BSDF </render/shader_nodes/shader/transparent>`:
render passes are passed through,
and it is affected by light path max transparent bounces.

.. note::

   - The *Ray Portal BSDF* only allows rays to pass through it in one direction. Add a
     second portal at the target location to make rays go in the other direction as well.

   - Light sampling does not work efficiently through portals. This can lead to increased
     noise from lights on the other side of portals. Particularly small lights may be very
     noisy, or not pass through at all.


Inputs
======

Color
   Tint rays passing through the portal.
Position
   Ray start position at new location. Defaults to the current position,
   matching the Position output of the
   :doc:`Geometry node </render/shader_nodes/input/geometry>`.
Direction
   Ray direction at the new location. Defaults to the current view direction,
   which is the same as the negation of the Incoming output of the
   :doc:`Geometry node </render/shader_nodes/input/geometry>`.


Properties
==========

This node has no properties.


Outputs
=======

BSDF
   Standard shader output.

Examples
========

One use case for the *Ray Portal BSDF* is to connect two spaces together to
create effects like a portal to an alternative dimension, or "impossible spaces"
where something is bigger or smaller on the inside than expected.

To set up a *Ray Portal BSDF* for a technique like this, augment the
*Position* and *Incoming* outputs of the
:doc:`Geometry node </render/shader_nodes/input/geometry>` to set the exit point
and direction of the ray through the portal. Here are some examples:

Simple Offset
-------------

.. figure:: /images/render_shader-nodes_ray-portal-bsdf_simple-ray-offset-nodes.jpg


   This simple node setup offsets the ray position.
   In this example, the ray is offset 0 units along the X axis,
   4 units along the Y axis, and 5 units along the Z axis.

Portal
------

.. figure:: /images/render_shader-nodes_ray-portal-bsdf_gateway-example.jpg

.. figure:: /images/render_shader-nodes_ray-portal-bsdf_ray-augmentation-nodes.jpg

   In this example, the *Location of Portal Target* and *Rotation of Portal Target*
   vectors are obtained from a target portal object using
   :doc:`Drivers </animation/drivers/introduction>`.

Camera Feed
-----------

Along with augmenting rays, the ray position and ray direction can be replaced,
for effects like a camera feed on a screen.

.. figure:: /images/render_shader-nodes_ray-portal-bsdf_portal-to-screen-example.jpg

   Using the Ray Portal BSDF to replicate the effect of a camera feed on a screen.

.. figure:: /images/render_shader-nodes_ray-portal-bsdf_portal-to-screen-nodes.jpg

   Node setup for replicating a camera feed like effect on a screen.


## Refraction

.. _bpy.types.ShaderNodeBsdfRefraction:

***************
Refraction BSDF
***************

.. figure:: /images/node-types_ShaderNodeBsdfRefraction.webp
   :align: right
   :alt: Refraction BSDF node.

The *Refraction* :abbr:`BSDF (Bidirectional Scattering Distribution Function)`
is used to add glossy refraction with sharp or microfacet distribution,
used for materials that transmit light. For best results this node should be considered as
a building block and not be used on its own,
but rather mixed with a glossy node using a Fresnel factor.
Otherwise it will give quite dark results at the edges for glossy refraction.


Inputs
======

Color
   Color of the surface, or physically speaking, the probability that light is refracted for each wavelength.
Roughness
   Influences sharpness of the refraction; perfectly sharp at 0.0 and smoother with higher values.
Normal
   Normal used for shading; if nothing is connected the default shading normal is used.


Properties
==========

Distribution
   Microfacet distribution to use.

   :GGX: GGX microfacet distribution.
   :Beckmann: :guilabel:`Cycles Only`
      Beckmann microfacet distribution.


Outputs
=======

BSDF
   Standard shader output.


Examples
========

.. figure:: /images/render_shader-nodes_shader_refraction_example.jpg

   Refraction Shader.


## Sheen

.. _bpy.types.ShaderNodeBsdfVelvet:
.. _bpy.types.ShaderNodeBsdfSheen:

**********
Sheen BSDF
**********

.. figure:: /images/node-types_ShaderNodeBsdfSheen.png
   :align: right
   :alt: Sheen BSDF node.

:guilabel:`Cycles Only`

The *Sheen* :abbr:`BSDF (Bidirectional Scattering Distribution Function)`
is used to add reflection to materials that have micro surface details such as cloth or dust.
This shader is intended to be layered on top of other shaders such as
:term:`dielectric <Dielectric Material>` or metallic shader setups.


Inputs
======

Color
   Color of the surface, or physically speaking, the probability that light is reflected for each wavelength.
Roughness
   Controls the amount of color that is reflected back to the camera,
   higher values reflect more color and can give a dusty appearance, while lower values look fuzzy and darker.
Normal
   Normal used for shading; if nothing is connected the default shading normal is used.


Properties
==========

Distribution
   Sheen shading model.

   :Ashikhmin: Classic Ashikhmin velvet, used in Blender versions prior to 4.0
   :Microfiber: Microflake-based model of multiple scattering between normal-oriented fibers.


Outputs
=======

BSDF
   Standard shader output.


Examples
========

.. list-table::
   :widths: auto

   * - .. figure:: /images/render_shader-nodes_shader_sheen_example.png

          The Sheen shader example.

     - .. figure:: /images/render_shader-nodes_shader_sheen_behavior.svg
          :width: 308px

          The Sheen shader behavior.


## Specular Bsdf

.. _bpy.types.ShaderNodeEeveeSpecular:

*************
Specular BSDF
*************

.. figure:: /images/node-types_ShaderNodeEeveeSpecular.webp
   :align: right
   :alt: Specular BSDF node.

:guilabel:`EEVEE Only`

The *Specular* :abbr:`BSDF (Bidirectional Scattering Distribution Function)`
combines multiple layers into a single easy to use node.

It is similar to the :doc:`Principled BSDF </render/shader_nodes/shader/principled>` node
but uses the *specular* workflow instead of the metallic.
It has far fewer parameters and supports less features. Both might be merged into one node in the future.

The specular workflow functions by specifying the facing (along normal) reflection color.
The result may not be physically plausible because there is no energy conservation.


Inputs
======

Base Color
   Diffuse surface color. For conductor materials (metals) it should be black.

Specular
   Amount of specular reflection. Specifies facing (along normal)
   reflectivity. Conductor materials (metals) can have colored specular reflection.

   .. hint::

      To compute this value for a realistic material with a known index of
      refraction, you may use this special case of the Fresnel formula:
      :math:`specular = ((ior - 1)/(ior + 1))^2`

      For example:

      - water: ior = 1.33, specular = 0.02
      - glass: ior = 1.5, specular = 0.04
      - diamond: ior = 2.417, specular = 0.17

Roughness
   Specifies microfacet roughness of the surface for diffuse and specular reflection.

   .. hint::

      When converting from the older *Glossy BSDF* node, use the square root of the original value.

Emissive Color
   Color of the emitted light. This light is added to the BSDF result.

Transparency
   Transparency factor. This is the inverse of the alpha channel (1 - alpha) you find in an image.
   Use an Invert node to convert alpha to transparency.
   This will only have an effect if the material uses a blend mode other than opaque.

Normal
   Controls the normals of the base layers.

Clear Coat
   Extra white specular layer on top of others.
   This is useful for materials like car paint and the like.

Clear Coat Roughness:
   Roughness of clear coat specular.

Clear Coat Normal
   Controls the normals of the *Clear Coat* layer.

Ambient Occlusion
   Amount of occlusion to apply to indirect lighting. Usually a bake ambient occlusion map.
   The final occlusion factor is the minimum of this input and the runtime ambient occlusion effect.


Properties
==========

This node has no properties.


Outputs
=======

BSDF
   Standard shader output.


## Sss

.. _bpy.types.ShaderNodeSubsurfaceScattering:

*********************
Subsurface Scattering
*********************

.. figure:: /images/node-types_ShaderNodeSubsurfaceScattering.webp
   :align: right
   :alt: Subsurface Scattering node.

The *Subsurface Scattering* node is used to add simple subsurface multiple scattering,
for materials such as skin, wax, marble, milk and others. For these materials,
rather than light being reflect directly off the surface, it will penetrate the surface and
bounce around internally before getting absorbed or leaving the surface at a nearby point.

How far the color scatters on average can be configured per RGB color channel. For example,
for skin, red colors scatter further, which gives distinctive red-colored shadows,
and a soft appearance.


Inputs
======

Color
   Color of the surface, or physically speaking, the probability that light is reflected for each wavelength.
Scale
   Global scale factor for the scattering radius.
Radius
   Average distance that light scatters below the surface.
   Higher radius gives a softer appearance, as light bleeds into shadows and through the object.
   The scattering distance is specified separately for the RGB channels,
   to render materials such as skin where red light scatters deeper.
   The X, Y and Z values are mapped to the R, G and B values, respectively.
IOR :guilabel:`Cycles Only`
   Index of refraction for *Subsurface Scattering*.
Anisotropy :guilabel:`Cycles Only`
   Directionality of subsurface scattering. Higher anisotropy scatters deeper into the object.
Roughness :guilabel:`Cycles Only`
   Roughness of the glossy surface surrounding the subsurface volume.
Normal
   Normal used for shading; if nothing is connected the default shading normal is used.


Properties
==========

Subsurface Method
   Rendering method to simulate subsurface scattering.

   .. note:: EEVEE does not support the *Random Walk* methods.

   :Christensen-Burley:
      An approximation to physically-based volume scattering.
      This method is less accurate than *Random Walk* however,
      in some situations this method will resolve noise faster.
   :Random Walk (Fixed Radius):
      Provides accurate results for thin and curved objects.
      Random Walk uses true volumetric scattering inside the mesh,
      which means that it works best for closed meshes.
      Overlapping faces and holes in the mesh can cause problems.
   :Random Walk:
      Behaves similarly to *Random Walk (Fixed Radius)* but modulates
      the *Radius* based on the *Color*, *Anisotropy*, and *IOR*.
      This method thereby attempts to retain greater surface detail and color
      than *Random Walk (Fixed Radius)*.


Outputs
=======

BSSRDF
   :abbr:`BSSRDF (Bidirectional Scattering Surface Reflectance Distribution Function)` shader output.


Examples
========

.. figure:: /images/render_shader-nodes_shader_sss_example.jpg

   Random walk subsurface scattering.


## Toon

.. _bpy.types.ShaderNodeBsdfToon:

*********
Toon BSDF
*********

.. figure:: /images/node-types_ShaderNodeBsdfToon.webp
   :align: right
   :alt: Toon BSDF node.

:guilabel:`Cycles Only`

The *Toon* :abbr:`BSDF (Bidirectional Scattering Distribution Function)`
is used to create *Diffuse* and *Glossy* materials with cartoon light effects.


Inputs
======

Color
   Color of the surface, or physically speaking, the probability that light is reflected for each wavelength.
Size
   Parameter between 0.0 and 1.0 that gives an angle of reflection between 0° and 90°.
Smooth
   This value specifies an angle over which a smooth transition from full to no reflection happens.
Normal
   Normal used for shading; if nothing is connected the default shading normal is used.


Properties
==========

Component
   The material component to base the toon effect.

   :Diffuse: Use shading based on the Diffuse BSDF.
   :Glossy: Use shading based on the Glossy BSDF for specular reflection.


Outputs
=======

BSDF
   Standard shader output.


Examples
========

.. figure:: /images/render_shader-nodes_shader_toon_example.jpg

   Example of Toon Shader.


## Translucent

.. _bpy.types.ShaderNodeBsdfTranslucent:

****************
Translucent BSDF
****************

.. figure:: /images/node-types_ShaderNodeBsdfTranslucent.webp
   :align: right
   :alt: Translucent BSDF node.

The *Translucent* :abbr:`BSDF (Bidirectional Scattering Distribution Function)`
is used to add Lambertian diffuse transmission.


Inputs
======

Color
   Color of the surface, or physically speaking, the probability that light is transmitted for each wavelength.
Normal
   Normal used for shading; if nothing is connected the default shading normal is used.


Properties
==========

This node has no properties.


Outputs
=======

BSDF
   Standard shader output.


Examples
========

.. list-table::
   :widths: auto

   * - .. figure:: /images/render_shader-nodes_shader_translucent_example.jpg

          Translucent shader example.

     - .. figure:: /images/render_shader-nodes_shader_translucent_behavior.svg
          :width: 308px

          Translucent shader behavior.


## Transparent

.. _bpy.types.ShaderNodeBsdfTransparent:

****************
Transparent BSDF
****************

.. figure:: /images/node-types_ShaderNodeBsdfTransparent.webp
   :align: right
   :alt: Transparent BSDF node.

The *Transparent* :abbr:`BSDF (Bidirectional Scattering Distribution Function)`
is used to add transparency without refraction, passing straight through the surface,
as if there were no geometry there. Useful with alpha maps, for example.
This shader :ref:`affects light paths somewhat differently <render-cycles-light-paths-transparency>`
than other BSDFs.
Note that only pure white transparent shaders are completely transparent.


Inputs
======

Color
   Color of the surface, or physically speaking,
   the probability for each wavelength that light is blocked or passes straight through the surface.


Properties
==========

This node has no properties.


Outputs
=======

BSDF
   Standard shader output.


Examples
========

.. list-table::
   :widths: auto

   * - .. figure:: /images/render_shader-nodes_shader_transparent_example.jpg

          Transparent shader (pure white).

     - .. figure:: /images/render_shader-nodes_shader_transparent_behavior.svg
          :width: 308px

          Transparent shader behavior.

   * - .. figure:: /images/render_shader-nodes_shader_transparent_example-dark.jpg

          Transparent shader (gray).

     - ..


## Volume Absorption

.. _bpy.types.ShaderNodeVolumeAbsorption:

*****************
Volume Absorption
*****************

.. figure:: /images/node-types_ShaderNodeVolumeAbsorption.webp
   :align: right
   :alt: Volume Absorption node.

The *Volume Absorption* node allows light to be absorbed as it passes through the volume.
Typical usage for this node would be water and colored glass.


Inputs
======

Color
   Color of the volume.
Density
   The density of the absorption effect.


Properties
==========

This node has no properties.


Outputs
=======

Volume
   The Volume Shader output must be plugged into the *Volume Input*
   of the :doc:`Material </render/shader_nodes/output/material>`
   or :doc:`World </render/shader_nodes/output/world>` Output node.


Examples
========

.. figure:: /images/render_shader-nodes_shader_volume-absorption_example.png

   Example of Volume Absorption.


## Volume Principled

.. _bpy.types.ShaderNodeVolumePrincipled:

*****************
Principled Volume
*****************

.. figure:: /images/node-types_ShaderNodeVolumePrincipled.webp
   :align: right
   :alt: Principled Volume node.

The *Principled Volume* shader combines all volume shading components into
a single easy to use node. Volumes like smoke and fire can be rendered with
a single shader node, which includes scattering, absorption and blackbody emission.


Inputs
======

Color
   Volume scattering color.
Color Attribute
   Volume grid for coloring the volume. Use "color" for smoke simulations.
Density
   Density of the volume.
Density Attribute
   Volume grid to define the density, typically "density".
Anisotropy
   Backward or forward scattering direction.
Absorption Color
   Volume shadow color tint.
Emission Strength
   Amount of light to emit.
Emission Color
   Emission color tint.
Blackbody Intensity
   Blackbody emission for fire. Set to 1 for physically accurate intensity.
Blackbody Tint
   Color tint for blackbody emission.
Temperature
   Temperature in kelvin for blackbody emission, higher values emit more.
Temperature Attribute
   Volume grid to define the temperature, typically "temperature".


Properties
==========

This node has no properties.


Outputs
=======

Volume
   Standard shader output.


Examples
========

.. figure:: /images/render_materials_components_volume_principled.jpg


## Volume Scatter

.. _bpy.types.ShaderNodeVolumeScatter:

**************
Volume Scatter
**************

.. figure:: /images/node-types_ShaderNodeVolumeScatter.webp
   :align: right
   :alt: Volume Scatter node.

The *Volume Scatter* node allows light to be scattered as it passes through the volume.
Typical usage would be to add fog to a scene. It can also be used with
the :doc:`Volume Absorption </render/shader_nodes/shader/volume_absorption>`
node to create smoke.


Inputs
======

Color
   Color of the volume.
Density
   The density of the scatter effect.
Anisotropy
   Controls the look of the scatter effect depending on the direction of the light passing through it.


Properties
==========

Volume
   The Volume Shader output must be plugged into the *Volume Input*
   of the :doc:`Material </render/shader_nodes/output/material>`
   or :doc:`World </render/shader_nodes/output/world>` Output node.


Examples
========

.. figure:: /images/render_shader-nodes_shader_volume-scatter_example.png

   Example of Volume Scatter.


## Brick

.. _bpy.types.ShaderNodeTexBrick:

******************
Brick Texture Node
******************

.. figure:: /images/node-types_ShaderNodeTexBrick.webp
   :align: right
   :alt: Brick Texture node.

The *Brick Texture* is used to add a procedural texture producing bricks.


Inputs
======

Color 1/2
   Color of the bricks.
Mortar
   The color of the area between bricks.
Scale
   Overall texture scale.
Mortar Size
   The size of the filling between the bricks known as "mortar"; 0 means no mortar.
Mortar Smooth
   Blurs/softens the edge between the mortar and the bricks.
   This can be useful with a texture and displacement textures.
Bias
   The color variation between *Color 1/2*.
   Values of -1 and 1 only use one of the two colors; values in between mix the colors.
Brick Width
   The ratio of brick's width relative to the texture scale.
Row Height
   The ratio of brick's row height relative to the texture scale.


Properties
==========

Offset
   Determines the brick offset of the various rows.
Frequency
   How often rows are offset; a value of 2 gives an even/uneven pattern of rows.

Squash
   Factor to adjust the brick's width for particular rows determined by the *Frequency*
Frequency
   How often rows consist of "squished" bricks.


Outputs
=======

Color
   Texture color output.
Factor
   Mortar mask (1 = mortar).


Examples
========

.. figure:: /images/render_shader-nodes_textures_brick_example.jpg
   :width: 200px

   Brick texture: Colors changed, Squash 0.62, Squash Frequency 3.


## Checker

.. _bpy.types.ShaderNodeTexChecker:

********************
Checker Texture Node
********************

.. figure:: /images/node-types_ShaderNodeTexChecker.webp
   :align: right
   :alt: Checker Texture Node.

The *Checker Texture* is used to add a checkerboard texture.


Inputs
======

Vector
   Texture coordinate to sample texture at;
   defaults to Generated texture coordinates if the socket is left unconnected.

   .. warning::

      This node can have precision issues with some vector inputs.
      See the notes for the :doc:`White Noise Texture </render/shader_nodes/textures/white_noise>`
      for ways to mitigate this issue.

Color1, Color 2
   Color of the checkers.
Scale
   Overall texture scale. The scale is a factor of the bounding box of the face divided by the scale.
   For example, a scale of 15 will result in 15 alternate patterns over the overall UV bounding box.
   Different patterns could be achieved using other nodes to give different input patterns to this socket.
   For example, using the Math Node.


Properties
==========

This node has no properties.


Outputs
=======

Color
   Texture color output.
Factor
   Checker 1 mask (1 = Checker 1).


Examples
========

.. figure:: /images/render_shader-nodes_textures_checker_example.jpg
   :width: 200px

   Default Checker texture.


## Environment

.. _bpy.types.ShaderNodeTexEnvironment:

************************
Environment Texture Node
************************

.. figure:: /images/node-types_ShaderNodeTexEnvironment.webp
   :align: right
   :alt: Environment Texture Node.

The Node *Environmental Texture* is used to light your scene using an environment map image file as a texture.


Inputs
======

Vector
   Texture coordinate for texture look-up. If this socket is left unconnected,
   the image is mapped as environment with the Z axis as up.


Properties
==========

Image
   Image data-block used as the image source.
   Additional settings can be found in :menuselection:`Sidebar --> Item --> Properties`:
   These include options to control the alpha channel along with addition options for the color space.
   These addition options are documented with the rest of
   :ref:`Common Image Settings <editors-image-image-settings-common>`.
Color Space
   Type of data that the image contains, either Color or Non-Color Data.
   For most color textures the default of Color should be used, but in case of e.g. a bump or alpha map,
   the pixel values should be interpreted as Non-Color Data, to avoid doing any unwanted color space conversions.

   The list of color spaces depends on the active :ref:`OCIO config <ocio-config>`.
   The default supported color spaces are described in detail here:
   :ref:`Default OpenColorIO Configuration <ocio-config-default-color-spaces>`

Texture Interpolation
   Interpolation method used for the environment texture. The following interpolations are available:

   .. same as in the Image Texture node

   :Linear: Regular quality interpolation.
   :Closest: No interpolation, use closest pixel.
   :Cubic: Smoother, better quality interpolation.
   :Smart: Bicubic when magnifying, otherwise Bilinear is used.
      This is only available for :doc:`OSL </render/shader_nodes/osl>`.

Projection Method
   Allows you to use different types of environmental maps. The following methods are supported:

   :Equirectangular: Projection from an Equirectangular photo.
   :Mirror Ball: Projection from an orthographic photo or mirror ball.


Outputs
=======

Color
   RGB color from the image.


Examples
========

.. figure:: /images/render_shader-nodes_textures_environment_example.jpg
   :width: 200px

   HDR image from `OpenFootage.net <https://www.openfootage.net/?p=986>`__.


## Gradient

.. _bpy.types.ShaderNodeTexGradient:

*********************
Gradient Texture Node
*********************

.. figure:: /images/node-types_ShaderNodeTexGradient.webp
   :align: right
   :alt: Gradient Texture Node.

The *Gradient Texture* node generates interpolated color and intensity values based on the input vector.


Inputs
======

Vector
   Texture coordinate to sample texture at;
   defaults to Generated texture coordinates if the socket is left unconnected.


Properties
==========

Type
   Controls the type of gradient generated.

   :Linear: Directly outputs the input X coordinate.
   :Quadratic: Interpolates the input X coordinate quadratically.
   :Easing: Uses a combination of quadratic and linear interpolation
      to generate a smooth gradient from the input X coordinate.
   :Diagonal: Averages the input X and Y coordinates.
   :Spherical: Creates an inverse gradient using the length of the input vector; the maximum value is at (0, 0, 0).
   :Quadratic Sphere: The same as Spherical, except interpolated quadratically.
   :Radial: Outputs a value based on the angle of the input around the Z axis.


Outputs
=======

Color
   Texture color output.
Factor
   Texture intensity output.


Examples
========

.. figure:: /images/render_shader-nodes_textures_gradient_example.jpg
   :width: 200px

   Gradient texture using object coordinates.


## Ies

.. _bpy.types.ShaderNodeTexIES:

****************
IES Texture Node
****************

.. figure:: /images/node-types_ShaderNodeTexIES.webp
   :align: right
   :alt: Gradient Texture Node.

The *IES Texture* is used to match real world lights based on IES files
(:abbr:`IES (Illuminating Engineering Society of North America)`).
IES files store the directional intensity distribution of light sources.


Inputs
======

Vector
   Texture coordinate for lookup in the light distribution.
   Defaults to the normal.
Strength
   Light strength multiplier.


Properties
==========

Mode
   The location to load the IES file from.

   :Internal: Use IES profile from a file embedded in a text data-block in the blend-file, for easy distribution.
   :External: Load IES profile from a file on the drive.


Outputs
=======

Factor
   Light intensity, typically plugged into the Strength input of an Emission node.


Examples
========

.. figure:: /images/render_shader-nodes_textures_ies_example.jpg

   Lights with different IES profiles.


## Image

.. _bpy.types.ShaderNodeTexImage:

******************
Image Texture Node
******************

.. figure:: /images/node-types_ShaderNodeTexImage.webp
   :align: right
   :alt: Image Texture Node.

Used for applying an image as a texture.


Inputs
======

Vector
   3D coordinate that's projected onto the 2D image using the selected *Projection* method.
   The node then outputs the color and alpha at this projected point.

   This slot is usually connected to an output of the :doc:`/render/shader_nodes/input/texture_coordinate`.
   If left unconnected, the coordinate is taken from the object's active UV map (with Z = 0).


Properties
==========

Image
   Image data-block to use.

Interpolation
   Method to scale images up or down for rendering.

   .. same as in the Environment Texture node

   :Linear: Regular quality interpolation.
   :Cubic: Smoother, better quality interpolation. Bump maps should use this for best results.
   :Closest: No interpolation (nearest neighbor). Useful for rendering pixel art.
   :Smart: :guilabel:`Cycles Only`
      Only for Open Shading Language. Use cubic interpolation when scaling up and linear when scaling down,
      for better performance and sharpness.

Projection
   How to project *Vector* onto the image for arriving at a color.

   :Flat:
      Place the image in a unit square (stretching from (0, 0, 0) to (1, 1, 0))
      and project the *Vector* vertically onto it. This projection is typically used in combination
      with UV maps.
   :Box:
      Place the image on each side of a unit cube (stretching from (0, 0, 0) to (1, 1, 1))
      and project the *Vector* onto this cube, along the axis that's closest to the mesh normal.
      This projection is commonly used in architectural models considering these have lots of
      box-shaped objects.

      Blend
         Rather than projecting onto just one side (which creates sharp transitions), project onto
         multiple sides and blend the results together. The higher the value, the more blending and
         the smoother the result.

   :Sphere:
      Wrap the image around a sphere with origin (0.5, 0.5, 0.5), and project the *Vector* from
      this origin onto this sphere. This projection is, of course, perfect for spherical objects
      such as planets, and is also useful for organic objects.
   :Tube:
      Wrap the image around a cylinder with base (0.5, 0.5, 0) and height 1, and project the
      *Vector* horizontally from the central axis onto this cylinder. This projection is useful for
      a label on a bottle, for example. However, it's not suited for the top or bottom side of objects.

   .. list-table::
      Projections demonstrated using "Object" texture coordinates

      * - .. figure:: /images/render_shader-nodes_textures_image_projection-flat.png

             Flat projection

        - .. figure:: /images/render_shader-nodes_textures_image_projection-box.png

             Box projection

      * - .. figure:: /images/render_shader-nodes_textures_image_projection-sphere.png

             Sphere projection

        - .. figure:: /images/render_shader-nodes_textures_image_projection-tube.png

             Tube projection

Extension
   How the image is extrapolated if *Vector* lies outside the regular (0, 0, 0) to (1, 1, 1) bounds:

   :Repeat: Repeat the image horizontally and vertically (tiling).
   :Extend: Extend the image by repeating the pixels on its edges.
   :Clip: Clip to the original image size and set all the exterior pixels values to transparent black.
   :Mirror: Repeatedly flip the image horizontally and vertically.

Source
   Type of image (Single Image, Movie...). See :doc:`/editors/image/image_settings`.

Frames
   How many frames of the Movie-type image (video) to play. Past this point, the video will be paused
   (unless *Cyclic* is enabled).

   If you want to play the whole video, you can click
   :ref:`Match Movie Length <bpy.ops.image.match_movie_length>` in the Image Editor's Sidebar,
   then copy the *Frames* from there to the node.

Start Frame
   Scene frame at which the video should start playing.

Offset
   Number of frames to offset the video to an earlier point in time.
   (Put differently: how many frames at the start of the video to skip.)

   .. hint::

      Blender plays video textures at the scene framerate, not their original framerate,
      meaning they'll be faster or slower than intended if these framerates don't match up.
      You can put a :doc:`Driver </animation/drivers/introduction>` on the Offset to work
      around this. Simply type the following into the field, replacing *StartFrame*,
      *VideoFrameRate* and *SceneFrameRate* by their respective numbers:

      #(frame - StartFrame) * (VideoFrameRate - SceneFrameRate) / SceneFrameRate

Cyclic
   Start over after the last frame to create a continuous loop.

Auto Refresh
   Update the video texture in the 3D Viewport when moving through the timeline.

Color Space
   The :term:`Color Space` the image file was saved in.
   See :ref:`Image Settings <bpy.types.ColorManagedInputColorspaceSettings.name>` for details.

Alpha
   How the image uses its :term:`Alpha Channel`.
   See :ref:`Image Settings <bpy.types.Image.alpha_mode>` for details.


Outputs
=======

Color
   RGB color from image. If the image has transparency, the color is premultiplied if the Alpha output is used,
   and unpremultiplied (straight) otherwise.
Alpha
   Alpha channel from image.


Examples
========

.. figure:: /images/render_shader-nodes_textures_image_example.jpg

   Image texture from `GoodTextures.com <https://www.goodtextures.com/>`__.


## Index

.. _textures:

###########
  Texture
###########

.. toctree::
   :maxdepth: 1

   brick.rst
   checker.rst
   environment.rst
   gradient.rst
   ies.rst
   image.rst
   magic.rst
   musgrave.rst
   noise.rst
   point_density.rst
   sky.rst
   voronoi.rst
   wave.rst
   white_noise.rst


## Magic

.. _bpy.types.ShaderNodeTexMagic:

******************
Magic Texture Node
******************

.. figure:: /images/node-types_ShaderNodeTexMagic.webp
   :align: right
   :alt: Magic Texture Node.

The Magic Texture node is used to add a psychedelic color texture.
It can be used for "Thin Film Interference" if you assign a *Reflection* Texture Coordinate
to the Vector input and use a relatively high *Turbulence*.
The RGB components are generated independently with a sine formula.


Inputs
======

Vector
   Texture coordinate to sample texture at;
   defaults to Generated texture coordinates if the socket is left unconnected.
Scale
   Scale of the texture.
Distortion
   Amount of distortion.


Properties
==========

Depth
   Number of iterations.


Outputs
=======

Color
   Texture color output.
Factor
   Texture intensity output.


Examples
========

.. figure:: /images/render_shader-nodes_textures_magic_example.jpg
   :width: 200px

   Magic texture: Depth 10, Distortion 2.0.


## Musgrave

.. _bpy.types.ShaderNodeTexMusgrave:

*********************
Musgrave Texture Node
*********************

The Musgrave texture node was replaced by the
:doc:`Noise Texture </render/shader_nodes/textures/noise>` node,
which includes all the same functionality.

* The Dimension input was replaced by a Roughness input, where :math:`Roughness = Lacunarity^{-Dimension}`.
* The Detail input value must be subtracted by 1 compared to the old Musgrave Texture node.



## Noise

.. _bpy.types.ShaderNodeTexNoise:

******************
Noise Texture Node
******************

.. figure:: /images/node-types_ShaderNodeTexNoise.webp
   :align: right
   :alt: Noise Texture Node.

The *Noise Texture* node evaluates a fractal Perlin noise at the input texture coordinates.
It can be used for a single Perlin noise evaluation, or for combining multiple octaves
(layers) with increasingly finer detail.


Inputs
======

The inputs are dynamic: they become available if needed depending on the node properties.

Vector
   Texture coordinate to evaluate the noise at;
   defaults to *Generated* texture coordinates if the socket is left unconnected.
W
   Texture coordinate to evaluate the noise at.
Scale
   Scale of the base noise octave.
Detail
   Number of noise octaves. This can have a fractional part, in which case a blend
   is performed (e.g. a Detail of 2.5 results in a 50% blend between 2 and 3 octaves).
Roughness
   Blend between a smoother noise pattern, and rougher with sharper peaks.
Lacunarity
   The difference between the scale of each two consecutive octaves.
   Larger values corresponds to larger scale for higher octaves.
Offset
   An added offset to each octave, determines the level where the highest octave will appear.
Gain
   An extra multiplier to tune the magnitude of octaves.
Distortion
   Amount of distortion.


Properties
==========

Dimensions
   The dimensions of the space to evaluate the noise in.

   :1D: Evaluate the noise in 1D space at the input *W*.
   :2D: Evaluate the noise in 2D space at the input *Vector*. The Z component is ignored.
   :3D: Evaluate the noise in 3D space at the input *Vector*.
   :4D: Evaluate the noise in 4D space at the input *Vector* and the input *W* as the fourth dimension.

   .. note::

      Higher dimensions corresponds to higher render time,
      so lower dimensions should be used unless higher dimensions are necessary.

Type
   Type of Noise texture, with different ways to combine octaves.

   :fBM:
      Fractal Brownian motion, produces a homogeneous and isotropic result.
      Values from octaves are added together.
   :Multifractal:
      More uneven, varying by location similar to real terrain.
      Values from octaves are multiplied together.
   :Hybrid Multifractal:
      Creates peaks and valleys with different roughness values, like real mountains rise out of flat plains.
      Combines octaves using both addition and multiplication.
   :Ridged Multifractal:
      Creates sharp peaks. Calculates the absolute value of the noise,
      creating "canyons", and then flips the surface upside down.
   :Hetero Terrain:
      Similar to *Hybrid Multifractal* creates a heterogeneous terrain, but with the likeness of river channels.

Normalize :guilabel:`fBM`
   If enabled, ensures that the output values stay in the range 0.0 to 1.0.
   If disabled, the range is at most -(*Detail* + 1) to *Detail* + 1 (smaller if *Roughness* < 1).

Outputs
=======

Factor
   Value of fractal noise.
Color
   Color with different fractal noise in each component.


Examples
========

.. figure:: /images/render_shader-nodes_textures_noise_example.jpg

   Noise Texture with high detail.

.. list-table:: Different Noise types with the same parameters.

   * - .. figure:: /images/render_shader-nodes_textures_musgrave_example-type-fbm.jpg
          :width: 320px

          fBM (fractal Brownian Motion).

     - .. figure:: /images/render_shader-nodes_textures_musgrave_example-type-multifractal.jpg
          :width: 320px

          Multifractal.

   * - .. figure:: /images/render_shader-nodes_textures_musgrave_example-type-hybrid.jpg
          :width: 320px

          Hybrid Multifractal.

     - .. figure:: /images/render_shader-nodes_textures_musgrave_example-type-terrain.jpg
          :width: 320px

          Heterogeneous Terrain.

   * - .. figure:: /images/render_shader-nodes_textures_musgrave_example-type-ridged.jpg
          :width: 320px

          Ridged Multifractal.

     - ..


Notes
=====

While the noise is random in nature, it follows a certain pattern that might not evaluate to
random values in some configurations. For instance, consider the following configuration
where a grid of objects have a material that evaluates a noise texture at their locations.
One might expect the objects to have random values since they have different locations,
but this is not the case.

.. figure:: /images/render_shader-nodes_textures_noise_issue-constant-value.png

   An example configuration where the noise evaluates to a constant value.

It seems all objects have a value of 0.5. To understand why this happens, let us
look at the following plot of a 1D noise texture.

.. figure:: /images/render_shader-nodes_textures_noise_1d-noise-plot.png

   A plot of a 1D noise with zero details and zero distortion.

The horizontal line denotes a value of 0.5 and the vertical lines denotes whole numbers assuming
a noise scale of 1. As can be seen, the noise always intersects the 0.5 line at whole numbers.
Since the aforementioned objects were distributed on a grid and have whole number locations,
they all evaluate to 0.5. Which explains the issue at hand.

Generally, any discrete evaluation of noise at integer multiples of the reciprocal of
the noise scale will always evaluate to 0.5. It also follows that evaluations closer to
that will have values close to 0.5. In such cases, it is almost always preferred to use
the White Noise Texture.

Regardless, one can mitigate this issue in a number of ways:

- Adjust the scale of the noise to avoid aligning the noise with the evaluation domain.
- Add an arbitrary offset to the texture coordinates to break the alignment with the evaluation domain.
- Evaluate the noise at a higher dimension and adjust the extra dimension
  until a satisfactory result is achieved.

.. list-table::

   * - .. figure:: /images/render_shader-nodes_textures_noise_issue-constant-value.png

          Constant value issue.

     - .. figure:: /images/render_shader-nodes_textures_noise_solution1-constant-value.png

          Mitigating the issue by adjusting the scale.

   * - .. figure:: /images/render_shader-nodes_textures_noise_solution2-constant-value.png

          Mitigating the issue by adding an arbitrary offset.

     - .. figure:: /images/render_shader-nodes_textures_noise_solution3-constant-value.png

          Mitigating the issue by evaluating at a higher dimension.

Similarly, in other configurations, one might experience some banding patterns in the noise,
where there are bands of high contrast areas followed by banding of low contrast areas.
For instance, planar surfaces that are slightly tilted along one of the axis
will have such a banding pattern.

.. figure:: /images/render_shader-nodes_textures_noise_issue-banding.png

   An example configuration where the noise have a banding pattern.

This happens because the slight tilt along one of the axis causes values along
the perpendicular axis to change very slowly making the grid structure of
the noise more apparent. The easiest way to mitigate this issue to rotate
the coordinates by an arbitrary amount.

.. figure:: /images/render_shader-nodes_textures_noise_solution-banding.png

   Mitigating the issue by rotating the coordinates by an arbitrary amount.


## Point Density

.. _bpy.types.ShaderNodeTexPointDensity:

******************
Point Density Node
******************

.. figure:: /images/node-types_ShaderNodeTexPointDensity.webp
   :align: right
   :alt: Point Density Node.

The *Point Density* node is available in volume shaders,
to render volumetric points for each particle or vertex of another object.


Inputs
======

Vector
   Texture coordinate to sample texture at;
   defaults to global position (*Position* output of *Geometry* node) if the socket is left unconnected.


Properties
==========

Point Data
   Where to get points from.

   Particle System
      Use each particle position from the specified particle system.
   Object Vertices
      Use each vertex position from the specified object.
Object
   Which object's vertices or particle system will be used.
Particle System
   Particle positions from this system will be used.
Space
   The coordinate system for mapping points.

   :World Space: Map each point exactly where the source particle or vertex is.
   :Object Space:
      Fit the points from the source particles/vertices
      inside the bounding box of the object with the point density texture.

Radius
   Size of the points.

Interpolation
   Texel filtering type.

   :Closest: No interpolation, use nearest texel. Produces blocky looking points.
   :Linear: Interpolate linearly between texels, producing soft, round points.
   :Cubic: Use cubic falloff, producing very soft points. Useful when points are very densely packed.
Resolution
   The dimensions of the texture holding the point data.
Color Source
   Which attribute of the particle system or mesh is used to color the output.

   Particle Color Sources
      :Particle Age: Lifetime mapped as (0.0 - 1.0) intensity.
      :Particle Speed: Particle speed (absolute magnitude of velocity) mapped as (0.0 - 1.0) intensity.
      :Particle Velocity: XYZ velocity mapped to RGB colors.
   Vertex Color Sources
      :Vertex Color:
         Use a Color Attribute for coloring the point density texture.

         .. note::

            Color Attributes are defined per face corner.
            A single vertex can have as many different colors as faces it is part of.
            The actual color of the point density texture is averaged from all vertex corners.

      :Vertex Weight: Use weights from a vertex group as intensity values.
      :Vertex Normals: Use object-space vertex normals as RGB values.


Outputs
=======

Color
   Texture color output.
Density
   Density of volume.


Examples
========

.. figure:: /images/render_shader-nodes_textures_point-density_example.jpg
   :width: 200px

   Domain object with Point Density texture using vertices from ball as points.


## Sky

.. _bpy.types.ShaderNodeTexSky:

****************
Sky Texture Node
****************

.. figure:: /images/node-types_ShaderNodeTexSky.webp
   :align: right
   :alt: Sky Texture Node.

The *Sky Texture* node adds a procedural Sky texture.


Inputs
======

Vector
   Texture coordinate to sample texture at;
   defaults to Generated texture coordinates if the socket is left unconnected.


Properties
==========

Sky Type
   Sky model to use.

   Preetham
      Based on the 1999 `paper <https://doi.org/10.1145/311535.311545>`__ by Preetham et al.
   Hosek/Wilkie
      Based on the 2012 `paper <https://cgg.mff.cuni.cz/projects/SkylightModelling/>`__ by Hosek and Wilkie.
   Nishita
      Improved version of the 1993
      `model <https://www.scratchapixel.com/lessons/procedural-generation-virtual-worlds/simulating-sky/simulating-
      colors-of-the-sky.html>`__
      by Nishita et al.

      Note that this sky type is quite bright and makes the image look overexposed with the default scene settings.
      You can reduce the Exposure setting in :menuselection:`Properties --> Render --> Film` to fix this.

Sun Direction
   Sun direction vector.

Turbidity
   Atmospheric turbidity.

   - 2: Arctic like
   - 3: clear sky
   - 6: warm/moist day
   - 10: hazy day

Ground Albedo
   Amount of light reflected from the planet surface back into the atmosphere.

Sun Disc :guilabel:`Cycles Only`
   Enable/Disable sun disc lighting.

Sun Size
   Angular diameter of the sun disc (in degrees).

Sun Intensity
   Multiplier for sun disc lighting.

Sun Elevation
   Rotation of the sun from the horizon (in degrees).

Sun Rotation
   Rotation of the sun around the zenith (in degrees).

Altitude
   The distance from sea level to the location of the camera.
   For example, if the camera is placed on a beach then a value of 0 should be used.
   However, if the camera is in the cockpit of a flying airplane then a value of 10 km will be more suitable.
   Note, this is limited to 60 km because the mathematical model only accounts
   for the first two layers of the earth's atmosphere (which ends around 60 km).

Air
   Density of air molecules.

   - 0 no air
   - 1 clear day atmosphere
   - 2 highly polluted day

Dust
   Density of dust and water droplets.

   - 0 no dust
   - 1 clear day atmosphere
   - 5 city like atmosphere
   - 10 hazy day

Ozone
   Density of ozone molecules;
   useful to make the sky appear bluer.

   - 0 no ozone
   - 1 clear day atmosphere
   - 2 city like atmosphere


Outputs
=======

Color
   Texture color output.


Examples
========

.. figure:: /images/render_shader-nodes_textures_sky_example.jpg
   :width: 200px

   Example of Sky Texture.


## Voronoi

.. _bpy.types.ShaderNodeTexVoronoi:

********************
Voronoi Texture Node
********************

.. figure:: /images/node-types_ShaderNodeTexVoronoi.webp
   :align: right
   :alt: Voronoi Texture Node.

The *Voronoi Texture* node evaluates a `Worley Noise <https://en.wikipedia.org/wiki/Worley_noise>`__ at
the input texture coordinates.


Inputs
======

The inputs are dynamic, they become available if needed depending on the node properties.

Vector
   Texture coordinate to evaluate the noise at;
   defaults to *Generated* texture coordinates if the socket is left unconnected.
W
   Texture coordinate to evaluate the noise at.
Scale
   Scale of the noise.
Detail
   Number of noise octaves.
   The fractional part of the input is multiplied by the magnitude of the highest octave.
   Higher number of octaves corresponds to a higher evaluation time.
Roughness
   Blend between a smoother noise pattern, and rougher with sharper peaks.
Lacunarity
   The difference between the scale of each two consecutive octaves.
   Larger values corresponds to larger scale for higher octaves.
Smoothness
   The smoothness of the noise.

   .. list-table::

      * - .. figure:: /images/render_shader-nodes_textures_voronoi_smoothness-distance-zero.png

             Smoothness: 0.0.

        - .. figure:: /images/render_shader-nodes_textures_voronoi_smoothness-distance-quarter.png

             Smoothness: 0.25.

        - .. figure:: /images/render_shader-nodes_textures_voronoi_smoothness-distance-half.png

             Smoothness: 0.5.

        - .. figure:: /images/render_shader-nodes_textures_voronoi_smoothness-distance-one.png

             Smoothness: 1.0.

      * - .. figure:: /images/render_shader-nodes_textures_voronoi_smoothness-color-zero.png

             Smoothness: 0.0.

        - .. figure:: /images/render_shader-nodes_textures_voronoi_smoothness-color-quarter.png

             Smoothness: 0.25.

        - .. figure:: /images/render_shader-nodes_textures_voronoi_smoothness-color-half.png

             Smoothness: 0.5.

        - .. figure:: /images/render_shader-nodes_textures_voronoi_smoothness-color-one.png

             Smoothness: 1.0.

Exponent
   Exponent of the Minkowski distance metric.

   .. list-table::

      * - .. figure:: /images/render_shader-nodes_textures_voronoi_minkowski-half.png

             Exponent: 0.5.

        - .. figure:: /images/render_shader-nodes_textures_voronoi_minkowski-one.png

             Exponent: 1.0.

        - .. figure:: /images/render_shader-nodes_textures_voronoi_minkowski-two.png

             Exponent: 2.0.

        - .. figure:: /images/render_shader-nodes_textures_voronoi_minkowski-32.png

             Exponent: 32.0.

Randomness
   The randomness of the noise.

   .. list-table::

      * - .. figure:: /images/render_shader-nodes_textures_voronoi_randomness-one.png

             Randomness: 1.0.

        - .. figure:: /images/render_shader-nodes_textures_voronoi_randomness-half.png

             Randomness: 0.5.

        - .. figure:: /images/render_shader-nodes_textures_voronoi_randomness-quarter.png

             Randomness: 0.25.

        - .. figure:: /images/render_shader-nodes_textures_voronoi_randomness-zero.png

             Randomness: 0.0.


Properties
==========

Dimensions
   The dimensions of the space to evaluate the noise in.

   :1D: Evaluate the noise in 1D space at the input W.
   :2D: Evaluate the noise in 2D space at the input Vector. The Z component is ignored.
   :3D: Evaluate the noise in 3D space at the input Vector.
   :4D: Evaluate the noise in 4D space at the input Vector and the input W as the fourth dimension.

   Higher dimensions corresponds to higher render time,
   so lower dimensions should be used unless higher dimensions are necessary.

Feature
   The Voronoi feature that the node will compute.

   :F1:
      The distance to the closest feature point as well as its position and color.

      .. list-table::

         * - .. figure:: /images/render_shader-nodes_textures_voronoi_smoothness-distance-zero.png

                Distance.

           - .. figure:: /images/render_shader-nodes_textures_voronoi_smoothness-color-zero.png

                Color.

           - .. figure:: /images/render_shader-nodes_textures_voronoi_f1-position.png

                Position.

   :F2:
      The distance to the second closest feature point as well as its position and color.

      .. list-table::

         * - .. figure:: /images/render_shader-nodes_textures_voronoi_f2-distance.png

                Distance.

           - .. figure:: /images/render_shader-nodes_textures_voronoi_f2-color.png

                Color.

           - .. figure:: /images/render_shader-nodes_textures_voronoi_f2-position.png

                Position.

   :Smooth F1:
      A smooth version of F1.

      .. list-table::

         * - .. figure:: /images/render_shader-nodes_textures_voronoi_smoothness-distance-one.png

                Distance.

           - .. figure:: /images/render_shader-nodes_textures_voronoi_smoothness-color-one.png

                Color.

           - .. figure:: /images/render_shader-nodes_textures_voronoi_smooth-f1-position.png

                Position.

   :Distance to Edge:
      The distance to the edges of the Voronoi cells.

      .. list-table::

         * - .. figure:: /images/render_shader-nodes_textures_voronoi_distance-to-edge.png

                Distance.

           - .. figure:: /images/render_shader-nodes_textures_voronoi_distance-to-edge-less-than.png

                Distance smaller than 0.05.

   :N-Sphere Radius:
      The radius of the n-sphere inscribed in the Voronoi cells.
      In other words, it is half the distance between the closest feature point and the feature point closest to it.

      .. list-table::

         * - .. figure:: /images/render_shader-nodes_textures_voronoi_n-sphere-radius.png

                The n-sphere radius can be used to create tightly packed n-spheres.

           - .. figure:: /images/render_shader-nodes_textures_voronoi_n-sphere-radius-nodetree.png

                Node tree for the shader to the left.

Distance Metric
   The distance metric used to compute the texture.

   :Euclidean:
      Use the `Euclidean distance metric <https://en.wikipedia.org/wiki/Euclidean_distance>`__.
   :Manhattan:
      Use the `Manhattan distance metric <https://en.wikipedia.org/wiki/Taxicab_geometry>`__.
   :Chebychev:
      Use the `Chebychev distance metric <https://en.wikipedia.org/wiki/Chebyshev_distance>`__.
   :Minkowski:
      Use the `Minkowski distance metric <https://en.wikipedia.org/wiki/Minkowski_distance>`__.
      The Minkowski distance is a generalization of the aforementioned metrics with an *Exponent* as a parameter.
      Minkowski with an exponent of one is equivalent to the *Manhattan* distance metric.
      Minkowski with an exponent of two is equivalent to the *Euclidean* distance metric.
      Minkowski with an infinite exponent is equivalent to the *Chebychev* distance metric.

   .. list-table::

      * - .. figure:: /images/render_shader-nodes_textures_voronoi_minkowski-half.png

             Minkowski Exponent: 0.5 (Minkowski 1/2).

        - .. figure:: /images/render_shader-nodes_textures_voronoi_minkowski-one.png

             Minkowski Exponent: 1.0 (Manhattan).

        - .. figure:: /images/render_shader-nodes_textures_voronoi_minkowski-two.png

             Minkowski Exponent: 2.0 (Euclidean).

        - .. figure:: /images/render_shader-nodes_textures_voronoi_minkowski-32.png

             Minkowski Exponent: 32.0 (approximation of Chebychev).

Normalize
   If enabled, ensures that the output values stay in the range 0.0 to 1.0.
   In rare cases, the output value may be outside that range when *Feature* is *F2*.


Outputs
=======

Distance
   Distance.
Color
   Cell color. The color is arbitrary.
Position
   Position of feature point.
W
   Position of feature point.
Radius
   N-Sphere radius.


Notes
=====

In some configurations of the node, especially for low values of *Randomness*,
rendering artifacts may occur. This happens due to the same reasons described
in the :ref:`Notes section <shader-white-noise-notes>` in the White Noise Texture page
and can be fixed in a similar manner as described there.


Examples
========

.. figure:: /images/render_shader-nodes_textures_voronoi_example-beveled-cells.png

   The difference between *F1* and *Smooth F1* can be used to create beveled Voronoi cells.

.. figure:: /images/render_shader-nodes_textures_voronoi_example-hammered-metal.jpg

   Creating a hammered metal shader using the *Voronoi Texture* node.


## Wave

.. _bpy.types.ShaderNodeTexWave:

*****************
Wave Texture Node
*****************

.. figure:: /images/node-types_ShaderNodeTexWave.webp
   :align: right
   :alt: Wave Texture Node.

The *Wave Texture* node adds procedural bands or rings with noise distortion.


Inputs
======

Vector
   Texture coordinate to sample texture at;
   defaults to Generated texture coordinates if the socket is left unconnected.
Scale
   Overall texture scale.
Distortion
   Amount of distortion of the wave.

   .. hint::

      In general, textures can be distorted by mixing their texture coordinates with another texture.
      The distortion built into the *Wave Texture Node* uses the *Color* output of the
      :doc:`Noise Texture Node </render/shader_nodes/textures/noise>`.

      To replicate this, center its value range around zero, multiply it by a factor proportional to
      *Distortion*/*Scale* and add the result onto the texture coordinates.
      *Detail*, *Detail Scale*, and *Roughness* of the *Wave Texture Node* correspond to the inputs on the
      :doc:`Noise Texture Node </render/shader_nodes/textures/noise>`.
Detail
   Amount of distortion noise detail.
Detail Scale
   Scale of distortion noise.
Roughness
   Blend between a smoother noise pattern, and rougher with sharper peaks.
Phase Offset
   Position of the wave along the *Bands Direction*.
   This can be used as an input for more control over the distortion.


Properties
==========

Type
   *Bands* or *Rings* shaped waves.
Bands/Rings Direction
   The axis the bands or rings propagate from i.e. which axis they are perpendicular to.
   When using *Bands* a *Diagonal* axis is an option and when using *Rings* the rings
   can propagate outwards from a single point by using *Spherical* direction.
Wave Profile
   Controls the look of the wave type.

   :Saw: Uses a sawtooth profile.
   :Sine: Uses the standard sine profile.


Outputs
=======

Color
   Texture color output.
Factor
   Texture intensity output.


Examples
========

.. figure:: /images/render_shader-nodes_textures_wave_example.png

   Wave Texture.


## White Noise

.. _bpy.types.ShaderNodeTexWhiteNoise:

************************
White Noise Texture Node
************************

The *White Noise Texture* node returns a random number based on an input :term:`Seed`.
The seed can be a number, a 2D vector, a 3D vector, or a 4D vector; depending on the *Dimensions* property.
The output number ranges between zero and one.

.. figure:: /images/node-types_ShaderNodeTexWhiteNoise.webp
   :align: right
   :alt: White Noise Texture Node.


Inputs
======

The inputs are dynamic, they become available if needed depending on the node properties.

Vector
   Vector used as seed in 2D, 3D, and 4D dimensions.
W
   Value used as seed in 1D and 4D dimensions.


Properties
==========

Dimensions
   The dimensions of the space to evaluate the noise in.

   :1D: The *W* input is used as seed.
   :2D: The X and Y components of the *Vector* input are used as seed.
   :3D: The *Vector* input is used as seed.
   :4D: Both the *Vector* input and the *W* input are used as seed.


Outputs
=======

Value
   Output random value.
Color
   Output random color.


.. _shader-white-noise-notes:

Notes
=====

The slightest difference in seed values would result in completely different outputs.
Consequently, bad precision may have significant impact on the output.
Usually, we can mitigate this issue by:

- Eliminating the problematic seed value. If the problematic seed value is constant,
  it should be eliminated by choosing a lower dimension or multiplying it by zero.
- Adding an arbitrary value to the seed. The issue might only happen at certain boundaries,
  like unit boundaries, so simply adding an arbitrary value might solve the issue.
- Taking the absolute value of the seed. In computing, zero may be positive or negative,
  so taking the absolute values unifies the zero into a single value.

.. list-table::

   * - .. figure:: /images/render_shader-nodes_textures_white-noise_issue.png

          Precision issue due to signed zeros on the Z axis.

     - .. figure:: /images/render_shader-nodes_textures_white-noise_solution1.png

          Mitigating the issue by eliminating the Z axis.

   * - .. figure:: /images/render_shader-nodes_textures_white-noise_solution2.png

          Mitigating the issue by adding an arbitrary value.

     - .. figure:: /images/render_shader-nodes_textures_white-noise_solution3.png

          Mitigating the issue by taking the absolute value.


Examples
========

.. figure:: /images/render_shader-nodes_textures_white-noise_solution1.png

   Generating cell noise using the *Snap* vector operation and the *White Noise* node.


## Bump

.. _bpy.types.ShaderNodeBump:

*********
Bump Node
*********

.. figure:: /images/node-types_ShaderNodeBump.webp
   :align: right
   :alt: Bump Node.

The *Bump* node generates a perturbed normal from a height texture, for bump mapping.
The height value will be sampled at the shading point and two nearby points
on the surface to determine the local direction of the normal.


Inputs
======

Strength
   Strength of the bump mapping effect, interpolating between no bump mapping and full bump mapping.
Distance
   Multiplier for the height value to control the overall distance for bump mapping.
Height
   Scalar value giving the height offset from the surface at the shading point; this is where you plug in textures.
Normal
   Standard normal input.


Properties
==========

Invert
   Invert the bump mapping, to displace into the surface instead of out.


Outputs
=======

Normal
   Standard normal output.

.. tip::

   If the *Height* input is not connected, the node becomes a :doc:`no-op </render/cycles/optimizations/nodes>`
   that outputs its *Normal* input as is, or defaults to the geometry normal if not connected. Routing a node
   group input via a no-op Bump node before doing math effectively makes it default to normal.

Examples
========

.. figure:: /images/render_shader-nodes_vector_bump_node-setup.png

The above node setup will only bump the diffuse part of the shader,
simulating a bumpy diffuse surface coated with a smooth glossy "glaze" layer.

.. figure:: /images/render_shader-nodes_vector_bump_example.jpg


## Curves

.. _bpy.types.ShaderNodeVectorCurve:
.. DO NOT EDIT FILE. This is simply a stub which copies everything from the link below.
.. include:: /compositing/types/vector/vector_curves.rst
   :start-after: .. --- copy below this line ---


## Displacement

.. _bpy.types.ShaderNodeDisplacement:

*****************
Displacement Node
*****************

.. figure:: /images/node-types_ShaderNodeDisplacement.webp
   :align: right
   :alt: Displacement Node.

The *Displacement* node is used to displace the surface along the surface normal,
to add more detail to the geometry. Both procedural textures and baked displacement maps
can be used.

By default, Blender only uses :term:`Bump Mapping` to render displacement.
However with true displacement, the rendered geometry will be physically displaced. To use true displacement
the :ref:`Displacement method <bpy.types.Material.displacement_method>` must be set accordingly.

.. tip::

   For best results when using true displacement,
   the mesh must be subdivided finely to bring out the detail in the displacement texture.

.. seealso::

   :doc:`Material Displacement </render/materials/components/displacement>`
   for more details on displacement workflows.


Inputs
======

Height
   Distance to displace the surface along the normal.
   This is where a texture node can be connected.
Midlevel
   Neutral displacement value that causes no displacement.
   With the default 0.5, any lower values will cause the surfaces to be pushed inwards,
   and any higher values will push them outwards.
Scale
   Increase or decrease the amount of displacement.
Normal
   Standard normal input.


Properties
==========

Space
   Object Space means the displacement scales along with the object.
   When using World Space the object scale is ignored.


Outputs
=======

Displacement
   Displacement offset to be connected into the Material Output.


Examples
========

.. figure:: /images/render_materials_components_displacement_node-setup.png

   Typical displacement node setup.

.. figure:: /images/render_materials_components_displacement_example.jpg

   Bump only, displacement only, and displacement and bump combined.


## Index


##########
  Vector
##########

.. toctree::
   :maxdepth: 1

   bump.rst
   displacement.rst
   mapping.rst
   normal.rst
   normal_map.rst
   curves.rst
   vector_displacement.rst
   vector_rotate.rst
   transform.rst


## Mapping

.. _bpy.types.ShaderNodeMapping:

************
Mapping Node
************

The *Mapping* node transforms the input vector by applying translation, rotation, and scaling.

.. figure:: /images/node-types_ShaderNodeMapping.webp
   :alt: Mapping node.


Inputs
======

The inputs of the node are dynamic. In particular, the *Location* input is only available in
the *Texture* and *Point* vector types.

Vector
   The vector to be transformed.

Location
   The amount of translation along each axis.

Rotation
   The amount of rotation along each axis. XYZ order.

Scale
   The amount of scaling along each axis.


Properties
==========

Vector Type
   The node applies the transformation differently depending on the semantic type of the input vector.

   :Point:
      For this vector type, the node performs a straightforward transformation.

      Transforming a texture coordinates is analogous to transforming a UV map.
      For instance, translating the texture coordinates along the **positive** X axis would result
      in the evaluated texture to move in the **negative** X axis, much like if one translated a UV map.
      Similarly, scaling the texture coordinates **up** would result in the evaluated texture to scale **down**.
      So transforming the texture coordinates would appear to have the opposite effect on the evaluated texture.

      The order of transformation is: Scale --> Rotate --> Translate, which means:

      - Translation moves the input along the local rotation axis.
      - Rotation rotates the input around the origin of the space.
      - Scaling scales the input along the global axis.
   :Texture:
      For this vector type, the node performs an inverse transformation.

      Inverse transforming a texture coordinates would, as opposed to the *Point* type,
      transform the evaluated texture itself. For instance, translating the texture coordinates along
      the positive X axis would result in the evaluated texture to move in the positive X axis,
      as one would expected. Similarly, scaling the texture coordinates up would result in
      the evaluated texture to scale up, as one would expect.

      The order of transformation is: Translate --> Rotate --> Scale, which means:

      - Translation moves the input along the global axis.
      - Rotation rotates the input around the translation vector.
      - Scaling scales the input along the local rotation axis.
   :Vector:
      For this vector type, a *Point* transformation is performed, but with zero translation.
   :Normal:
      For this vector type, the node performs the inverse transpose of the transformation and normalize the result.
      Such transformation ensures correct normals after non-uniform scaling.
      So this type should be used when transforming normals.


Outputs
=======

Vector
   The input vector after transformation.


Examples
========

.. figure:: /images/render_shader-nodes_vector_mapping_example.png

   Mapping node example.


## Normal

.. _bpy.types.ShaderNodeNormal:
.. DO NOT EDIT FILE. This is simply a stub which copies everything from the link below.
.. include:: /compositing/types/vector/normal.rst
   :start-after: .. --- copy below this line ---


## Normal Map

.. _bpy.types.ShaderNodeNormalMap:

***************
Normal Map Node
***************

.. figure:: /images/node-types_ShaderNodeNormalMap.webp
   :align: right
   :alt: Normal Map Node.

The *Normal Map* node generates a perturbed normal from an RGB normal map image.
This is usually chained with an *Image Texture* node in the color input,
to specify the normal map image. For tangent space normal maps,
the UV coordinates for the image must match,
and the image texture should be set to *Non-Color* mode to give correct results.


Inputs
======

Strength
   Strength of the normal mapping effect.

   .. figure:: /images/render_shader-nodes_vector_normal-map_strength.jpg

      Strength is set to 0, 0.5, 1, 2 (from left to right).

Color
   RGB color that encodes the normal map in the specified space.


Properties
==========

Space
   The input RGB color can be in one of three spaces: Tangent, Object and World space.
   Tangent space normal maps are the most common, as they support object transformation and mesh deformations.
   Object space normal maps keep sticking to the surface under object transformations,
   while World normal maps do not.
UV Map
   Name of the UV map to derive normal mapping tangents from. When chained with an Image Texture node,
   this UV map should be the same as the UV map used to map the texture.


Outputs
=======

Normal
   Normal that can be used as an input to BSDF nodes.


Example
=======

.. figure:: /images/render_shader-nodes_vector_normal-map_example.jpg
   :align: center

   The Normal Map Strength is set to 1.


## Transform

.. _bpy.types.ShaderNodeVectorTransform:

*********************
Vector Transform Node
*********************

.. figure:: /images/node-types_ShaderNodeVectorTransform.webp
   :align: right
   :alt: Vector Transform node.

The *Vector Transform* node allows converting a vector, point, or normal between
world and camera and object coordinate space.


Inputs
======

Vector Input
   Standard vector input.


Properties
==========

Type
   Specifies the input/output type.

   Vector, Point, Normal.
Convert From
   Coordinate Space to convert from:

   World, Object, Camera.
Convert To
   Coordinate Space to convert to:

   World, Object, Camera.


Outputs
=======

Vector Output
   The transformed output vector.


Examples
========

Todo <2.8 add.


## Vector Displacement

.. _bpy.types.ShaderNodeVectorDisplacement:

************************
Vector Displacement Node
************************

.. figure:: /images/node-types_ShaderNodeVectorDisplacement.webp
   :align: right
   :alt: Vector Displacement Node.

The *Vector Displacement* node is used to displace the surface along arbitrary directions,
unlike the regular Displacement node which only displaces along the surface normal.

It is typically used to apply vector displacement maps created by other sculpting
software. Vector displacement maps can fully represent the high resolution detail to
be applied on a smooth base mesh, unlike regular displacement maps.

By default, Blender only uses :term:`Bump Mapping` to render displacement.
However with true displacement, the rendered geometry will be physically displaced. To use true displacement
the :ref:`Displacement method <bpy.types.Material.displacement_method>` must be set accordingly.

.. tip::

   For best results when using true displacement,
   the mesh must be subdivided finely to bring out the detail in the displacement texture.

.. seealso::

   :doc:`Material Displacement </render/materials/components/displacement>`
   for more details on displacement workflows.


Inputs
======

Vector
   Vector specifying the displacement along three axes.
   This is where a texture node can be connected.

   Typically a baked vector displacement image texture is used.
   For Object Space, RGB colors in the image are interpreted as an XYZ offset in object space.
   For Tangent Space, R is an offset along the tangent, G along the normal and B along the bitangent.
Midlevel
   Neutral displacement value that causes no displacement.
   With the default 0.0, any lower values will cause the surfaces to be pushed inwards,
   and any higher values will push them outwards.
Scale
   Increase or decrease the amount of displacement.


Properties
==========

Space
   Object Space maps work for static meshes, and will render slightly faster with less memory usage.
   Tangent Space maps can be used for meshes that will be deformed, like animated characters,
   so the displacement follows the deformation.


Outputs
=======

Displacement
   Displacement offset to be connected into the Material Output.


Examples
========

.. figure:: /images/render_shader-nodes_vector_vector-displacement_example.jpg

   Regular and exaggerated vector displacement on a smooth base mesh.


## Vector Rotate

.. _bpy.types.ShaderNodeVectorRotate:

.. Editor's Note: This page gets copied into:
.. - :doc:`</modeling/geometry_nodes/utilities/vector/vector_rotate>`

.. --- copy below this line ---

******************
Vector Rotate Node
******************

.. figure:: /images/node-types_ShaderNodeVectorRotate.png
   :align: center
   :alt: Vector Rotate Node.

The *Vector Rotate Node* provides the ability to rotate a vector around a pivot point (*Center*).


Inputs
======

Vector
   Vector to be rotated.

Center
   Point to rotate around.

Axis
   Axis to rotate around.

Angle
   Angle to rotate the input vector by.

Rotation
   When *Type* is set to *Euler*, rotate the input vector
   by these angles around the X, Y, then Z axes in that order.


Properties
==========

Type
   The type of angle input.

   :X/Y/Z Axis:
      Rotates the vector around the defined axis and
      the amount of rotation is defined by the *Angle* input.
   :Axis Angle:
      Rotates the vector around an arbitrary axis defined by the *Axis* input vector.
      The amount of rotation is defined by the *Angle* input.
   :Euler:
      Rotates the vector about a center point defined by the *Center* input vector.
      The amount of rotation on each axis is defined by the *Rotation* input vector.
Invert
   Inverts the rotation angle.


Outputs
=======

Vector
   The rotated vector.


Examples
========

.. figure:: /images/render_shader-nodes_vector_vector-rotate_example.png
   :align: right

   Vector Rotate node example.


## Color

.. _bpy.types.View3DShading.color_type:

*****
Color
*****

The colors that the Workbench uses to render objects can be changed.

.. reference::

   :Panel:     :menuselection:`Render --> Color`

:Material:
   Use the color that can be set per material
   in the Viewport Display :ref:`properties-material-viewport-display` panel.
:Object:
   Use the color that can be set per object
   in the Viewport Display :ref:`properties-object-viewport-display` panel.
:Attribute:
   Display the active :ref:`Color Attribute <modeling-meshes-properties-object_data-color-attributes>`
   of an object. When an object has no active Color Attribute it will be rendered in the color set
   in the Viewport Display :ref:`properties-object-viewport-display` panel.
:Single:
   Render the whole scene using a single color. The color can be chosen.
:Random:
   A random color will be selected for every object in the scene.
:Texture:
   Show the texture from the active :doc:`/render/shader_nodes/textures/image`
   using the active UV map. If there is no active texture, the object will be
   rendered with the settings in the :ref:`properties-material-viewport-display`'s
   Viewport Display panel.


## Display Settings


****************
Viewport Display
****************

The Workbench engine does not work with shader trees. In various tabs of the Properties
are Viewport Display panels where settings can be adjusted that the Workbench engine uses.


.. _properties-object-viewport-display:

Object
======

The Viewport Display panel in the Object Properties has several settings that
are used by the Workbench Engine.

.. reference::

   :Panel:     :menuselection:`Properties --> Object --> Viewport Display`

Shadow
   When the *Shadow* in the :doc:`/render/workbench/options` is enabled
   this object will cast a shadow.
In Front
   When checked the object will be rendered in front of the other objects in the scene.
Color
   The color to render the object in when object color needs to be rendered.
   The alpha channel can be used to render the object transparent.


.. _properties-material-viewport-display:

Material
========

The Viewport Display panel in the Material Properties has several settings that
are used by the Workbench Engine.

.. reference::

   :Panel:     :menuselection:`Properties --> Material --> Viewport Display`

Color
   The color when rendering the material.
   The alpha channel can be used to render the object transparent.
Metallic
   Changes the amount of specular lighting. This is only available when
   *Specular Lighting* in the :doc:`/render/workbench/options` is enabled.
Roughness
   Changes the amount of roughness for specular lighting. This is only available when
   *Specular Lighting* in the :doc:`/render/workbench/options` is enabled.


.. _properties-world-viewport-display:

World
=====

The Viewport Display panel in the World Properties has several settings that
are used by the Workbench Engine.

.. reference::

   :Panel:     :menuselection:`Properties --> World --> Viewport Display`

Color
   The color of the world background. This color will be rendered
   in the background of the scene.


## Grease Pencil


*************
Grease Pencil
*************

.. reference::

   :Panel:     :menuselection:`Render --> Grease Pencil`

This panel is comprised of settings to control the rendering of :doc:`Grease Pencil Lines </grease_pencil/index>`.

Anti-Aliasing Threshold
   Threshold for the edge detection algorithm used to correct aliasing,
   higher values might over blur some part of the image.


## Index


#############
  Workbench
#############

.. toctree::
   :titlesonly:
   :maxdepth: 2

   introduction.rst
   performance.rst
   sampling.rst
   lighting.rst
   color.rst
   options.rst
   grease_pencil.rst
   display_settings.rst


## Introduction


************
Introduction
************

The Workbench Engine is a render engine optimized for fast rendering during modeling and animation preview.
It is not intended to be a render engine that will render final images for a project.
Its primary task is to display a scene in the 3D Viewport when it is being worked on.

.. note::

   While its not intended to be used for final renders,
   the Workbench render engine can be selected as the *Render Engine* in the Render properties.

By default the 3D Viewport uses Workbench to shade and light objects.
Unlike other render engines such as EEVEE or Cycles, the Workbench engine does not use shader nodes.
Instead, shading settings can be tweaked in the 3D Viewport's :doc:`Shading popover </editors/3dview/display/shading>`
or the render properties when doing final renders.

Workbench supports assigning random colors to objects to make each visually distinct.
Other coloring mechanisms also exist, including; materials, Color Attributes, and textures.

Workbench also has an X-ray mode to see through objects,
along with cavity and shadow shading to help display details in objects.
Workbench supports several lighting mechanisms including studio lighting and MatCaps.

The image below is an excellent example of the Workbench engine's capabilities
using random coloring and shadows to show the details of the model.

.. figure:: /images/render_workbench_introduction_example.png

   Workbench example.


## Lighting


********
Lighting
********

The Workbench engine does not use the lights of the scene.
The lighting conditions that will be used can be set in the Lighting panel.

.. reference::

   :Panel:     :menuselection:`Properties --> Render --> Lighting`

.. _bpy.types.View3DShading.light:

:Flat:
   Objects are "shaded" in a flat color, without any hilights or shadows.

:Studio:
   Use a predefined studio lighting setup, such as a key light
   shining from the front and a rim light shining from the back.
   Click the sphere to choose a different setup.

   The studio lights can be configured in the :ref:`Preferences <prefs-lights-studio>`.
   By default, they follow the viewport camera around, but this can be changed:

   World Space Lighting
      Keep the lights fixed in place rather than following the viewport camera.
   Rotation
      The rotation of the lights on the Z axis.

.. _render-workbench-matcap:

:MatCap:
   Use a Material Capture, which is an image with texturing, lighting
   and even reflections baked into it. Objects are shaded by
   simply picking colors from this image based on the direction of
   the normal in relation to the camera.

   Click the sphere to choose a different MatCap,
   or the double arrow button to flip it horizontally.

   Custom MatCaps can be loaded in the :ref:`Preferences <prefs-lights-matcaps>`.



## Options


*******
Options
*******

.. reference::

   :Menu:      :menuselection:`Properties --> Render --> Options` panel.

Backface Culling
   Use backface culling to hide backsides of faces.

X-Ray
   Render the scene transparent. With the slider you can control how
   transparent the scene should appear.

Shadow
   Renders a sharp shadow in the scene.

   Darkness
      Defines how dark the shadow should be rendered. This slider can be adjusted
      between 0 (shadow not visible) and 1 (shadow is black).

   Light Direction
      Controls the direction of the light source that casts the shadows.

   Shadow Shift
      Controls the Shadow termination angle. It can be used to limit self shadowing artifacts.

   Shadow Focus
      Controls the falloff near the edge of the shadow.

Cavity
   Highlight ridges and valleys in the scene geometry.

   Type
      How to calculate cavities.

      :World: More precise but is slower to calculate.
      :Screen: Fast but does not take the size of the ridges and valleys into account.
      :Both: Use both methods.

   Ridge
      Control the visibility of ridges.

   Valley
      Control the visibility of valleys.

Depth of Field
   Use the Depth of Field settings of the active camera in the viewport.
   Only visible when looking through the camera.

   The settings are located on :menuselection:`Properties --> Camera --> Depth of Field` panel.

Outline
   Render the outline of objects in the viewport. The color of the outline can be adjusted.

Specular Highlighting
   Render specular highlights.

   .. note::

      Only available when Lighting is set to *Studio* lighting or when a MatCap
      has been selected that contains a specular pass.


## Performance


***********
Performance
***********

.. reference::

   :Panel:     :menuselection:`Properties --> Render --> Performance`

High Quality Normals
   Uses higher precision normals and tangents which can improve
   visual quality for dense meshes with high frequency textures at the cost of memory.


## Sampling

.. _bpy.types.SceneDisplay.render_aa:
.. _bpy.types.SceneDisplay.viewport_aa:

********
Sampling
********

The quality of the renders can be adjusted by changing the :term:`Anti-Aliasing` method.
A different one can be selected for the 3D Viewport, viewport rendering and
for final rendering.

The setting for the 3D Viewport is a user preference to specify the anti-aliasing method
that runs best on the used system. The setting for viewport rendering
and final rendering is saved per scene.

.. reference::

   :Panel:     :menuselection:`Render --> Sampling`
               :menuselection:`Preferences --> Viewport`

No Anti-Aliasing
   With this option selected no anti-aliasing will be applied.

Single Pass Anti-Aliasing
   Scene will be rendered with a post-process anti-aliasing pass.

Multisample
   The scene will be rendered multiple times with a slight offset.
   The anti-aliasing will be gathered from the multiple renders.
   The number of samples are predefined so it uses the best distribution of the samples.

   5, 8, 11, 16, 32

   .. tip::

      Multisample anti-aliasing is well suited for rendering small details like hair.

   *Progressive Viewport Rendering*

   For the 3D Viewport, one sample is rendered at a time. When there are no changes
   to the scene or viewport the next sample will be rendered.


